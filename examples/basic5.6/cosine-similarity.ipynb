{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "\n",
    "# Cosine Similarity: From Vectors to NLP Applications\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Mathematical foundations of cosine similarity\n",
    "- How to calculate cosine similarity between 2D vectors\n",
    "- How to extend cosine similarity to higher-dimensional spaces\n",
    "- Practical applications of cosine similarity in NLP and text analysis\n",
    "- How to use cosine similarity with HuggingFace transformers for semantic search\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of linear algebra (vectors and dot products)\n",
    "- Familiarity with Python and NumPy\n",
    "- Basic knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of transformers and embeddings (refer to previous notebooks)\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. Section 1: Mathematical Foundations - Understanding cosine similarity formula\n",
    "2. Section 2: 2D Vector Examples - Visual and geometric intuition\n",
    "3. Section 3: Higher-Dimensional Vectors - Extending to multi-dimensional space\n",
    "4. Section 4: Text Similarity with HuggingFace - Practical NLP applications\n",
    "5. Section 5: Advanced Applications - Semantic search and similarity matrices\n",
    "6. Section 6: Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundations of Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the similarity between two vectors by computing the cosine of the angle between them.\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "The cosine similarity between two vectors $\\vec{A}$ and $\\vec{B}$ is defined as:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Where:\n",
    "- $\\vec{A} \\cdot \\vec{B}$ is the dot product of vectors A and B\n",
    "- $\\|\\vec{A}\\|$ and $\\|\\vec{B}\\|$ are the magnitudes (L2 norms) of vectors A and B\n",
    "- $n$ is the dimensionality of the vectors\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Range**: Cosine similarity values range from -1 to 1\n",
    "  - `1`: Vectors point in the same direction (perfect similarity)\n",
    "  - `0`: Vectors are orthogonal (no similarity)\n",
    "  - `-1`: Vectors point in opposite directions (perfect dissimilarity)\n",
    "\n",
    "- **Direction vs. Magnitude**: Cosine similarity focuses on the **direction** of vectors, not their magnitude\n",
    "  - Vectors `[2, 2]` and `[4, 4]` have cosine similarity = 1 (same direction)\n",
    "  - This is why cosine similarity is ideal for text analysis where frequency matters less than semantic meaning\n",
    "\n",
    "> üí° **Pro Tip**: In NLP, cosine similarity is preferred over Euclidean distance because it's scale-invariant. A document mentioning \"machine learning\" twice vs. four times should still be semantically similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cosine Similarity Between 2D Vectors\n",
    "\n",
    "Let's start with simple 2D vectors to build geometric intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set reproducible environment with repository standard seed=16\n",
    "np.random.seed(16)\n",
    "print(\"üî¢ Random seed set to 16 for reproducibility\")\n",
    "\n",
    "# Configure visualization style (repository standard)\n",
    "sns.set_style('darkgrid')  # Better readability with gridlines\n",
    "sns.set_palette(\"husl\")     # Consistent, accessible colors\n",
    "print(\"üìä Visualization style configured: darkgrid with husl palette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_manual(vector_a: np.ndarray, vector_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors manually.\n",
    "    \n",
    "    This function demonstrates the mathematical formula step-by-step\n",
    "    for educational purposes.\n",
    "    \n",
    "    Args:\n",
    "        vector_a: First vector (numpy array)\n",
    "        vector_b: Second vector (numpy array)\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate dot product\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    \n",
    "    # Step 2: Calculate magnitude (L2 norm) of each vector\n",
    "    magnitude_a = np.linalg.norm(vector_a)\n",
    "    magnitude_b = np.linalg.norm(vector_b)\n",
    "    \n",
    "    # Step 3: Calculate cosine similarity\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    cosine_sim = dot_product / (magnitude_a * magnitude_b + 1e-8)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def visualize_2d_vectors(vector_a: np.ndarray, vector_b: np.ndarray, \n",
    "                         similarity: float, title: str = \"2D Vector Comparison\"):\n",
    "    \"\"\"\n",
    "    Visualize two 2D vectors and their cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        vector_a: First 2D vector\n",
    "        vector_b: Second 2D vector\n",
    "        similarity: Cosine similarity value\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Plot vectors as arrows from origin\n",
    "    plt.quiver(0, 0, vector_a[0], vector_a[1], angles='xy', scale_units='xy', \n",
    "               scale=1, color='blue', width=0.008, label='Vector A')\n",
    "    plt.quiver(0, 0, vector_b[0], vector_b[1], angles='xy', scale_units='xy', \n",
    "               scale=1, color='red', width=0.008, label='Vector B')\n",
    "    \n",
    "    # Set axis limits with some padding\n",
    "    max_val = max(np.max(np.abs(vector_a)), np.max(np.abs(vector_b))) * 1.2\n",
    "    plt.xlim(-max_val, max_val)\n",
    "    plt.ylim(-max_val, max_val)\n",
    "    \n",
    "    # Add labels and grid\n",
    "    plt.xlabel('X-axis', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Y-axis', fontsize=12, fontweight='bold')\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add title with similarity score\n",
    "    plt.title(f\"{title}\\nCosine Similarity: {similarity:.4f}\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Similar Vectors (High Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two vectors pointing in similar directions\n",
    "vector_1 = np.array([3, 4])\n",
    "vector_2 = np.array([4, 5])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity_manual(vector_1, vector_2)\n",
    "\n",
    "print(\"üìê Vector Analysis:\")\n",
    "print(f\"Vector A: {vector_1}\")\n",
    "print(f\"Vector B: {vector_2}\")\n",
    "print(f\"\\nDot Product: {np.dot(vector_1, vector_2)}\")\n",
    "print(f\"Magnitude A: {np.linalg.norm(vector_1):.4f}\")\n",
    "print(f\"Magnitude B: {np.linalg.norm(vector_2):.4f}\")\n",
    "print(f\"\\n‚ú® Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"\\nüí° Interpretation: Similarity of {similarity:.4f} indicates vectors are pointing in very similar directions\")\n",
    "\n",
    "# Visualize\n",
    "visualize_2d_vectors(vector_1, vector_2, similarity, \"Example 1: Similar Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Orthogonal Vectors (Zero Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two perpendicular vectors (90 degree angle)\n",
    "vector_1 = np.array([5, 0])\n",
    "vector_2 = np.array([0, 5])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity_manual(vector_1, vector_2)\n",
    "\n",
    "print(\"üìê Vector Analysis:\")\n",
    "print(f\"Vector A: {vector_1}\")\n",
    "print(f\"Vector B: {vector_2}\")\n",
    "print(f\"\\nDot Product: {np.dot(vector_1, vector_2)}\")\n",
    "print(f\"Magnitude A: {np.linalg.norm(vector_1):.4f}\")\n",
    "print(f\"Magnitude B: {np.linalg.norm(vector_2):.4f}\")\n",
    "print(f\"\\n‚ú® Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"\\nüí° Interpretation: Similarity of {similarity:.4f} indicates vectors are orthogonal (90¬∞ angle)\")\n",
    "\n",
    "# Visualize\n",
    "visualize_2d_vectors(vector_1, vector_2, similarity, \"Example 2: Orthogonal Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Opposite Vectors (Negative Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two vectors pointing in opposite directions\n",
    "vector_1 = np.array([3, 4])\n",
    "vector_2 = np.array([-3, -4])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity_manual(vector_1, vector_2)\n",
    "\n",
    "print(\"üìê Vector Analysis:\")\n",
    "print(f\"Vector A: {vector_1}\")\n",
    "print(f\"Vector B: {vector_2}\")\n",
    "print(f\"\\nDot Product: {np.dot(vector_1, vector_2)}\")\n",
    "print(f\"Magnitude A: {np.linalg.norm(vector_1):.4f}\")\n",
    "print(f\"Magnitude B: {np.linalg.norm(vector_2):.4f}\")\n",
    "print(f\"\\n‚ú® Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"\\nüí° Interpretation: Similarity of {similarity:.4f} indicates vectors point in exactly opposite directions (180¬∞ angle)\")\n",
    "\n",
    "# Visualize\n",
    "visualize_2d_vectors(vector_1, vector_2, similarity, \"Example 3: Opposite Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Scale Invariance Property\n",
    "\n",
    "This example demonstrates that cosine similarity is invariant to vector magnitude - only direction matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two vectors with same direction but different magnitudes\n",
    "vector_1 = np.array([2, 2])\n",
    "vector_2 = np.array([8, 8])  # 4x the magnitude of vector_1\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity_manual(vector_1, vector_2)\n",
    "\n",
    "print(\"üìê Vector Analysis (Scale Invariance):\")\n",
    "print(f\"Vector A: {vector_1} (magnitude: {np.linalg.norm(vector_1):.4f})\")\n",
    "print(f\"Vector B: {vector_2} (magnitude: {np.linalg.norm(vector_2):.4f})\")\n",
    "print(f\"\\nüîç Vector B is 4x longer than Vector A\")\n",
    "print(f\"\\n‚ú® Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"\\nüí° Key Insight: Despite different magnitudes, cosine similarity = 1.0\")\n",
    "print(\"   This is because they point in the exact same direction!\")\n",
    "print(\"\\nüöÄ Why This Matters in NLP:\")\n",
    "print(\"   - Document length doesn't affect semantic similarity\")\n",
    "print(\"   - Term frequency differences are normalized out\")\n",
    "print(\"   - We focus on semantic direction, not scale\")\n",
    "\n",
    "# Visualize\n",
    "visualize_2d_vectors(vector_1, vector_2, similarity, \"Example 4: Scale Invariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cosine Similarity in Higher-Dimensional Space\n",
    "\n",
    "Real-world applications, especially in NLP, involve high-dimensional vectors. Let's extend our understanding to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn for efficient computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"‚úÖ sklearn library imported for efficient cosine similarity computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: 3D Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D vectors\n",
    "vector_3d_a = np.array([1, 2, 3])\n",
    "vector_3d_b = np.array([2, 3, 4])\n",
    "vector_3d_c = np.array([5, 1, 2])\n",
    "\n",
    "# Calculate similarities using our manual function\n",
    "sim_ab = cosine_similarity_manual(vector_3d_a, vector_3d_b)\n",
    "sim_ac = cosine_similarity_manual(vector_3d_a, vector_3d_c)\n",
    "sim_bc = cosine_similarity_manual(vector_3d_b, vector_3d_c)\n",
    "\n",
    "print(\"üìê 3D Vector Analysis:\")\n",
    "print(f\"Vector A: {vector_3d_a}\")\n",
    "print(f\"Vector B: {vector_3d_b}\")\n",
    "print(f\"Vector C: {vector_3d_c}\")\n",
    "print(f\"\\nüîç Pairwise Cosine Similarities:\")\n",
    "print(f\"A vs B: {sim_ab:.4f} (high similarity - similar direction)\")\n",
    "print(f\"A vs C: {sim_ac:.4f} (moderate similarity)\")\n",
    "print(f\"B vs C: {sim_bc:.4f} (moderate similarity)\")\n",
    "print(f\"\\nüí° As dimensionality increases, the geometric interpretation becomes abstract,\")\n",
    "print(\"   but the mathematical formula remains the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: High-Dimensional Vectors (Common in NLP)\n",
    "\n",
    "In NLP, embeddings are typically 384, 768, or even higher dimensions. Let's work with more realistic sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-dimensional random vectors (simulating sentence embeddings)\n",
    "# Common embedding dimensions: 384 (sentence-transformers), 768 (BERT), 1024 (large models)\n",
    "embedding_dim = 384\n",
    "\n",
    "# Generate random embeddings (in practice, these come from a model)\n",
    "# Using repository standard seed=16 for reproducibility\n",
    "np.random.seed(16)\n",
    "embedding_1 = np.random.randn(embedding_dim)\n",
    "embedding_2 = np.random.randn(embedding_dim)\n",
    "embedding_3 = embedding_1 + np.random.randn(embedding_dim) * 0.1  # Similar to embedding_1\n",
    "\n",
    "print(f\"üìä High-Dimensional Vector Analysis:\")\n",
    "print(f\"Embedding dimension: {embedding_dim}D\")\n",
    "print(f\"\\nVector shapes:\")\n",
    "print(f\"  Embedding 1: {embedding_1.shape}\")\n",
    "print(f\"  Embedding 2: {embedding_2.shape}\")\n",
    "print(f\"  Embedding 3: {embedding_3.shape}\")\n",
    "\n",
    "# Calculate similarities using sklearn (more efficient for high dimensions)\n",
    "# Reshape for sklearn: it expects 2D arrays (n_samples, n_features)\n",
    "embeddings = np.array([embedding_1, embedding_2, embedding_3])\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(f\"\\nüîç Similarity Matrix ({embedding_dim}D space):\")\n",
    "print(\"     Emb1    Emb2    Emb3\")\n",
    "for i, row in enumerate(similarity_matrix):\n",
    "    print(f\"Emb{i+1} {row[0]:7.4f} {row[1]:7.4f} {row[2]:7.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® Key Observations:\")\n",
    "print(f\"  ‚Ä¢ Diagonal values = 1.0 (each vector is identical to itself)\")\n",
    "print(f\"  ‚Ä¢ Emb1 vs Emb3 = {similarity_matrix[0, 2]:.4f} (high, as Emb3 was created from Emb1)\")\n",
    "print(f\"  ‚Ä¢ Emb1 vs Emb2 = {similarity_matrix[0, 1]:.4f} (lower, as they're independent)\")\n",
    "print(f\"  ‚Ä¢ Matrix is symmetric (sim(A,B) = sim(B,A))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing High-Dimensional Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more embeddings for better visualization\n",
    "np.random.seed(16)\n",
    "n_embeddings = 8\n",
    "embeddings_list = []\n",
    "\n",
    "# Create base embeddings\n",
    "for i in range(n_embeddings):\n",
    "    embeddings_list.append(np.random.randn(embedding_dim))\n",
    "\n",
    "# Stack into matrix\n",
    "embeddings_matrix = np.array(embeddings_list)\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix_large = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarity_matrix_large,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='viridis',\n",
    "    xticklabels=[f'Emb{i+1}' for i in range(n_embeddings)],\n",
    "    yticklabels=[f'Emb{i+1}' for i in range(n_embeddings)],\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "plt.title(f'Cosine Similarity Matrix for {embedding_dim}D Embeddings',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Embeddings', fontweight='bold')\n",
    "plt.ylabel('Embeddings', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Similarity matrix visualization complete\")\n",
    "print(\"\\nüí° In NLP applications, this matrix shows which sentences/documents are semantically similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Similarity with HuggingFace Transformers\n",
    "\n",
    "Now let's apply cosine similarity to real NLP tasks using HuggingFace transformers. This is where cosine similarity truly shines in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFace libraries\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Check device availability\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    COLAB_AVAILABLE = True\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Device Priority:\n",
    "    - General: CUDA GPU > TPU (Colab only) > MPS (Apple Silicon) > CPU\n",
    "    - Google Colab: Always prefer TPU when available\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    # Google Colab: Always prefer TPU when available\n",
    "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
    "        try:\n",
    "            device = xm.xla_device()\n",
    "            print(\"üî• Using Google Colab TPU for optimal performance\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TPU initialization failed: {e}\")\n",
    "    \n",
    "    # Standard device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get optimal device\n",
    "device = get_device()\n",
    "print(f\"‚úÖ Device configured: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Sentence Transformer Model\n",
    "\n",
    "We'll use a sentence-transformer model optimized for semantic similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentence transformer model\n",
    "# sentence-transformers/all-MiniLM-L6-v2 is a popular choice:\n",
    "# - Small and fast (384-dimensional embeddings)\n",
    "# - Trained specifically for semantic similarity\n",
    "# - Good performance on various NLP tasks\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"üì• Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to optimal device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"üìê Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(text: str, tokenizer, model, device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate sentence embedding using HuggingFace model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        model: HuggingFace model\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Sentence embedding vector\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use mean pooling on token embeddings to get sentence embedding\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Convert to numpy and return\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "print(\"‚úÖ Embedding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Semantic Similarity Between Sentences\n",
    "\n",
    "Let's reproduce the example from the issue - comparing sentences about pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences (from the issue example)\n",
    "sentences = [\n",
    "    \"I took my dog for a walk\",\n",
    "    \"I took my cat for a walk\",\n",
    "    \"The weather is beautiful today\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "print(\"üìù Analyzing sentence similarity...\\n\")\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "sentence_embeddings = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    embedding = get_sentence_embedding(sentence, tokenizer, model, device)\n",
    "    sentence_embeddings.append(embedding)\n",
    "    print(f\"‚úì Sentence {i+1} embedded: {embedding.shape}\")\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "print(f\"\\nüìä All embeddings shape: {sentence_embeddings.shape}\")\n",
    "print(f\"   ({len(sentences)} sentences √ó 384 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix\n",
    "# This is exactly what was shown in the issue!\n",
    "similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "print(\"üîç Cosine Similarity Matrix:\")\n",
    "print(\"\\nRows/Columns represent sentences:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i+1}. {sent}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'':30} Sen1    Sen2    Sen3    Sen4\")\n",
    "for i, (sent, row) in enumerate(zip(sentences, similarity_matrix)):\n",
    "    short_sent = sent[:25] + \"...\" if len(sent) > 25 else sent\n",
    "    print(f\"Sen{i+1}: {short_sent:25} {row[0]:.4f}  {row[1]:.4f}  {row[2]:.4f}  {row[3]:.4f}\")\n",
    "\n",
    "print(\"\\n‚ú® Key Insights:\")\n",
    "print(f\"  ‚Ä¢ Dog walk vs Cat walk: {similarity_matrix[0, 1]:.4f} (HIGH - very similar semantic meaning!)\")\n",
    "print(f\"  ‚Ä¢ Dog walk vs Weather: {similarity_matrix[0, 2]:.4f} (LOW - different topics)\")\n",
    "print(f\"  ‚Ä¢ Dog walk vs ML: {similarity_matrix[0, 3]:.4f} (LOW - completely different domains)\")\n",
    "print(\"\\nüí° This confirms the example from the issue: 'dog walk' and 'cat walk' have high overlap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the similarity matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create short labels for readability\n",
    "labels = [sent[:30] + '...' if len(sent) > 30 else sent for sent in sentences]\n",
    "\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='viridis',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "\n",
    "plt.title('Sentence Similarity Matrix (HuggingFace Transformer Embeddings)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentences', fontweight='bold')\n",
    "plt.ylabel('Sentences', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Similarity matrix visualization complete\")\n",
    "print(\"\\nüì∏ This recreates the visualization shown in the GitHub issue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Applications - Semantic Search\n",
    "\n",
    "One powerful application of cosine similarity in NLP is **semantic search** - finding the most relevant documents or sentences for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document collection\n",
    "documents = [\n",
    "    \"The cat sat on the mat in the living room.\",\n",
    "    \"A feline rested comfortably on the soft rug.\",\n",
    "    \"The dog ran quickly through the green park.\",\n",
    "    \"Machine learning algorithms can solve complex problems.\",\n",
    "    \"Artificial intelligence is transforming modern technology.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"The weather was sunny and warm yesterday.\",\n",
    "    \"I enjoy reading books about science fiction.\"\n",
    "]\n",
    "\n",
    "print(f\"üìö Document Collection: {len(documents)} documents\")\n",
    "print(\"\\nGenerating embeddings for all documents...\")\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "document_embeddings = []\n",
    "for doc in documents:\n",
    "    embedding = get_sentence_embedding(doc, tokenizer, model, device)\n",
    "    document_embeddings.append(embedding)\n",
    "\n",
    "document_embeddings = np.array(document_embeddings)\n",
    "print(f\"‚úÖ Document embeddings generated: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, documents: List[str], document_embeddings: np.ndarray, \n",
    "                   top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        documents: List of document strings\n",
    "        document_embeddings: Pre-computed document embeddings\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (document, similarity_score)\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = get_sentence_embedding(query, tokenizer, model, device)\n",
    "    query_embedding = query_embedding.reshape(1, -1)  # Reshape for sklearn\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n",
    "    \n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return results\n",
    "    results = [(documents[idx], similarities[idx]) for idx in top_indices]\n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Semantic search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Semantic Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Tell me about cats\",\n",
    "    \"What is AI and deep learning?\",\n",
    "    \"Programming languages\"\n",
    "]\n",
    "\n",
    "print(\"üîç Semantic Search Results\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîé Query: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = semantic_search(query, documents, document_embeddings, top_k=3)\n",
    "    \n",
    "    for rank, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n  Rank {rank}: (Similarity: {score:.4f})\")\n",
    "        print(f\"  üìÑ {doc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚ú® Notice how the search understands semantic meaning:\")\n",
    "print(\"  ‚Ä¢ 'cats' query finds both 'cat' and 'feline' documents\")\n",
    "print(\"  ‚Ä¢ 'AI and deep learning' retrieves relevant ML documents\")\n",
    "print(\"  ‚Ä¢ 'Programming languages' finds Python document\")\n",
    "print(\"\\nüí° This is the power of cosine similarity with transformer embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Semantic Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one query for detailed visualization\n",
    "query = \"artificial intelligence and neural networks\"\n",
    "query_embedding = get_sentence_embedding(query, tokenizer, model, device)\n",
    "query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "# Calculate similarities with all documents\n",
    "similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "sorted_similarities = similarities[sorted_indices]\n",
    "sorted_docs = [documents[i] for i in sorted_indices]\n",
    "\n",
    "# Create labels (truncated)\n",
    "labels = [doc[:40] + '...' if len(doc) > 40 else doc for doc in sorted_docs]\n",
    "\n",
    "# Plot\n",
    "colors = plt.cm.viridis(sorted_similarities)\n",
    "bars = plt.barh(range(len(sorted_similarities)), sorted_similarities, color=colors)\n",
    "plt.yticks(range(len(sorted_similarities)), labels)\n",
    "plt.xlabel('Cosine Similarity', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Documents', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Semantic Search: \"{query}\"\\nCosine Similarity Scores',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, sim) in enumerate(zip(bars, sorted_similarities)):\n",
    "    plt.text(sim + 0.02, i, f'{sim:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Semantic search visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Best Practices\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "\n",
    "1. **Mathematical Foundation**\n",
    "   - Cosine similarity measures the angle between vectors\n",
    "   - Formula: $\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$\n",
    "   - Range: [-1, 1] where 1 = identical direction, 0 = orthogonal, -1 = opposite\n",
    "\n",
    "2. **Geometric Intuition**\n",
    "   - In 2D: Can visualize as angle between arrows\n",
    "   - Scale invariance: Only direction matters, not magnitude\n",
    "   - Extends seamlessly to high-dimensional spaces (384D, 768D, etc.)\n",
    "\n",
    "3. **NLP Applications**\n",
    "   - Convert text to embeddings using transformer models\n",
    "   - Compare semantic similarity between sentences/documents\n",
    "   - Enable semantic search and information retrieval\n",
    "   - Build recommendation systems based on content similarity\n",
    "\n",
    "4. **Implementation with HuggingFace**\n",
    "   - Use sentence-transformer models for optimal embeddings\n",
    "   - sklearn's `cosine_similarity` for efficient computation\n",
    "   - Mean pooling to convert token embeddings to sentence embeddings\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Use `sentence-transformers` models for semantic similarity tasks\n",
    "   - Popular choices: `all-MiniLM-L6-v2` (fast), `all-mpnet-base-v2` (accurate)\n",
    "   - Consider embedding dimension vs. performance trade-offs\n",
    "\n",
    "2. **Normalization**\n",
    "   - Cosine similarity automatically normalizes by magnitude\n",
    "   - Consider L2 normalization for faster computation (dot product)\n",
    "   - For large-scale applications, use FAISS for efficient similarity search\n",
    "\n",
    "3. **Computational Efficiency**\n",
    "   - Pre-compute and cache document embeddings\n",
    "   - Use batch processing for multiple texts\n",
    "   - Consider approximate nearest neighbor methods for large datasets\n",
    "\n",
    "4. **Interpretation**\n",
    "   - Similarity > 0.8: Very similar (likely paraphrases)\n",
    "   - Similarity 0.5-0.8: Moderately similar (related topics)\n",
    "   - Similarity < 0.5: Dissimilar (different topics)\n",
    "   - Exact thresholds depend on domain and model\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Notebook: Feature Extraction**: Deep dive into transformer embeddings\n",
    "- **Documentation: FAISS.md**: Efficient similarity search at scale\n",
    "- **Advanced Topics**: Semantic clustering, duplicate detection, recommendation systems\n",
    "- **External Resources**: \n",
    "  - [HuggingFace Sentence Transformers](https://www.sbert.net/)\n",
    "  - [sklearn Cosine Similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n",
    "\n",
    "> **Key Takeaway**: Cosine similarity is the fundamental metric for measuring semantic similarity in NLP. By converting text to embeddings and computing cosine similarity, we can build powerful applications like semantic search, duplicate detection, and recommendation systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
