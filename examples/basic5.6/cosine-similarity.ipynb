{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/cosine-similarity.ipynb)\n",
    "\n",
    "# Understanding Cosine Similarity: From Vectors to Text Embeddings\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What cosine similarity is and how it measures vector similarity\n",
    "- How to calculate cosine similarity between 2D vectors with visualization\n",
    "- How to compute cosine similarity in higher dimensions\n",
    "- How to apply cosine similarity to text embeddings in NLP and HuggingFace\n",
    "- Practical applications for semantic similarity in hate speech detection\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of vectors and linear algebra\n",
    "- Familiarity with Python and NumPy\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Basic understanding of machine learning concepts\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Mathematical Foundation**: Understanding cosine similarity formula and intuition\n",
    "2. **2D Vector Similarity**: Visual demonstration with 2D vectors\n",
    "3. **Higher Dimensions**: Computing similarity in multi-dimensional space\n",
    "4. **Text Embeddings**: Applying cosine similarity to text using HuggingFace transformers\n",
    "5. **Practical Applications**: Semantic search and similarity matrices\n",
    "6. **Summary**: Key takeaways and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundation of Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the similarity between two vectors by computing the cosine of the angle between them. Unlike Euclidean distance, cosine similarity focuses on the *direction* rather than the *magnitude* of vectors.\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "For two vectors $\\mathbf{A}$ and $\\mathbf{B}$:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product of vectors A and B\n",
    "- $\\|\\mathbf{A}\\|$ is the magnitude (L2 norm) of vector A\n",
    "- $\\|\\mathbf{B}\\|$ is the magnitude (L2 norm) of vector B\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Value Range**: Cosine similarity ranges from -1 to 1\n",
    "  - `1`: Vectors point in the same direction (perfectly similar)\n",
    "  - `0`: Vectors are orthogonal (no similarity)\n",
    "  - `-1`: Vectors point in opposite directions (perfectly dissimilar)\n",
    "\n",
    "- **Key Property**: Scale-invariant - it measures orientation, not magnitude\n",
    "  - `[1, 2]` and `[2, 4]` have cosine similarity of 1 (same direction)\n",
    "  - This is perfect for text embeddings where frequency shouldn't dominate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch numpy matplotlib scikit-learn seaborn\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set reproducible environment with repository standard seed=16\n",
    "torch.manual_seed(16)\n",
    "np.random.seed(16)\n",
    "print(\"üî¢ Random seed set to 16 for reproducibility\")\n",
    "\n",
    "# Configure visualization style (repository standard)\n",
    "sns.set_style('darkgrid')  # Better readability with gridlines\n",
    "sns.set_palette(\"husl\")     # Consistent, accessible colors\n",
    "print(\"üìä Visualization style configured: darkgrid with husl palette\")\n",
    "\n",
    "# For Google Colab TPU compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    COLAB_AVAILABLE = True\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "print(\"\\nüìö Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Device Priority:\n",
    "    - General: CUDA GPU > TPU (Colab only) > MPS (Apple Silicon) > CPU\n",
    "    - Google Colab: Always prefer TPU when available\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    # Google Colab: Always prefer TPU when available\n",
    "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
    "        try:\n",
    "            # Try to initialize TPU\n",
    "            device = xm.xla_device()\n",
    "            print(\"üî• Using Google Colab TPU for optimal performance\")\n",
    "            print(\"üí° TPU is preferred in Colab for training and inference\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TPU initialization failed: {e}\")\n",
    "            print(\"Falling back to GPU/CPU detection\")\n",
    "    \n",
    "    # Standard device detection for other environments\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU/TPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get device for later use\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cosine Similarity in 2D Space\n",
    "\n",
    "Let's start with a simple example using 2D vectors that we can visualize. This will help build intuition before moving to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_manual(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors manually.\n",
    "    \n",
    "    This implementation shows the mathematical steps explicitly\n",
    "    for educational purposes.\n",
    "    \n",
    "    Args:\n",
    "        vector_a: First vector (numpy array)\n",
    "        vector_b: Second vector (numpy array)\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate dot product (numerator)\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    \n",
    "    # Step 2: Calculate magnitudes (denominator)\n",
    "    magnitude_a = np.sqrt(np.sum(vector_a ** 2))\n",
    "    magnitude_b = np.sqrt(np.sum(vector_b ** 2))\n",
    "    \n",
    "    # Step 3: Calculate cosine similarity\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    cosine_sim = dot_product / (magnitude_a * magnitude_b + 1e-8)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "# Example 1: Two 2D vectors pointing in similar directions\n",
    "vector1 = np.array([3, 4])\n",
    "vector2 = np.array([4, 5])\n",
    "\n",
    "# Calculate similarity manually\n",
    "similarity = cosine_similarity_manual(vector1, vector2)\n",
    "\n",
    "print(\"üìê 2D Vector Similarity Example\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vector A: {vector1}\")\n",
    "print(f\"Vector B: {vector2}\")\n",
    "print(f\"\\nDot product: {np.dot(vector1, vector2)}\")\n",
    "print(f\"Magnitude A: {np.linalg.norm(vector1):.4f}\")\n",
    "print(f\"Magnitude B: {np.linalg.norm(vector2):.4f}\")\n",
    "print(f\"\\n‚úÖ Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "# Verify with sklearn\n",
    "similarity_sklearn = cosine_similarity([vector1], [vector2])[0, 0]\n",
    "print(f\"‚úÖ sklearn verification: {similarity_sklearn:.4f}\")\n",
    "\n",
    "# Example 2: Orthogonal vectors (perpendicular)\n",
    "vector3 = np.array([1, 0])\n",
    "vector4 = np.array([0, 1])\n",
    "similarity_orthogonal = cosine_similarity_manual(vector3, vector4)\n",
    "\n",
    "print(f\"\\nüìê Orthogonal Vectors:\")\n",
    "print(f\"Vector C: {vector3}\")\n",
    "print(f\"Vector D: {vector4}\")\n",
    "print(f\"‚úÖ Cosine Similarity: {similarity_orthogonal:.4f} (perpendicular)\")\n",
    "\n",
    "# Example 3: Opposite direction vectors\n",
    "vector5 = np.array([2, 3])\n",
    "vector6 = np.array([-2, -3])\n",
    "similarity_opposite = cosine_similarity_manual(vector5, vector6)\n",
    "\n",
    "print(f\"\\nüìê Opposite Direction Vectors:\")\n",
    "print(f\"Vector E: {vector5}\")\n",
    "print(f\"Vector F: {vector6}\")\n",
    "print(f\"‚úÖ Cosine Similarity: {similarity_opposite:.4f} (opposite directions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2D vectors and their cosine similarity\n",
    "def visualize_2d_vectors(vector_a, vector_b, title=\"2D Vector Cosine Similarity\"):\n",
    "    \"\"\"\n",
    "    Visualize two 2D vectors and display their cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        vector_a: First vector (numpy array of shape [2])\n",
    "        vector_b: Second vector (numpy array of shape [2])\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity_manual(vector_a, vector_b)\n",
    "    \n",
    "    # Calculate angle between vectors (in degrees)\n",
    "    angle_radians = np.arccos(np.clip(similarity, -1.0, 1.0))\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot vectors\n",
    "    ax.quiver(0, 0, vector_a[0], vector_a[1], angles='xy', scale_units='xy', \n",
    "              scale=1, color='blue', width=0.015, label=f'Vector A: {vector_a}')\n",
    "    ax.quiver(0, 0, vector_b[0], vector_b[1], angles='xy', scale_units='xy', \n",
    "              scale=1, color='red', width=0.015, label=f'Vector B: {vector_b}')\n",
    "    \n",
    "    # Set axis properties\n",
    "    max_val = max(abs(vector_a).max(), abs(vector_b).max()) * 1.3\n",
    "    ax.set_xlim(-max_val, max_val)\n",
    "    ax.set_ylim(-max_val, max_val)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Add grid and labels\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('X axis', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Y axis', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add title with similarity information\n",
    "    ax.set_title(f'{title}\\nCosine Similarity: {similarity:.4f} | Angle: {angle_degrees:.2f}¬∞', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    # Add text box with interpretation\n",
    "    interpretation = \"\"\n",
    "    if similarity > 0.9:\n",
    "        interpretation = \"Very Similar (same direction)\"\n",
    "    elif similarity > 0.5:\n",
    "        interpretation = \"Moderately Similar\"\n",
    "    elif similarity > -0.5:\n",
    "        interpretation = \"Low Similarity\"\n",
    "    else:\n",
    "        interpretation = \"Opposite Directions\"\n",
    "    \n",
    "    textstr = f'Interpretation: {interpretation}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the examples\n",
    "print(\"\\nüìä Visualizing 2D Vector Examples\\n\")\n",
    "\n",
    "# Example 1: Similar direction\n",
    "visualize_2d_vectors(vector1, vector2, \"Example 1: Similar Direction Vectors\")\n",
    "\n",
    "# Example 2: Orthogonal vectors\n",
    "visualize_2d_vectors(vector3, vector4, \"Example 2: Orthogonal Vectors\")\n",
    "\n",
    "# Example 3: Opposite directions\n",
    "visualize_2d_vectors(vector5, vector6, \"Example 3: Opposite Direction Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cosine Similarity in Higher Dimensions\n",
    "\n",
    "While 2D examples help build intuition, real-world applications like text embeddings work in much higher dimensions (often 384, 768, or even 4096 dimensions). The mathematical principles remain the same, but visualization becomes impossible.\n",
    "\n",
    "Let's compute cosine similarity for higher-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random high-dimensional vectors for demonstration\n",
    "# Using seed=16 for reproducibility (repository standard)\n",
    "np.random.seed(16)\n",
    "\n",
    "# Create vectors of different dimensions\n",
    "dimensions = [10, 100, 384, 768]\n",
    "\n",
    "print(\"üìä Cosine Similarity in Higher Dimensions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dim in dimensions:\n",
    "    # Generate random vectors\n",
    "    vec_a = np.random.randn(dim)\n",
    "    vec_b = np.random.randn(dim)\n",
    "    \n",
    "    # Create a similar vector (vec_c = vec_a with some noise)\n",
    "    vec_c = vec_a + 0.1 * np.random.randn(dim)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sim_random = cosine_similarity([vec_a], [vec_b])[0, 0]\n",
    "    sim_similar = cosine_similarity([vec_a], [vec_c])[0, 0]\n",
    "    \n",
    "    print(f\"\\nüìê Dimension: {dim}\")\n",
    "    print(f\"   Random vectors similarity:  {sim_random:.4f}\")\n",
    "    print(f\"   Similar vectors similarity: {sim_similar:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   In high dimensions, random vectors tend to be nearly orthogonal (similarity ‚âà 0)\")\n",
    "print(\"   Vectors derived from each other maintain high similarity even in high dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate efficiency comparison: manual vs sklearn\n",
    "import time\n",
    "\n",
    "# Create large high-dimensional vectors\n",
    "dim = 768  # Common dimension for BERT embeddings\n",
    "n_vectors = 1000\n",
    "\n",
    "np.random.seed(16)\n",
    "vectors = np.random.randn(n_vectors, dim)\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Computing similarity matrix for {n_vectors} vectors of dimension {dim}\")\n",
    "\n",
    "# Using sklearn (optimized)\n",
    "start_time = time.time()\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ sklearn.metrics.pairwise.cosine_similarity:\")\n",
    "print(f\"   Time: {sklearn_time:.4f} seconds\")\n",
    "print(f\"   Output shape: {similarity_matrix.shape}\")\n",
    "print(f\"   Memory efficient and optimized for large-scale computations\")\n",
    "\n",
    "# Show some example similarities\n",
    "print(f\"\\nüìä Sample Similarity Values:\")\n",
    "print(f\"   Vector 0 vs Vector 0: {similarity_matrix[0, 0]:.4f} (self-similarity, always 1.0)\")\n",
    "print(f\"   Vector 0 vs Vector 1: {similarity_matrix[0, 1]:.4f}\")\n",
    "print(f\"   Vector 0 vs Vector 2: {similarity_matrix[0, 2]:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Best Practice:\")\n",
    "print(\"   Always use sklearn.metrics.pairwise.cosine_similarity for production code\")\n",
    "print(\"   It's highly optimized and handles edge cases properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cosine Similarity for Text with HuggingFace Transformers\n",
    "\n",
    "Now let's apply cosine similarity to real text data using HuggingFace transformers. This is where the concept becomes powerful for NLP applications.\n",
    "\n",
    "### Process:\n",
    "1. **Tokenize** text into tokens\n",
    "2. **Generate embeddings** using a pre-trained transformer model\n",
    "3. **Compute cosine similarity** between text embeddings\n",
    "4. **Interpret results** for semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for generating text embeddings\n",
    "# Using sentence-transformers/all-MiniLM-L6-v2 - optimized for semantic similarity\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"üì• Loading model: {model_name}\")\n",
    "print(\"This model generates 384-dimensional embeddings optimized for semantic similarity\\n\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to optimal device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully on {device}\")\n",
    "print(f\"üìä Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Generate text embedding using HuggingFace transformer model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        model: HuggingFace model\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Text embedding vector\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate embeddings (no gradient computation needed)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Use mean pooling to get sentence-level embedding\n",
    "        # This averages token embeddings to create a single vector for the entire text\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Convert to numpy and return\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Example sentences for similarity comparison\n",
    "sentences = [\n",
    "    \"I took my dog for a walk in the park.\",\n",
    "    \"I took my cat for a walk in the park.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\"\n",
    "]\n",
    "\n",
    "print(\"üìù Sample Sentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"   {i}. {sent}\")\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "print(f\"\\nüîÑ Generating embeddings...\")\n",
    "sentence_embeddings = np.array([\n",
    "    get_text_embedding(sent, tokenizer, model, device) \n",
    "    for sent in sentences\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Generated embeddings shape: {sentence_embeddings.shape}\")\n",
    "print(f\"   (5 sentences √ó 384 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix between all sentence pairs\n",
    "# This is the key function call from the issue example\n",
    "similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "print(\"üìä Cosine Similarity Matrix\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSimilarity scores between all pairs of sentences:\\n\")\n",
    "\n",
    "# Display similarity matrix with sentence labels\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i < j:  # Only show upper triangle (avoid duplicates)\n",
    "            print(f\"Sentence {i+1} ‚Üî Sentence {j+1}: {similarity_matrix[i, j]:.4f}\")\n",
    "            print(f\"  ‚Üí \\\"{sentences[i][:50]}...\\\"\")\n",
    "            print(f\"  ‚Üí \\\"{sentences[j][:50]}...\\\"\")\n",
    "            print()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Sentences 1 & 2 have high similarity (dog vs cat walk) - same structure\")\n",
    "print(\"   ‚Ä¢ Sentences 4 & 5 have high similarity - both about AI/ML concepts\")\n",
    "print(\"   ‚Ä¢ Sentence 3 (weather) has low similarity with technical sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the similarity matrix as a heatmap\n",
    "def visualize_similarity_heatmap(similarity_matrix, sentences):\n",
    "    \"\"\"\n",
    "    Visualize text similarity matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: Cosine similarity matrix\n",
    "        sentences: List of text strings\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create shorter labels for readability\n",
    "    labels = [f\"S{i+1}: {sent[:30]}...\" if len(sent) > 30 else f\"S{i+1}: {sent}\" \n",
    "              for i, sent in enumerate(sentences)]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        annot=True,           # Show values in cells\n",
    "        fmt='.3f',            # Format numbers to 3 decimal places\n",
    "        cmap='viridis',       # Color scheme\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        cbar_kws={'label': 'Cosine Similarity'},\n",
    "        vmin=0,               # Minimum value\n",
    "        vmax=1,               # Maximum value\n",
    "        square=True           # Make cells square\n",
    "    )\n",
    "    \n",
    "    plt.title('Text Similarity Matrix (Cosine Similarity)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Sentences', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Sentences', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "print(\"\\nüìä Visualizing Similarity Matrix as Heatmap\\n\")\n",
    "visualize_similarity_heatmap(similarity_matrix, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Application - Semantic Search\n",
    "\n",
    "Let's implement a simple semantic search engine using cosine similarity. This is a common real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document collection for semantic search\n",
    "documents = [\n",
    "    \"Hate speech detection is crucial for online safety.\",\n",
    "    \"Machine learning models can identify toxic content.\",\n",
    "    \"Natural language processing helps understand text meaning.\",\n",
    "    \"Transformers revolutionized NLP with attention mechanisms.\",\n",
    "    \"Social media platforms use AI to moderate content.\",\n",
    "    \"Deep learning requires large amounts of training data.\",\n",
    "    \"BERT and RoBERTa are popular transformer models.\",\n",
    "    \"Toxic comment classification protects online communities.\",\n",
    "    \"The weather forecast predicts rain tomorrow.\",\n",
    "    \"Python is a popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "print(\"üìö Document Collection:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"   {i:2d}. {doc}\")\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "print(f\"\\nüîÑ Generating document embeddings...\")\n",
    "document_embeddings = np.array([\n",
    "    get_text_embedding(doc, tokenizer, model, device) \n",
    "    for doc in documents\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Generated {len(documents)} document embeddings\")\n",
    "print(f\"   Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, documents, document_embeddings, tokenizer, model, device, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        documents: List of document strings\n",
    "        document_embeddings: Pre-computed document embeddings\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        model: HuggingFace model\n",
    "        device: Device for inference\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = get_text_embedding(query, tokenizer, model, device)\n",
    "    \n",
    "    # Calculate cosine similarity between query and all documents\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return results\n",
    "    results = [(documents[idx], similarities[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"How to detect hate speech in text?\",\n",
    "    \"What are transformer models?\",\n",
    "    \"Weather information\"\n",
    "]\n",
    "\n",
    "print(\"üîç Semantic Search Examples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nüìù Query: \\\"{query}\\\"\")\n",
    "    print(\"\\nTop 3 Most Relevant Documents:\")\n",
    "    \n",
    "    results = semantic_search(query, documents, document_embeddings, \n",
    "                             tokenizer, model, device, top_k=3)\n",
    "    \n",
    "    for rank, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n   {rank}. Similarity: {score:.4f}\")\n",
    "        print(f\"      ‚Üí {doc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"   Semantic search finds relevant documents even when exact words don't match!\")\n",
    "print(\"   It understands the meaning and context of the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Hate Speech Detection Example\n",
    "\n",
    "Let's apply cosine similarity to a hate speech detection scenario, which is a key focus area of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts including normal and potentially toxic content\n",
    "sample_texts = [\n",
    "    \"I love spending time with my friends and family.\",\n",
    "    \"This is a great community with wonderful people.\",\n",
    "    \"Thank you for your help and support!\",\n",
    "    \"I disagree with this policy but respect your opinion.\",\n",
    "    \"This content is inappropriate and offensive.\",\n",
    "    \"Stop spreading hate and negativity online.\",\n",
    "]\n",
    "\n",
    "print(\"üìù Sample Texts for Analysis:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"   {i}. {text}\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(f\"\\nüîÑ Generating embeddings...\")\n",
    "sample_embeddings = np.array([\n",
    "    get_text_embedding(text, tokenizer, model, device) \n",
    "    for text in sample_texts\n",
    "])\n",
    "\n",
    "# Calculate similarity matrix\n",
    "sample_similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "print(f\"‚úÖ Computed similarity matrix: {sample_similarity_matrix.shape}\")\n",
    "\n",
    "# Visualize\n",
    "print(\"\\nüìä Similarity Matrix Heatmap:\\n\")\n",
    "visualize_similarity_heatmap(sample_similarity_matrix, sample_texts)\n",
    "\n",
    "# Analyze patterns\n",
    "print(\"\\nüîç Analysis:\")\n",
    "print(\"   ‚Ä¢ Positive messages (1-3) cluster together with high similarity\")\n",
    "print(\"   ‚Ä¢ Critical/negative messages (5-6) show similarity to each other\")\n",
    "print(\"   ‚Ä¢ Neutral message (4) has moderate similarity with both groups\")\n",
    "print(\"\\nüí° Application:\")\n",
    "print(\"   Cosine similarity helps identify content patterns for moderation\")\n",
    "print(\"   Can be used to find similar toxic content or cluster user behaviors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Understanding the Similarity Matrix\n",
    "\n",
    "Let's dive deeper into interpreting similarity matrices and what they tell us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed analysis function\n",
    "def analyze_similarity_matrix(similarity_matrix, texts, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Analyze and interpret a similarity matrix.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: Cosine similarity matrix\n",
    "        texts: List of text strings\n",
    "        threshold: Similarity threshold for \"high similarity\"\n",
    "    \"\"\"\n",
    "    n = len(texts)\n",
    "    \n",
    "    print(\"üìä Similarity Matrix Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Basic statistics\n",
    "    # Extract upper triangle (excluding diagonal)\n",
    "    upper_triangle = similarity_matrix[np.triu_indices(n, k=1)]\n",
    "    \n",
    "    print(f\"\\nüìà Statistics (excluding self-similarity):\")\n",
    "    print(f\"   Mean similarity:   {upper_triangle.mean():.4f}\")\n",
    "    print(f\"   Median similarity: {np.median(upper_triangle):.4f}\")\n",
    "    print(f\"   Min similarity:    {upper_triangle.min():.4f}\")\n",
    "    print(f\"   Max similarity:    {upper_triangle.max():.4f}\")\n",
    "    print(f\"   Std deviation:     {upper_triangle.std():.4f}\")\n",
    "    \n",
    "    # Find highly similar pairs\n",
    "    print(f\"\\nüîó Highly Similar Pairs (similarity > {threshold}):\")\n",
    "    high_similarity_count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if similarity_matrix[i, j] > threshold:\n",
    "                high_similarity_count += 1\n",
    "                print(f\"\\n   Pair {i+1}-{j+1}: {similarity_matrix[i, j]:.4f}\")\n",
    "                print(f\"   ‚Üí Text {i+1}: {texts[i][:60]}...\")\n",
    "                print(f\"   ‚Üí Text {j+1}: {texts[j][:60]}...\")\n",
    "    \n",
    "    if high_similarity_count == 0:\n",
    "        print(f\"   No pairs found with similarity > {threshold}\")\n",
    "    \n",
    "    # Find dissimilar pairs\n",
    "    print(f\"\\nüîÄ Most Dissimilar Pairs (lowest similarity):\")\n",
    "    min_indices = np.unravel_index(np.argmin(upper_triangle), (n, n))\n",
    "    dissimilar_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dissimilar_pairs.append((i, j, similarity_matrix[i, j]))\n",
    "    \n",
    "    dissimilar_pairs.sort(key=lambda x: x[2])\n",
    "    for i, j, sim in dissimilar_pairs[:3]:\n",
    "        print(f\"\\n   Pair {i+1}-{j+1}: {sim:.4f}\")\n",
    "        print(f\"   ‚Üí Text {i+1}: {texts[i][:60]}...\")\n",
    "        print(f\"   ‚Üí Text {j+1}: {texts[j][:60]}...\")\n",
    "\n",
    "# Analyze our sample similarity matrix\n",
    "analyze_similarity_matrix(similarity_matrix, sentences, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Best Practices and Tips\n",
    "\n",
    "Here are important considerations when working with cosine similarity in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate normalization importance\n",
    "def demonstrate_normalization():\n",
    "    \"\"\"\n",
    "    Show why vector normalization matters for cosine similarity.\n",
    "    \"\"\"\n",
    "    print(\"üìê Vector Normalization for Cosine Similarity\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create two vectors with different magnitudes but same direction\n",
    "    vec1 = np.array([1, 2, 3])\n",
    "    vec2 = np.array([2, 4, 6])  # Same direction, double magnitude\n",
    "    vec3 = np.array([1, 1, 1])  # Different direction\n",
    "    \n",
    "    print(\"\\nüìä Original Vectors:\")\n",
    "    print(f\"   Vector 1: {vec1} (magnitude: {np.linalg.norm(vec1):.4f})\")\n",
    "    print(f\"   Vector 2: {vec2} (magnitude: {np.linalg.norm(vec2):.4f})\")\n",
    "    print(f\"   Vector 3: {vec3} (magnitude: {np.linalg.norm(vec3):.4f})\")\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sim_1_2 = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "    sim_1_3 = cosine_similarity([vec1], [vec3])[0, 0]\n",
    "    \n",
    "    print(\"\\n‚úÖ Cosine Similarities:\")\n",
    "    print(f\"   Vec1 ‚Üî Vec2 (same direction): {sim_1_2:.4f}\")\n",
    "    print(f\"   Vec1 ‚Üî Vec3 (different direction): {sim_1_3:.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insight:\")\n",
    "    print(\"   Cosine similarity is magnitude-invariant!\")\n",
    "    print(\"   Vectors with same direction have similarity = 1.0 regardless of magnitude\")\n",
    "    print(\"   This is why it's perfect for text: 'cat' and 'cat cat cat' should be similar\")\n",
    "    \n",
    "    # Normalize vectors manually\n",
    "    vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "    \n",
    "    print(\"\\nüìê Normalized Vectors (unit length):\")\n",
    "    print(f\"   Vector 1: {vec1_norm}\")\n",
    "    print(f\"   Vector 2: {vec2_norm}\")\n",
    "    print(\"   Notice: Normalized vectors point in same direction with magnitude 1.0\")\n",
    "    \n",
    "    # For normalized vectors, cosine similarity = dot product\n",
    "    dot_product = np.dot(vec1_norm, vec2_norm)\n",
    "    print(f\"\\nüî¢ For normalized vectors: dot product = {dot_product:.4f}\")\n",
    "    print(f\"   This equals cosine similarity: {sim_1_2:.4f}\")\n",
    "\n",
    "demonstrate_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "print(\"üìö Best Practices for Cosine Similarity in NLP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_practices = {\n",
    "    \"‚úÖ DO's\": [\n",
    "        \"Use sklearn.metrics.pairwise.cosine_similarity for efficiency\",\n",
    "        \"Choose appropriate embedding models for your domain (e.g., hate speech models)\",\n",
    "        \"Normalize embeddings when computing similarities at scale\",\n",
    "        \"Use batch processing for large datasets to improve performance\",\n",
    "        \"Consider the context: high similarity threshold varies by application\",\n",
    "        \"Visualize similarity matrices to understand data relationships\",\n",
    "        \"Cache embeddings for repeated similarity computations\"\n",
    "    ],\n",
    "    \"‚ùå DON'Ts\": [\n",
    "        \"Don't use Euclidean distance when direction matters more than magnitude\",\n",
    "        \"Don't compare raw text without proper embeddings\",\n",
    "        \"Don't ignore computational costs for large-scale comparisons\",\n",
    "        \"Don't use generic models when domain-specific ones exist\",\n",
    "        \"Don't forget to handle edge cases (zero vectors, very short texts)\",\n",
    "        \"Don't rely solely on cosine similarity for complex NLP tasks\"\n",
    "    ],\n",
    "    \"üí° Tips\": [\n",
    "        \"For hate speech: Use specialized models like cardiffnlp/twitter-roberta-base-hate-latest\",\n",
    "        \"Combine cosine similarity with other features for robust classification\",\n",
    "        \"Use approximate nearest neighbor search (FAISS) for large-scale similarity\",\n",
    "        \"Monitor performance: embeddings generation is often the bottleneck\",\n",
    "        \"Consider fine-tuning embeddings on your specific domain\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"   ‚Ä¢ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "\n",
    "- **Cosine Similarity Formula**: Measures the cosine of the angle between vectors, focusing on direction rather than magnitude\n",
    "  $$\\text{cosine\\_similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}$$\n",
    "\n",
    "- **2D Visualization**: Demonstrated how cosine similarity works with visual examples in 2D space\n",
    "  - Similar direction vectors ‚Üí High similarity (close to 1.0)\n",
    "  - Orthogonal vectors ‚Üí Zero similarity\n",
    "  - Opposite direction vectors ‚Üí Negative similarity (close to -1.0)\n",
    "\n",
    "- **High-Dimensional Computation**: Applied cosine similarity to vectors in 384+ dimensions\n",
    "  - Random vectors in high dimensions tend to be nearly orthogonal\n",
    "  - Efficient computation using sklearn.metrics.pairwise.cosine_similarity\n",
    "\n",
    "- **Text Embeddings with HuggingFace**: Used transformer models to convert text into semantic vectors\n",
    "  - Loaded sentence-transformers/all-MiniLM-L6-v2 for 384-dimensional embeddings\n",
    "  - Computed similarity matrices to compare multiple texts\n",
    "  - Visualized results with heatmaps for interpretability\n",
    "\n",
    "- **Practical Applications**:\n",
    "  - **Semantic Search**: Finding relevant documents based on meaning, not just keywords\n",
    "  - **Content Moderation**: Identifying similar toxic/hate speech patterns\n",
    "  - **Clustering**: Grouping similar texts together\n",
    "  - **Recommendation**: Finding similar content based on text descriptions\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "\n",
    "- **Scale-Invariance**: Cosine similarity is perfect for text because it ignores frequency/length\n",
    "- **Efficiency**: Use sklearn.metrics.pairwise.cosine_similarity for optimized computations\n",
    "- **Normalization**: For normalized vectors, cosine similarity equals dot product\n",
    "- **Domain-Specific Models**: Use specialized models (e.g., hate speech detectors) for better embeddings\n",
    "- **Visualization**: Heatmaps help understand patterns in similarity matrices\n",
    "- **Reproducibility**: Always use seed=16 for consistent results (repository standard)\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Advanced Similarity**: Explore other similarity metrics (Euclidean, Manhattan, Jaccard)\n",
    "- **Large-Scale Search**: Learn FAISS for efficient similarity search on millions of vectors\n",
    "- **Fine-tuning**: Customize embeddings for your specific domain or task\n",
    "- **Hate Speech Detection**: Apply these concepts to hate speech classification models\n",
    "- **Documentation**: Check out [FAISS.md](../../docs/FAISS.md) for efficient similarity search\n",
    "\n",
    "### üìö Related Resources\n",
    "\n",
    "- **HF Transformer Trove**: [Feature Extraction Notebook](../basic1.2/06-feature-extraction.ipynb)\n",
    "- **Documentation**: [FAISS for Similarity Search](../../docs/FAISS.md)\n",
    "- **HuggingFace**: [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- **Research**: [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
