{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/semantic-search.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/semantic-search.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.6/semantic-search.ipynb)\n",
    "\n",
    "# Semantic Search with FAISS and HuggingFace\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How semantic search differs from traditional keyword-based search\n",
    "- How to generate text embeddings using transformer models\n",
    "- How to use FAISS for efficient similarity search\n",
    "- How to build a practical semantic search system\n",
    "- How to evaluate and interpret semantic search results\n",
    "\n",
    "## 📋 Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of transformers and embeddings\n",
    "\n",
    "## 📚 What We'll Cover\n",
    "1. **Introduction to Semantic Search**: Understanding context-aware search\n",
    "2. **Environment Setup**: Installing dependencies and configuring device\n",
    "3. **Loading and Preparing Dataset**: Working with the GitHub issues dataset\n",
    "4. **Creating Text Embeddings**: Using sentence transformers for encoding\n",
    "5. **Building FAISS Index**: Setting up efficient similarity search\n",
    "6. **Performing Semantic Search**: Querying the index with natural language\n",
    "7. **Evaluating Results**: Analyzing search quality and relevance\n",
    "8. **Summary and Best Practices**: Key takeaways and optimization tips\n",
    "\n",
    "## 💡 Reference\n",
    "This notebook is based on the [HuggingFace LLM Course Chapter 5.6](https://huggingface.co/learn/llm-course/chapter5/6?fw=pt) about semantic search with FAISS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Semantic Search\n",
    "\n",
    "### What is Semantic Search?\n",
    "\n",
    "**Semantic search** uses deep learning models to understand the *meaning* of text rather than just matching keywords. This enables:\n",
    "\n",
    "- **Context-aware retrieval**: Find documents with similar meaning even with different wording\n",
    "- **Better user experience**: Users can search naturally without exact keyword matching\n",
    "- **Cross-lingual search**: Find relevant content across languages\n",
    "- **Question answering**: Retrieve passages that answer questions, not just contain keywords\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Encode corpus**: Convert all documents to dense vector representations (embeddings)\n",
    "2. **Index vectors**: Store embeddings in FAISS for fast similarity search\n",
    "3. **Encode query**: Transform user query into same embedding space\n",
    "4. **Find similar**: Use FAISS to retrieve most similar documents based on vector similarity\n",
    "\n",
    "### Traditional vs Semantic Search\n",
    "\n",
    "**Traditional (Keyword-based)**:\n",
    "- Query: \"python error handling\"\n",
    "- Matches: Documents containing exact words \"python\", \"error\", \"handling\"\n",
    "\n",
    "**Semantic Search**:\n",
    "- Query: \"python error handling\"\n",
    "- Matches: Documents about exception management, try-catch blocks, debugging in Python\n",
    "- Can find: \"How to catch exceptions in Python\", \"Debugging Python code\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set random seeds for reproducibility (repository standard: seed=16)\n",
    "random.seed(16)\n",
    "np.random.seed(16)\n",
    "torch.manual_seed(16)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(16)\n",
    "    torch.cuda.manual_seed_all(16)\n",
    "\n",
    "print(\"🔢 Random seed set to 16 for reproducibility (repository standard)\")\n",
    "\n",
    "# Configure visualization style (repository standard)\n",
    "sns.set_style('darkgrid')  # Better readability with gridlines\n",
    "sns.set_palette(\"husl\")     # Consistent, accessible colors\n",
    "print(\"📊 Visualization style configured: darkgrid with husl palette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection with TPU support for Google Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    COLAB_AVAILABLE = True\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Device Priority:\n",
    "    - General: CUDA GPU > TPU (Colab only) > MPS (Apple Silicon) > CPU\n",
    "    - Google Colab: Always prefer TPU when available\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    # Google Colab: Always prefer TPU when available\n",
    "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
    "        try:\n",
    "            device = xm.xla_device()\n",
    "            print(\"🔥 Using Google Colab TPU for optimal performance\")\n",
    "            print(\"💡 TPU is preferred in Colab for training and inference\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ TPU initialization failed: {e}\")\n",
    "            print(\"Falling back to GPU/CPU detection\")\n",
    "    \n",
    "    # Standard device detection for other environments\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"🚀 Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"🍎 Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"💻 Using CPU (consider GPU/TPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install required packages\n",
    "print(\"📦 Checking required packages...\")\n",
    "\n",
    "# Check if FAISS is installed\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"✅ FAISS is already installed\")\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ FAISS not found. Installing...\")\n",
    "    print(\"💡 Run: pip install faiss-cpu\")\n",
    "    print(\"💡 Or for GPU: pip install faiss-gpu\")\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "if FAISS_AVAILABLE:\n",
    "    print(f\"📊 FAISS version: {faiss.__version__ if hasattr(faiss, '__version__') else 'Unknown'}\")\n",
    "    print(f\"🔧 FAISS available indices: IndexFlatL2, IndexFlatIP, IndexIVFFlat, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading and Preparing the Dataset\n",
    "\n",
    "We'll use the `lewtun/github-issues` dataset, which contains GitHub issues from various repositories. This is perfect for demonstrating semantic search as users often search for issues using natural language descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GitHub issues dataset\n",
    "print(\"📥 Loading GitHub issues dataset...\")\n",
    "print(\"📍 Dataset: lewtun/github-issues\")\n",
    "print(\"💡 This dataset contains GitHub issues from various repositories\\n\")\n",
    "\n",
    "try:\n",
    "    # Load the full dataset\n",
    "    github_issues = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "    \n",
    "    print(f\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"📊 Total examples: {len(github_issues):,}\")\n",
    "    print(f\"📋 Features: {list(github_issues.features.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    print(\"💡 Make sure you have internet connection and the datasets library is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"🔍 Dataset Structure Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to pandas for easier exploration\n",
    "df = github_issues.to_pandas()\n",
    "\n",
    "print(f\"\\n📊 Dataset shape: {df.shape}\")\n",
    "print(f\"\\n📋 Columns: {list(df.columns)}\")\n",
    "print(f\"\\n🔢 Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Display first few examples\n",
    "print(f\"\\n📝 Sample Issue:\")\n",
    "print(\"=\" * 60)\n",
    "sample_idx = 0\n",
    "sample = df.iloc[sample_idx]\n",
    "print(f\"Title: {sample['title'] if 'title' in df.columns else 'N/A'}\")\n",
    "print(f\"\\nBody: {sample['body'][:300] if 'body' in df.columns and pd.notna(sample['body']) else 'N/A'}...\")\n",
    "print(f\"\\nComments: {sample['comments'] if 'comments' in df.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for semantic search\n",
    "# We'll combine title and body for better context\n",
    "print(\"🔧 Preparing dataset for semantic search...\")\n",
    "\n",
    "def prepare_text_for_search(example):\n",
    "    \"\"\"\n",
    "    Combine title and body for semantic search.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'title' and 'body' fields\n",
    "        \n",
    "    Returns:\n",
    "        Example with added 'text' field\n",
    "    \"\"\"\n",
    "    # Get title\n",
    "    title = example.get('title', '')\n",
    "    if title is None:\n",
    "        title = ''\n",
    "    \n",
    "    # Get body\n",
    "    body = example.get('body', '')\n",
    "    if body is None:\n",
    "        body = ''\n",
    "    \n",
    "    # Combine title and body\n",
    "    # Title is more important, so we include it prominently\n",
    "    example['text'] = f\"{title}\\n{body}\"\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Apply preprocessing\n",
    "github_issues = github_issues.map(prepare_text_for_search)\n",
    "\n",
    "print(f\"✅ Text field created by combining title and body\")\n",
    "print(f\"📊 Sample combined text:\")\n",
    "print(f\"{github_issues[0]['text'][:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we'll use a subset of the data\n",
    "# This makes the notebook run faster while still showing the concepts\n",
    "print(\"🎲 Sampling dataset for faster processing...\")\n",
    "print(\"💡 Using repository standard seed=16 for reproducible sampling\\n\")\n",
    "\n",
    "# Sample 1000 issues for this demo (shuffle with seed=16)\n",
    "SAMPLE_SIZE = 1000\n",
    "github_issues_sample = github_issues.shuffle(seed=16).select(range(min(SAMPLE_SIZE, len(github_issues))))\n",
    "\n",
    "print(f\"✅ Sampled {len(github_issues_sample):,} issues for demonstration\")\n",
    "print(f\"🔢 Using seed=16 ensures reproducible results across runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating Text Embeddings\n",
    "\n",
    "Now we'll convert the text into dense vector embeddings using a sentence transformer model. These embeddings capture the semantic meaning of the text in a high-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence transformer model for embeddings\n",
    "print(\"📥 Loading sentence transformer model...\")\n",
    "print(\"🤖 Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"💡 This model is optimized for semantic similarity tasks\\n\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to optimal device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✅ Model loaded successfully\")\n",
    "print(f\"📱 Model on device: {device}\")\n",
    "print(f\"📊 Model max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Perform mean pooling on model output to get sentence embeddings.\n",
    "    \n",
    "    Mean pooling averages token embeddings, weighted by attention mask,\n",
    "    to produce a single vector representing the entire sentence.\n",
    "    \n",
    "    Args:\n",
    "        model_output: Output from transformer model\n",
    "        attention_mask: Attention mask indicating valid tokens\n",
    "        \n",
    "    Returns:\n",
    "        Sentence embeddings (batch_size x embedding_dim)\n",
    "    \"\"\"\n",
    "    # Extract token embeddings from model output\n",
    "    token_embeddings = model_output[0]  # Shape: (batch_size, seq_length, hidden_dim)\n",
    "    \n",
    "    # Expand attention mask to match token embeddings shape\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Sum token embeddings, weighted by attention mask\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    \n",
    "    # Calculate mean by dividing by number of valid tokens\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    return mean_embeddings\n",
    "\n",
    "def encode_texts(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode a list of texts into embeddings.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to encode\n",
    "        batch_size: Batch size for processing (larger = faster but more memory)\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embeddings (num_texts x embedding_dim)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches for memory efficiency\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding texts\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            # Apply mean pooling\n",
    "            embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"✅ Embedding functions defined\")\n",
    "print(\"📊 Functions: mean_pooling() and encode_texts()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all issues in our sample\n",
    "print(\"🔄 Generating embeddings for all issues...\")\n",
    "print(f\"📊 Processing {len(github_issues_sample):,} issues\")\n",
    "print(\"⏱️ This may take a few minutes depending on your hardware\\n\")\n",
    "\n",
    "# Extract texts from dataset\n",
    "texts = github_issues_sample['text']\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = encode_texts(texts, batch_size=32)\n",
    "\n",
    "print(f\"\\n✅ Embeddings generated successfully!\")\n",
    "print(f\"📊 Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"📏 Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"💾 Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building FAISS Index for Efficient Search\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search. We'll create an index that allows us to quickly find the most similar embeddings to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if FAISS is available, if not provide instructions\n",
    "if not FAISS_AVAILABLE:\n",
    "    print(\"⚠️ FAISS is not installed. Please install it to continue:\")\n",
    "    print(\"💡 Run: !pip install faiss-cpu\")\n",
    "    print(\"💡 Or for GPU: !pip install faiss-gpu\")\n",
    "else:\n",
    "    import faiss\n",
    "    \n",
    "    print(\"🏗️ Building FAISS index...\")\n",
    "    print(\"📊 Index type: IndexFlatIP (Inner Product for cosine similarity)\")\n",
    "    print(\"💡 IndexFlatIP is exact search - guarantees finding true nearest neighbors\\n\")\n",
    "    \n",
    "    # Get embedding dimension\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    \n",
    "    # Create FAISS index\n",
    "    # IndexFlatIP uses inner product (dot product) for similarity\n",
    "    # Since our embeddings are normalized, inner product = cosine similarity\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    # FAISS requires float32 format\n",
    "    embeddings_float32 = embeddings.astype('float32')\n",
    "    index.add(embeddings_float32)\n",
    "    \n",
    "    print(f\"✅ FAISS index built successfully!\")\n",
    "    print(f\"📊 Index contains {index.ntotal:,} vectors\")\n",
    "    print(f\"📏 Vector dimension: {embedding_dim}\")\n",
    "    print(f\"🔍 Index is ready for semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performing Semantic Search\n",
    "\n",
    "Now we can perform semantic search! We'll encode a query and find the most similar issues in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search to find most relevant issues.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with search results and scores\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = encode_texts([query], batch_size=1)\n",
    "    query_embedding_float32 = query_embedding.astype('float32')\n",
    "    \n",
    "    # Search the index\n",
    "    # Returns: scores (similarities) and indices of nearest neighbors\n",
    "    scores, indices = index.search(query_embedding_float32, k)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        result = {\n",
    "            'score': float(score),\n",
    "            'index': int(idx),\n",
    "            'title': github_issues_sample[int(idx)].get('title', 'N/A'),\n",
    "            'body': github_issues_sample[int(idx)].get('body', 'N/A'),\n",
    "            'text': github_issues_sample[int(idx)]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_search_results(query: str, results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Display search results in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        results: List of search results\n",
    "    \"\"\"\n",
    "    print(\"🔍 SEMANTIC SEARCH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n📝 Query: '{query}'\")\n",
    "    print(f\"\\n🎯 Found {len(results)} most relevant issues:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{'─' * 80}\")\n",
    "        print(f\"\\n{i}. Similarity Score: {result['score']:.4f}\")\n",
    "        print(f\"   Title: {result['title']}\")\n",
    "        \n",
    "        # Show first 200 characters of body\n",
    "        body = result['body'] if result['body'] else ''\n",
    "        body_preview = body[:200] + '...' if len(body) > 200 else body\n",
    "        print(f\"   Body: {body_preview}\")\n",
    "        print()\n",
    "\n",
    "print(\"✅ Search functions defined\")\n",
    "print(\"📊 Functions: semantic_search() and display_search_results()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Search for training-related issues\n",
    "if FAISS_AVAILABLE:\n",
    "    query1 = \"How to train a model on custom dataset?\"\n",
    "    results1 = semantic_search(query1, k=5)\n",
    "    display_search_results(query1, results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for tokenization issues\n",
    "if FAISS_AVAILABLE:\n",
    "    query2 = \"Problems with tokenizer padding and truncation\"\n",
    "    results2 = semantic_search(query2, k=5)\n",
    "    display_search_results(query2, results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Search for GPU/memory issues\n",
    "if FAISS_AVAILABLE:\n",
    "    query3 = \"Out of memory error when using GPU\"\n",
    "    results3 = semantic_search(query3, k=5)\n",
    "    display_search_results(query3, results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search - Try your own queries!\n",
    "if FAISS_AVAILABLE:\n",
    "    print(\"🎮 Interactive Semantic Search\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n💡 Try searching with natural language queries!\")\n",
    "    print(\"💡 Examples:\")\n",
    "    print(\"   - 'How to save and load a trained model?'\")\n",
    "    print(\"   - 'Fine-tuning BERT for classification'\")\n",
    "    print(\"   - 'Error loading pretrained weights'\")\n",
    "    print(\"\\n📝 Uncomment the code below and add your query:\\n\")\n",
    "    \n",
    "    # Uncomment and modify the query below to search:\n",
    "    # custom_query = \"Your search query here\"\n",
    "    # custom_results = semantic_search(custom_query, k=3)\n",
    "    # display_search_results(custom_query, custom_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualizing Search Quality\n",
    "\n",
    "Let's analyze the quality of our semantic search by examining similarity scores and comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze similarity score distribution\n",
    "if FAISS_AVAILABLE:\n",
    "    print(\"📊 Analyzing semantic search quality...\\n\")\n",
    "    \n",
    "    # Collect similarity scores from multiple queries\n",
    "    test_queries = [\n",
    "        \"How to train a model?\",\n",
    "        \"Tokenization problems\",\n",
    "        \"GPU out of memory\",\n",
    "        \"Model loading error\",\n",
    "        \"Fine-tuning transformers\"\n",
    "    ]\n",
    "    \n",
    "    all_scores = []\n",
    "    for query in test_queries:\n",
    "        results = semantic_search(query, k=10)\n",
    "        scores = [r['score'] for r in results]\n",
    "        all_scores.extend(scores)\n",
    "    \n",
    "    # Plot similarity score distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_scores, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Similarity Score', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Distribution of Similarity Scores', fontsize=14, fontweight='bold')\n",
    "    plt.axvline(np.mean(all_scores), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(all_scores):.3f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(all_scores, vert=True)\n",
    "    plt.ylabel('Similarity Score', fontsize=12)\n",
    "    plt.title('Similarity Score Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n📈 Similarity Score Statistics:\")\n",
    "    print(f\"   Mean: {np.mean(all_scores):.4f}\")\n",
    "    print(f\"   Median: {np.median(all_scores):.4f}\")\n",
    "    print(f\"   Std Dev: {np.std(all_scores):.4f}\")\n",
    "    print(f\"   Min: {np.min(all_scores):.4f}\")\n",
    "    print(f\"   Max: {np.max(all_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare keyword vs semantic search\n",
    "if FAISS_AVAILABLE:\n",
    "    print(\"🔍 Comparing Keyword vs Semantic Search\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_query = \"model training fails\"\n",
    "    \n",
    "    print(f\"\\n📝 Query: '{comparison_query}'\\n\")\n",
    "    \n",
    "    # Semantic search results\n",
    "    print(\"🧠 SEMANTIC SEARCH (Understanding Meaning):\")\n",
    "    print(\"─\" * 80)\n",
    "    semantic_results = semantic_search(comparison_query, k=3)\n",
    "    for i, result in enumerate(semantic_results, 1):\n",
    "        print(f\"\\n{i}. Score: {result['score']:.4f}\")\n",
    "        print(f\"   Title: {result['title']}\")\n",
    "    \n",
    "    # Simple keyword matching (for comparison)\n",
    "    print(\"\\n\\n🔤 KEYWORD SEARCH (Exact Matching):\")\n",
    "    print(\"─\" * 80)\n",
    "    keywords = comparison_query.lower().split()\n",
    "    keyword_matches = []\n",
    "    \n",
    "    for idx, text in enumerate(github_issues_sample['text'][:100]):\n",
    "        text_lower = text.lower()\n",
    "        matches = sum(1 for kw in keywords if kw in text_lower)\n",
    "        if matches > 0:\n",
    "            keyword_matches.append((matches, idx))\n",
    "    \n",
    "    keyword_matches.sort(reverse=True)\n",
    "    \n",
    "    for i, (matches, idx) in enumerate(keyword_matches[:3], 1):\n",
    "        print(f\"\\n{i}. Keyword matches: {matches}\")\n",
    "        print(f\"   Title: {github_issues_sample[idx].get('title', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\\n💡 Key Difference:\")\n",
    "    print(\"   • Semantic search understands MEANING and context\")\n",
    "    print(\"   • Keyword search only finds EXACT word matches\")\n",
    "    print(\"   • Semantic search finds relevant results even with different wording\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced FAISS Index Types\n",
    "\n",
    "FAISS offers different index types optimized for different use cases. Let's explore some alternatives to `IndexFlatIP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAISS_AVAILABLE:\n",
    "    print(\"🏗️ Comparing Different FAISS Index Types\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # IndexFlatIP (Inner Product - Exact Search)\n",
    "    print(\"\\n1️⃣ IndexFlatIP (Current Index - Exact Search)\")\n",
    "    print(\"   • Uses inner product for similarity (cosine with normalized vectors)\")\n",
    "    print(\"   • Exact search: guarantees finding true nearest neighbors\")\n",
    "    print(\"   • Best for: Small to medium datasets (<1M vectors)\")\n",
    "    print(f\"   • Current index size: {index.ntotal:,} vectors\")\n",
    "    \n",
    "    # Test search speed\n",
    "    test_query = \"How to train a model?\"\n",
    "    query_embedding = encode_texts([test_query], batch_size=1).astype('float32')\n",
    "    \n",
    "    start = time.time()\n",
    "    scores, indices = index.search(query_embedding, 5)\n",
    "    flat_time = time.time() - start\n",
    "    print(f\"   • Search time: {flat_time*1000:.2f} ms\")\n",
    "    \n",
    "    # IndexIVFFlat (Inverted File Index - Approximate Search)\n",
    "    print(\"\\n2️⃣ IndexIVFFlat (Approximate Search with Clustering)\")\n",
    "    print(\"   • Uses clustering to partition the vector space\")\n",
    "    print(\"   • Approximate search: trades accuracy for speed\")\n",
    "    print(\"   • Best for: Large datasets (>100K vectors)\")\n",
    "    \n",
    "    # Create IVF index (only if dataset is large enough)\n",
    "    if len(github_issues_sample) >= 100:\n",
    "        nlist = min(100, len(github_issues_sample) // 10)  # Number of clusters\n",
    "        quantizer = faiss.IndexFlatIP(embedding_dim)\n",
    "        index_ivf = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "        \n",
    "        # Train the index (IVF requires training)\n",
    "        index_ivf.train(embeddings_float32)\n",
    "        index_ivf.add(embeddings_float32)\n",
    "        index_ivf.nprobe = 10  # Number of clusters to search\n",
    "        \n",
    "        start = time.time()\n",
    "        scores_ivf, indices_ivf = index_ivf.search(query_embedding, 5)\n",
    "        ivf_time = time.time() - start\n",
    "        \n",
    "        print(f\"   • Number of clusters: {nlist}\")\n",
    "        print(f\"   • Search time: {ivf_time*1000:.2f} ms\")\n",
    "        print(f\"   • Speedup: {flat_time/ivf_time:.2f}x faster\")\n",
    "    \n",
    "    print(\"\\n💡 Choosing the Right Index:\")\n",
    "    print(\"   • <10K vectors: IndexFlatIP (exact, fast enough)\")\n",
    "    print(\"   • 10K-1M vectors: IndexIVFFlat (good speed/accuracy balance)\")\n",
    "    print(\"   • >1M vectors: IndexIVFPQ (with quantization for compression)\")\n",
    "    print(\"   • GPU available: Use GPU indices for massive speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Saving and Loading FAISS Index\n",
    "\n",
    "For production use, you'll want to save your index and embeddings to avoid recomputing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAISS_AVAILABLE:\n",
    "    import os\n",
    "    import tempfile\n",
    "    \n",
    "    print(\"💾 Saving FAISS Index and Metadata\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create temporary directory for saving\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    index_path = os.path.join(temp_dir, \"semantic_search.index\")\n",
    "    embeddings_path = os.path.join(temp_dir, \"embeddings.npy\")\n",
    "    dataset_path = os.path.join(temp_dir, \"dataset_sample\")\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"✅ FAISS index saved to: {index_path}\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f\"✅ Embeddings saved to: {embeddings_path}\")\n",
    "    \n",
    "    # Save dataset sample\n",
    "    github_issues_sample.save_to_disk(dataset_path)\n",
    "    print(f\"✅ Dataset saved to: {dataset_path}\")\n",
    "    \n",
    "    print(f\"\\n📊 Saved files:\")\n",
    "    print(f\"   • Index size: {os.path.getsize(index_path) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   • Embeddings size: {os.path.getsize(embeddings_path) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Demonstrate loading\n",
    "    print(\"\\n📥 Loading FAISS Index and Metadata\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load index\n",
    "    loaded_index = faiss.read_index(index_path)\n",
    "    print(f\"✅ Index loaded: {loaded_index.ntotal:,} vectors\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    loaded_embeddings = np.load(embeddings_path)\n",
    "    print(f\"✅ Embeddings loaded: {loaded_embeddings.shape}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    from datasets import load_from_disk\n",
    "    loaded_dataset = load_from_disk(dataset_path)\n",
    "    print(f\"✅ Dataset loaded: {len(loaded_dataset):,} examples\")\n",
    "    \n",
    "    # Verify loaded index works\n",
    "    print(\"\\n🔍 Testing loaded index...\")\n",
    "    test_query = \"model training issues\"\n",
    "    query_emb = encode_texts([test_query], batch_size=1).astype('float32')\n",
    "    scores, indices = loaded_index.search(query_emb, 3)\n",
    "    \n",
    "    print(f\"✅ Loaded index is functional!\")\n",
    "    print(f\"📊 Top result: {loaded_dataset[int(indices[0][0])].get('title', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n💡 In production:\")\n",
    "    print(\"   • Save index after initial embedding generation\")\n",
    "    print(\"   • Load index at startup for instant search capability\")\n",
    "    print(\"   • Update index periodically as new data arrives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Best Practices and Performance Tips\n",
    "\n",
    "Let's discuss best practices for building production-ready semantic search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Semantic Search Best Practices\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "📚 1. MODEL SELECTION\n",
    "   ✅ Use sentence-transformers models for semantic similarity\n",
    "      • all-MiniLM-L6-v2: Fast, good quality (384 dim)\n",
    "      • all-mpnet-base-v2: Best quality (768 dim)\n",
    "      • multi-qa-MiniLM-L6-cos-v1: Optimized for Q&A\n",
    "   \n",
    "   ✅ Consider domain-specific models\n",
    "      • Use models fine-tuned for your domain (legal, medical, etc.)\n",
    "      • Test multiple models and evaluate on your data\n",
    "\n",
    "🔧 2. EMBEDDING GENERATION\n",
    "   ✅ Normalize embeddings for cosine similarity\n",
    "      • Use L2 normalization: F.normalize(embeddings, p=2, dim=1)\n",
    "      • Allows using IndexFlatIP for cosine similarity\n",
    "   \n",
    "   ✅ Batch processing for efficiency\n",
    "      • Process in batches to utilize GPU effectively\n",
    "      • Balance batch size with memory constraints\n",
    "   \n",
    "   ✅ Handle long texts properly\n",
    "      • Truncate or chunk texts longer than model max length\n",
    "      • Consider using sliding window for very long documents\n",
    "\n",
    "🏗️ 3. INDEX SELECTION\n",
    "   ✅ Choose index based on scale\n",
    "      • <10K: IndexFlatIP (exact, fast)\n",
    "      • 10K-1M: IndexIVFFlat (approximate, faster)\n",
    "      • >1M: IndexIVFPQ (with compression)\n",
    "   \n",
    "   ✅ Consider GPU acceleration\n",
    "      • Use faiss-gpu for large-scale applications\n",
    "      • Can achieve 10-100x speedup\n",
    "\n",
    "💾 4. DATA MANAGEMENT\n",
    "   ✅ Save and version your indices\n",
    "      • Avoid recomputing embeddings\n",
    "      • Track which model version created embeddings\n",
    "   \n",
    "   ✅ Implement incremental updates\n",
    "      • Add new vectors without rebuilding entire index\n",
    "      • Periodic full rebuilds for optimization\n",
    "\n",
    "🔍 5. SEARCH OPTIMIZATION\n",
    "   ✅ Tune number of results (k)\n",
    "      • Return more candidates than needed\n",
    "      • Apply post-filtering and re-ranking\n",
    "   \n",
    "   ✅ Implement result filtering\n",
    "      • Filter by metadata (date, category, etc.)\n",
    "      • Use hybrid search (combine with keyword search)\n",
    "   \n",
    "   ✅ Add query preprocessing\n",
    "      • Clean and normalize queries\n",
    "      • Handle typos and variants\n",
    "\n",
    "📊 6. EVALUATION AND MONITORING\n",
    "   ✅ Measure search quality\n",
    "      • Track precision@k and recall@k\n",
    "      • Collect user feedback on results\n",
    "   \n",
    "   ✅ Monitor performance\n",
    "      • Track search latency\n",
    "      • Monitor memory usage\n",
    "   \n",
    "   ✅ A/B test improvements\n",
    "      • Test different models and parameters\n",
    "      • Measure impact on user satisfaction\n",
    "\n",
    "🚀 7. PRODUCTION DEPLOYMENT\n",
    "   ✅ Implement caching\n",
    "      • Cache frequent queries\n",
    "      • Cache embeddings for common items\n",
    "   \n",
    "   ✅ Scale horizontally\n",
    "      • Shard index across multiple servers\n",
    "      • Use load balancing for queries\n",
    "   \n",
    "   ✅ Handle edge cases\n",
    "      • Empty queries\n",
    "      • Very short or very long queries\n",
    "      • Non-English text (if applicable)\n",
    "\n",
    "💡 8. ADVANCED TECHNIQUES\n",
    "   ✅ Hybrid search\n",
    "      • Combine semantic and keyword search\n",
    "      • Use BM25 + semantic for best results\n",
    "   \n",
    "   ✅ Re-ranking\n",
    "      • Use cross-encoder for final ranking\n",
    "      • Apply business logic and rules\n",
    "   \n",
    "   ✅ Query expansion\n",
    "      • Generate query variations\n",
    "      • Use synonyms and related terms\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Best practices overview complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📋 Summary\n",
    "\n",
    "### 🔑 Key Concepts Mastered\n",
    "- **Semantic Search**: Using embeddings to find semantically similar content, not just keyword matches\n",
    "- **Text Embeddings**: Converting text to dense vectors that capture semantic meaning using transformer models\n",
    "- **FAISS Indexing**: Building efficient similarity search indices for fast retrieval at scale\n",
    "- **Mean Pooling**: Aggregating token embeddings into sentence-level representations\n",
    "- **Cosine Similarity**: Measuring similarity between normalized embeddings using inner product\n",
    "\n",
    "### 📈 Best Practices Learned\n",
    "- Always normalize embeddings when using cosine similarity for search\n",
    "- Choose appropriate FAISS index type based on dataset size and accuracy requirements\n",
    "- Process texts in batches for efficient GPU utilization\n",
    "- Save and version indices to avoid recomputing embeddings\n",
    "- Use sentence-transformer models optimized for semantic similarity tasks\n",
    "- Set seed=16 for reproducible results (repository standard)\n",
    "- Configure visualization style with darkgrid for better readability\n",
    "\n",
    "### 🚀 Next Steps\n",
    "- **Advanced Indexing**: Explore IVF and HNSW indices for larger datasets\n",
    "- **Hybrid Search**: Combine semantic search with keyword-based BM25\n",
    "- **Cross-Encoders**: Use re-ranking models for improved search quality\n",
    "- **Domain Adaptation**: Fine-tune embedding models for specific domains\n",
    "- **Multi-modal Search**: Extend to images, videos, and other modalities\n",
    "\n",
    "### 📚 Further Reading\n",
    "- [FAISS Documentation](https://faiss.ai/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [HuggingFace Course - Semantic Search](https://huggingface.co/learn/llm-course/chapter5/6)\n",
    "- [Dense Passage Retrieval (DPR)](https://arxiv.org/abs/2004.04906)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- 🌐 **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- 💼 **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- 💻 **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
