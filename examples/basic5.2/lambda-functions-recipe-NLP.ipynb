{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv97pxCQ0Eja"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/lambda-functions-recipe-NLP.ipynb)\n",
        "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/lambda-functions-recipe-NLP.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/lambda-functions-recipe-NLP.ipynb)\n",
        "\n",
        "# Python Lambda Functions Recipe for NLP\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- How to use lambda functions effectively with Hugging Face datasets\n",
        "- Advanced lambda patterns for NLP data preprocessing\n",
        "- Lambda functions for filtering hate speech and offensive content\n",
        "- Performance considerations when using lambdas in NLP pipelines\n",
        "- Real-world applications with HF transformers and datasets\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Basic understanding of Python lambda functions\n",
        "- Familiarity with Hugging Face datasets library\n",
        "- Knowledge of NLP preprocessing concepts\n",
        "- Experience with PyTorch tensors (basic level)\n",
        "\n",
        "## üìö What We'll Cover\n",
        "1. **Dataset Filtering with Lambda**: Filter hate speech, spam, and quality issues\n",
        "2. **Data Preprocessing**: Text cleaning and tokenization with lambda functions\n",
        "3. **Hate Speech Detection Pipeline**: Advanced lambda patterns for classification\n",
        "4. **Performance Optimization**: Vectorized vs lambda approaches\n",
        "5. **Best Practices**: When to use lambdas vs regular functions in NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YTSd-P-s0Ejc",
        "outputId": "92683644-ebe2-4d30-a6b4-7c8a7e027001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¢ Random seed set to 16 for reproducibility\n",
            "üíª Using CPU - consider GPU/TPU for better performance\n",
            "\n",
            "=== Setup Information ===\n",
            "Device: cpu\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# Repository standard: Always use seed=16 for reproducible results\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from typing import Dict, List, Any, Optional\n",
        "from collections import Counter\n",
        "\n",
        "# Set all random seeds for reproducibility (repository standard: seed=16)\n",
        "def set_seed(seed_value: int = 16):\n",
        "    \"\"\"Set seed for reproducibility across all random number generators.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"üî¢ Random seed set to {seed_value} for reproducibility\")\n",
        "\n",
        "set_seed(16)\n",
        "\n",
        "# Core imports for NLP and datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TPU detection for Google Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    COLAB_AVAILABLE = True\n",
        "    TPU_AVAILABLE = True\n",
        "    print(\"üî• Google Colab with TPU support detected\")\n",
        "except ImportError:\n",
        "    COLAB_AVAILABLE = False\n",
        "    TPU_AVAILABLE = False\n",
        "\n",
        "# Device detection with TPU priority for Colab\n",
        "def get_device():\n",
        "    \"\"\"Get optimal device with TPU priority in Colab.\"\"\"\n",
        "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
        "        try:\n",
        "            device = xm.xla_device()\n",
        "            print(\"üî• Using Google Colab TPU for optimal performance\")\n",
        "            return device\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è TPU initialization failed: {e}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU - consider GPU/TPU for better performance\")\n",
        "\n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "print(f\"\\n=== Setup Information ===\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yp1-noL0Ejd"
      },
      "source": [
        "## Section 1: Dataset Filtering with Lambda Functions\n",
        "\n",
        "Lambda functions are particularly powerful for filtering datasets based on specific criteria. In NLP, we often need to filter out poor quality data, missing values, or content that doesn't meet our requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s4FTR4_90Ejd",
        "outputId": "1ced12ca-2ce0-4fc7-81c4-f0a7e3f56dd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ°Ô∏è DATASET FILTERING WITH LAMBDA FUNCTIONS\n",
            "==================================================\n",
            "üì• Loading dataset: tdavidson/hate_speech_offensive\n",
            "‚úÖ Loaded 5000 training examples\n",
            "üìã Features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'class', 'tweet']\n",
            "\n",
            "üìä Original label distribution:\n",
            "  1 (offensive language): 3,919 examples\n",
            "  0 (hate speech): 268 examples\n",
            "  2 (neither): 813 examples\n"
          ]
        }
      ],
      "source": [
        "print(\"üõ°Ô∏è DATASET FILTERING WITH LAMBDA FUNCTIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load preferred hate speech dataset (repository standard)\n",
        "dataset_name = \"tdavidson/hate_speech_offensive\"\n",
        "print(f\"üì• Loading dataset: {dataset_name}\")\n",
        "\n",
        "try:\n",
        "    # Load full dataset\n",
        "    full_dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get a sample with repository seed=16\n",
        "    train_data = full_dataset[\"train\"].shuffle(seed=16).select(range(5000))\n",
        "    print(f\"‚úÖ Loaded {len(train_data)} training examples\")\n",
        "    print(f\"üìã Features: {list(train_data.features.keys())}\")\n",
        "\n",
        "    # Display label distribution\n",
        "    labels = train_data['class']\n",
        "    label_counts = Counter(labels)\n",
        "    print(f\"\\nüìä Original label distribution:\")\n",
        "    for label, count in label_counts.items():\n",
        "        label_name = [\"hate speech\", \"offensive language\", \"neither\"][label]\n",
        "        print(f\"  {label} ({label_name}): {count:,} examples\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"üí° Creating synthetic dataset for demonstration\")\n",
        "\n",
        "    # Create synthetic dataset for demonstration\n",
        "    synthetic_texts = [\n",
        "        \"I hate this stupid product\",\n",
        "        \"This is amazing, I love it!\",\n",
        "        \"You're such an idiot\",\n",
        "        \"Great work on this project\",\n",
        "        \"Damn, this is good\",\n",
        "        \"That's really nice\",\n",
        "        \"What a terrible day\",\n",
        "        \"Beautiful weather today\",\n",
        "        \"\",  # Empty text\n",
        "        \"   \",  # Only whitespace\n",
        "        \"a\",  # Too short\n",
        "        \"This is a normal sentence with reasonable length\"\n",
        "    ] * 500  # Repeat to get decent sample size\n",
        "\n",
        "    synthetic_labels = [0, 1, 0, 1, 0, 1, 1, 1, 2, 2, 2, 2] * 500\n",
        "\n",
        "    train_data = Dataset.from_dict({\n",
        "        'tweet': synthetic_texts,\n",
        "        'class': synthetic_labels\n",
        "    })\n",
        "\n",
        "    print(f\"‚úÖ Created synthetic dataset with {len(train_data)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HeuzrUiW0Ejd",
        "outputId": "f193cb97-5fce-4535-ebba-c5d079585e7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç LAMBDA FILTERING PATTERNS\n",
            "==============================\n",
            "1Ô∏è‚É£ Filtering out None and empty values\n",
            "   Before: 5000 examples\n",
            "   After:  5000 examples\n",
            "   Removed: 0 empty/None examples\n",
            "\n",
            "2Ô∏è‚É£ Filtering by minimum text length (>= 10 characters)\n",
            "   Before: 5000 examples\n",
            "   After:  4995 examples\n",
            "   Removed: 5 too-short examples\n",
            "\n",
            "3Ô∏è‚É£ Filtering specific classes\n",
            "   Toxic content only: 4183 examples\n",
            "   Neutral content only: 812 examples\n",
            "\n",
            "4Ô∏è‚É£ Advanced pattern filtering\n",
            "   Before quality filter: 4995 examples\n",
            "   After quality filter:  4938 examples\n",
            "   Removed: 57 low-quality examples\n",
            "\n",
            "5Ô∏è‚É£ Combined filtering with complex lambda\n",
            "   Original: 5000 examples\n",
            "   After comprehensive filter: 3672 examples\n",
            "   Retention rate: 73.4%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîç LAMBDA FILTERING PATTERNS\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Determine text column name\n",
        "text_column = 'tweet' if 'tweet' in train_data.features else 'text'\n",
        "\n",
        "# 1. Filter out None/empty values using lambda\n",
        "print(\"1Ô∏è‚É£ Filtering out None and empty values\")\n",
        "filtered_data = train_data.filter(lambda x: x[text_column] is not None and x[text_column].strip() != \"\")\n",
        "print(f\"   Before: {len(train_data)} examples\")\n",
        "print(f\"   After:  {len(filtered_data)} examples\")\n",
        "print(f\"   Removed: {len(train_data) - len(filtered_data)} empty/None examples\")\n",
        "\n",
        "# 2. Filter by text length using lambda\n",
        "print(\"\\n2Ô∏è‚É£ Filtering by minimum text length (>= 10 characters)\")\n",
        "length_filtered = filtered_data.filter(lambda x: len(x[text_column]) >= 10)\n",
        "print(f\"   Before: {len(filtered_data)} examples\")\n",
        "print(f\"   After:  {len(length_filtered)} examples\")\n",
        "print(f\"   Removed: {len(filtered_data) - len(length_filtered)} too-short examples\")\n",
        "\n",
        "# 3. Filter by specific labels using lambda\n",
        "print(\"\\n3Ô∏è‚É£ Filtering specific classes\")\n",
        "# Filter for hate speech and offensive language only (exclude neutral)\n",
        "toxic_content = length_filtered.filter(lambda x: x['class'] in [0, 1])\n",
        "print(f\"   Toxic content only: {len(toxic_content)} examples\")\n",
        "\n",
        "# Filter for neutral content only\n",
        "neutral_content = length_filtered.filter(lambda x: x['class'] == 2)\n",
        "print(f\"   Neutral content only: {len(neutral_content)} examples\")\n",
        "\n",
        "# 4. Advanced text pattern filtering using lambda\n",
        "print(\"\\n4Ô∏è‚É£ Advanced pattern filtering\")\n",
        "# Filter out texts with excessive punctuation or caps\n",
        "quality_filtered = length_filtered.filter(\n",
        "    lambda x: (\n",
        "        len([c for c in x[text_column] if c in '!?']) <= 3 and  # Max 3 exclamation/question marks\n",
        "        sum(1 for c in x[text_column] if c.isupper()) / len(x[text_column]) <= 0.7  # Max 70% uppercase\n",
        "    )\n",
        ")\n",
        "print(f\"   Before quality filter: {len(length_filtered)} examples\")\n",
        "print(f\"   After quality filter:  {len(quality_filtered)} examples\")\n",
        "print(f\"   Removed: {len(length_filtered) - len(quality_filtered)} low-quality examples\")\n",
        "\n",
        "# 5. Combine multiple conditions in a single lambda\n",
        "print(\"\\n5Ô∏è‚É£ Combined filtering with complex lambda\")\n",
        "comprehensive_filtered = train_data.filter(\n",
        "    lambda x: (\n",
        "        x[text_column] is not None and  # Not None\n",
        "        len(x[text_column].strip()) >= 10 and  # Minimum length\n",
        "        len(x[text_column].strip()) <= 280 and  # Maximum length (Twitter-like)\n",
        "        not x[text_column].strip().startswith('RT') and  # Not a retweet\n",
        "        x['class'] is not None  # Valid label\n",
        "    )\n",
        ")\n",
        "print(f\"   Original: {len(train_data)} examples\")\n",
        "print(f\"   After comprehensive filter: {len(comprehensive_filtered)} examples\")\n",
        "print(f\"   Retention rate: {len(comprehensive_filtered)/len(train_data)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8jNb5X0Ejd"
      },
      "source": [
        "---\n",
        "\n",
        "## üìã Summary\n",
        "\n",
        "### üîë Key Concepts Mastered\n",
        "- **Dataset Filtering**: Using lambda functions to filter datasets based on content, quality, and class labels\n",
        "- **Data Preprocessing**: Text cleaning, normalization, and feature extraction using lambda transformations\n",
        "- **NLP Pipeline Integration**: Combining lambda functions with tokenizers and hate speech detection models\n",
        "- **Performance Optimization**: Understanding when to use lambda vs vectorized operations for efficiency\n",
        "- **Error Handling**: Safe patterns for lambda functions in data processing pipelines\n",
        "\n",
        "### üìà Best Practices Learned\n",
        "- Use lambda for simple, single-line operations and regular functions for complex logic\n",
        "- Always handle edge cases with `.get()` methods and default values in lambda functions\n",
        "- Consider performance implications: vectorized operations are often faster than lambdas for large datasets\n",
        "- Test lambda functions on small samples before applying to large datasets\n",
        "- Cache expensive operations outside lambda functions to avoid repeated computations\n",
        "\n",
        "### üöÄ Next Steps\n",
        "- **Advanced Fine-tuning**: Apply these filtering techniques to create high-quality training datasets\n",
        "- **Custom Datasets**: Use lambda functions to create domain-specific datasets from raw text\n",
        "- **Production Pipelines**: Integrate these patterns into MLOps workflows for automated data processing\n",
        "- **Evaluation Metrics**: Use lambda functions for custom evaluation and analysis of model predictions\n",
        "\n",
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}