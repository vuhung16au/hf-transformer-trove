{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/working-with-datasets.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/working-with-datasets.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.2/working-with-datasets.ipynb)\n",
    "\n",
    "# Working with HF Datasets & External Datasets\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to load datasets from the Hugging Face Hub\n",
    "- How to load datasets from various file formats (Parquet, CSV, JSON, Pickle)\n",
    "- How to work with datasets that aren't on the Hub\n",
    "- Dataset processing and transformation techniques\n",
    "- Best practices for dataset handling in NLP projects\n",
    "\n",
    "## \ud83d\udccb Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and pandas\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## \ud83d\udcda What We'll Cover\n",
    "1. **HF Hub Datasets**: Loading datasets from Hugging Face Hub\n",
    "2. **Apache Parquet Format**: Loading efficient columnar data\n",
    "3. **Pickled DataFrames**: Loading Python pickle files\n",
    "4. **JSON & JSON Lines**: Loading JSON-based datasets\n",
    "5. **CSV & TSV Files**: Loading comma/tab-separated data\n",
    "6. **Dataset Processing**: Transforming and preparing data\n",
    "7. **Best Practices**: Tips for efficient dataset handling\n",
    "\n",
    "## \ud83d\udca1 Reference\n",
    "This notebook is based on the [HuggingFace Course Chapter 5.2](https://huggingface.co/learn/llm-course/chapter5/2?fw=pt) about working with datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    Dataset, \n",
    "    DatasetDict,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility (repository standard: seed=16)\n",
    "import random\n",
    "random.seed(16)\n",
    "np.random.seed(16)\n",
    "torch.manual_seed(16)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(16)\n",
    "    torch.cuda.manual_seed_all(16)\n",
    "\n",
    "print(\"\ud83d\udd22 Random seed set to 16 for reproducibility\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device detection (includes TPU support for Google Colab)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    COLAB_AVAILABLE = True\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    Priority in Colab: TPU > GPU > CPU\n",
    "    Priority elsewhere: GPU > MPS > CPU\n",
    "    \"\"\"\n",
    "    # Google Colab: Always prefer TPU when available\n",
    "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
    "        try:\n",
    "            device = xm.xla_device()\n",
    "            print(\"\ud83d\udd25 Using Google Colab TPU for optimal performance\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f TPU initialization failed: {e}\")\n",
    "    \n",
    "    # Standard device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"\ud83d\ude80 Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"\ud83c\udf4e Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"\ud83d\udcbb Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"\\n\u2705 Setup complete!\")\n",
    "print(f\"\ud83d\udce6 PyTorch version: {torch.__version__}\")\n",
    "print(f\"\ud83d\udd27 Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading Datasets from Hugging Face Hub\n",
    "\n",
    "The Hugging Face Hub hosts thousands of datasets that can be loaded with a single line of code.\n",
    "For hate speech detection (our preferred application area), we'll use recommended datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a hate speech detection dataset from HF Hub\n",
    "# Using the preferred dataset: tdavidson/hate_speech_offensive\n",
    "print(\"\ud83d\udee1\ufe0f Loading hate speech detection dataset from HF Hub...\")\n",
    "print(\"\ud83d\udce5 Dataset: tdavidson/hate_speech_offensive\")\n",
    "\n",
    "try:\n",
    "    # Load the full dataset\n",
    "    hf_dataset = load_dataset(\"tdavidson/hate_speech_offensive\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Dataset loaded successfully!\")\n",
    "    print(f\"\ud83d\udcca Dataset type: {type(hf_dataset)}\")\n",
    "    print(f\"\ud83d\udccb Available splits: {list(hf_dataset.keys())}\")\n",
    "    \n",
    "    # Examine the dataset structure\n",
    "    train_data = hf_dataset['train']\n",
    "    print(f\"\\n\ud83d\udd0d Dataset Information:\")\n",
    "    print(f\"  Total examples: {len(train_data):,}\")\n",
    "    print(f\"  Features: {list(train_data.features.keys())}\")\n",
    "    print(f\"  Feature types: {train_data.features}\")\n",
    "    \n",
    "    # Show a sample example\n",
    "    print(f\"\\n\ud83d\udcdd Sample example:\")\n",
    "    sample = train_data[0]\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading dataset: {e}\")\n",
    "    print(\"\ud83d\udca1 Try checking your internet connection or using a different dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Specific Splits or Subsets\n",
    "\n",
    "You can load specific splits or create custom subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only a specific split\n",
    "print(\"\ud83d\udce5 Loading specific split...\")\n",
    "train_only = load_dataset(\"tdavidson/hate_speech_offensive\", split=\"train\")\n",
    "print(f\"\u2705 Loaded train split: {len(train_only):,} examples\")\n",
    "\n",
    "# Load a subset using slicing\n",
    "print(\"\\n\ud83d\udce5 Loading a subset (first 1000 examples)...\")\n",
    "subset = load_dataset(\"tdavidson/hate_speech_offensive\", split=\"train[:1000]\")\n",
    "print(f\"\u2705 Loaded subset: {len(subset):,} examples\")\n",
    "\n",
    "# Load a percentage of the data\n",
    "print(\"\\n\ud83d\udce5 Loading 10% of the training data...\")\n",
    "percentage_subset = load_dataset(\"tdavidson/hate_speech_offensive\", split=\"train[:10%]\")\n",
    "print(f\"\u2705 Loaded 10% subset: {len(percentage_subset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: What if My Dataset Isn't on the Hub?\n",
    "\n",
    "Let's explore how to load datasets from various file formats when they're not available on the Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Sample Data for Demonstration\n",
    "\n",
    "First, let's create sample datasets in different formats to demonstrate loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for our example files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"\ud83d\udcc1 Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Create sample hate speech detection data\n",
    "sample_data = [\n",
    "    {\"text\": \"This is a normal tweet about the weather today\", \"label\": 2, \"class_name\": \"neither\"},\n",
    "    {\"text\": \"Stop spreading lies and misinformation!\", \"label\": 1, \"class_name\": \"offensive\"},\n",
    "    {\"text\": \"Beautiful day at the park with my family\", \"label\": 2, \"class_name\": \"neither\"},\n",
    "    {\"text\": \"Can't believe how terrible some people can be\", \"label\": 1, \"class_name\": \"offensive\"},\n",
    "    {\"text\": \"Looking forward to the weekend!\", \"label\": 2, \"class_name\": \"neither\"},\n",
    "    {\"text\": \"This behavior is completely unacceptable\", \"label\": 1, \"class_name\": \"offensive\"},\n",
    "    {\"text\": \"Just finished a great book, highly recommend it\", \"label\": 2, \"class_name\": \"neither\"},\n",
    "    {\"text\": \"Why do people keep doing such awful things?\", \"label\": 1, \"class_name\": \"offensive\"},\n",
    "]\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"\\n\ud83d\udcca Sample dataset created:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loading from Apache Parquet Format\n",
    "\n",
    "Apache Parquet is a columnar storage format that's highly efficient for large datasets.\n",
    "It's the preferred format for the Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet\n",
    "parquet_file = os.path.join(temp_dir, \"dataset.parquet\")\n",
    "df.to_parquet(parquet_file, index=False)\n",
    "print(f\"\ud83d\udcbe Saved dataset as Parquet: {parquet_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(parquet_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Load from Parquet using HF datasets\n",
    "print(\"\\n\ud83d\udce5 Loading from Parquet format...\")\n",
    "parquet_dataset = load_dataset('parquet', data_files=parquet_file)\n",
    "\n",
    "print(f\"\\n\u2705 Parquet dataset loaded successfully!\")\n",
    "print(f\"\ud83d\udccb Available splits: {list(parquet_dataset.keys())}\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(parquet_dataset['train']):,}\")\n",
    "print(f\"\ud83d\udd27 Features: {list(parquet_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n\ud83d\udcdd Sample from Parquet:\")\n",
    "print(parquet_dataset['train'][0])\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Pro Tip: Parquet is highly efficient for:\")\n",
    "print(\"   - Large datasets (GB-scale)\")\n",
    "print(\"   - Columnar operations\")\n",
    "print(\"   - Cloud storage (S3, GCS, etc.)\")\n",
    "print(\"   - Preserving data types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loading from Pickled DataFrames\n",
    "\n",
    "Pickle files are Python-specific serialization format. They're quick but not portable across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Pickle\n",
    "pickle_file = os.path.join(temp_dir, \"dataset.pkl\")\n",
    "df.to_pickle(pickle_file)\n",
    "print(f\"\ud83d\udcbe Saved dataset as Pickle: {pickle_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(pickle_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Load from Pickle\n",
    "print(\"\\n\ud83d\udce5 Loading from Pickle format...\")\n",
    "loaded_df = pd.read_pickle(pickle_file)\n",
    "\n",
    "# Convert pandas DataFrame to HF Dataset\n",
    "pickle_dataset = Dataset.from_pandas(loaded_df)\n",
    "\n",
    "print(f\"\\n\u2705 Pickle dataset loaded and converted successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(pickle_dataset):,}\")\n",
    "print(f\"\ud83d\udd27 Features: {list(pickle_dataset.features.keys())}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n\ud83d\udcdd Sample from Pickle:\")\n",
    "print(pickle_dataset[0])\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Warning about Pickle:\")\n",
    "print(\"   - Only works with Python\")\n",
    "print(\"   - Security risk: don't load untrusted pickle files\")\n",
    "print(\"   - Not recommended for production or sharing\")\n",
    "print(\"   - Consider using Parquet instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Loading from JSON & JSON Lines\n",
    "\n",
    "JSON (JavaScript Object Notation) is a popular, human-readable format.\n",
    "JSON Lines (.jsonl) stores one JSON object per line, making it efficient for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as regular JSON\n",
    "json_file = os.path.join(temp_dir, \"dataset.json\")\n",
    "df.to_json(json_file, orient='records', indent=2)\n",
    "print(f\"\ud83d\udcbe Saved dataset as JSON: {json_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(json_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Save as JSON Lines (one JSON object per line)\n",
    "jsonl_file = os.path.join(temp_dir, \"dataset.jsonl\")\n",
    "df.to_json(jsonl_file, orient='records', lines=True)\n",
    "print(f\"\\n\ud83d\udcbe Saved dataset as JSON Lines: {jsonl_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(jsonl_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Show the difference between formats\n",
    "print(\"\\n\ud83d\udcc4 Regular JSON format (first 300 chars):\")\n",
    "with open(json_file, 'r') as f:\n",
    "    print(f.read()[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc4 JSON Lines format (first 2 lines):\")\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 2:\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from JSON using HF datasets\n",
    "print(\"\ud83d\udce5 Loading from JSON format...\")\n",
    "json_dataset = load_dataset('json', data_files=json_file)\n",
    "\n",
    "print(f\"\\n\u2705 JSON dataset loaded successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(json_dataset['train']):,}\")\n",
    "print(f\"\ud83d\udd27 Features: {list(json_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Load from JSON Lines\n",
    "print(\"\\n\ud83d\udce5 Loading from JSON Lines format...\")\n",
    "jsonl_dataset = load_dataset('json', data_files=jsonl_file)\n",
    "\n",
    "print(f\"\\n\u2705 JSON Lines dataset loaded successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(jsonl_dataset['train']):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n\ud83d\udcdd Sample from JSON:\")\n",
    "print(jsonl_dataset['train'][0])\n",
    "\n",
    "print(\"\\n\ud83d\udca1 When to use JSON vs JSON Lines:\")\n",
    "print(\"   JSON: Small datasets, human readability, web APIs\")\n",
    "print(\"   JSON Lines: Large datasets, streaming, append operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Loading from CSV & TSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) and TSV (Tab-Separated Values) are simple, widely-supported formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "csv_file = os.path.join(temp_dir, \"dataset.csv\")\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"\ud83d\udcbe Saved dataset as CSV: {csv_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(csv_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Save as TSV (Tab-Separated Values)\n",
    "tsv_file = os.path.join(temp_dir, \"dataset.tsv\")\n",
    "df.to_csv(tsv_file, index=False, sep='\\t')\n",
    "print(f\"\\n\ud83d\udcbe Saved dataset as TSV: {tsv_file}\")\n",
    "print(f\"\ud83d\udccf File size: {os.path.getsize(tsv_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Show the difference between formats\n",
    "print(\"\\n\ud83d\udcc4 CSV format (first 3 lines):\")\n",
    "with open(csv_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(line.strip())\n",
    "\n",
    "print(\"\\n\ud83d\udcc4 TSV format (first 3 lines):\")\n",
    "with open(tsv_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from CSV using HF datasets\n",
    "print(\"\ud83d\udce5 Loading from CSV format...\")\n",
    "csv_dataset = load_dataset('csv', data_files=csv_file)\n",
    "\n",
    "print(f\"\\n\u2705 CSV dataset loaded successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(csv_dataset['train']):,}\")\n",
    "print(f\"\ud83d\udd27 Features: {list(csv_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Load from TSV\n",
    "print(\"\\n\ud83d\udce5 Loading from TSV format...\")\n",
    "tsv_dataset = load_dataset('csv', data_files=tsv_file, delimiter='\\t')\n",
    "\n",
    "print(f\"\\n\u2705 TSV dataset loaded successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(tsv_dataset['train']):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n\ud83d\udcdd Sample from CSV:\")\n",
    "print(csv_dataset['train'][0])\n",
    "\n",
    "print(\"\\n\ud83d\udca1 CSV/TSV Best Practices:\")\n",
    "print(\"   - Simple and widely supported\")\n",
    "print(\"   - Good for spreadsheet compatibility\")\n",
    "print(\"   - Watch out for special characters in text\")\n",
    "print(\"   - Consider using quotes for text fields with commas/tabs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loading Multiple Files\n",
    "\n",
    "Often datasets are split across multiple files. The datasets library makes it easy to load them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple CSV files\n",
    "print(\"\ud83d\udcc1 Creating multiple split files...\")\n",
    "\n",
    "# Split the data into train/validation/test\n",
    "train_df = df.iloc[:5]\n",
    "val_df = df.iloc[5:7]\n",
    "test_df = df.iloc[7:]\n",
    "\n",
    "# Save each split\n",
    "train_csv = os.path.join(temp_dir, \"train.csv\")\n",
    "val_csv = os.path.join(temp_dir, \"validation.csv\")\n",
    "test_csv = os.path.join(temp_dir, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_csv, index=False)\n",
    "val_df.to_csv(val_csv, index=False)\n",
    "test_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"\u2705 Created 3 split files:\")\n",
    "print(f\"   Train: {len(train_df)} examples\")\n",
    "print(f\"   Validation: {len(val_df)} examples\")\n",
    "print(f\"   Test: {len(test_df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all splits at once\n",
    "print(\"\ud83d\udce5 Loading multiple files as DatasetDict...\")\n",
    "\n",
    "data_files = {\n",
    "    'train': train_csv,\n",
    "    'validation': val_csv,\n",
    "    'test': test_csv\n",
    "}\n",
    "\n",
    "multi_file_dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "print(f\"\\n\u2705 Multiple files loaded successfully!\")\n",
    "print(f\"\ud83d\udccb Dataset type: {type(multi_file_dataset)}\")\n",
    "print(f\"\ud83d\udcca Available splits: {list(multi_file_dataset.keys())}\")\n",
    "\n",
    "for split_name, split_data in multi_file_dataset.items():\n",
    "    print(f\"\\n  {split_name}: {len(split_data)} examples\")\n",
    "    print(f\"    Features: {list(split_data.features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Format Comparison & Best Practices\n",
    "\n",
    "Let's compare the different formats and discuss when to use each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes\n",
    "formats = {\n",
    "    'Parquet': parquet_file,\n",
    "    'Pickle': pickle_file,\n",
    "    'JSON': json_file,\n",
    "    'JSON Lines': jsonl_file,\n",
    "    'CSV': csv_file,\n",
    "    'TSV': tsv_file\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udcca File Size Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sizes = []\n",
    "for format_name, file_path in formats.items():\n",
    "    size_kb = os.path.getsize(file_path) / 1024\n",
    "    sizes.append(size_kb)\n",
    "    print(f\"{format_name:12} : {size_kb:6.2f} KB\")\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(formats.keys(), sizes, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Format', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('File Size (KB)', fontsize=12, fontweight='bold')\n",
    "plt.title('Dataset Format File Size Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f} KB',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Note: File sizes vary based on:\")\n",
    "print(\"   - Compression algorithms\")\n",
    "print(\"   - Data types and structure\")\n",
    "print(\"   - Text encoding\")\n",
    "print(\"   - Metadata overhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format recommendation table\n",
    "recommendations = pd.DataFrame([\n",
    "    {\n",
    "        'Format': 'Parquet',\n",
    "        'Use Case': 'Large datasets, production, cloud storage',\n",
    "        'Pros': 'Efficient, columnar, preserves types',\n",
    "        'Cons': 'Requires special libraries',\n",
    "        'Speed': '\u2b50\u2b50\u2b50\u2b50\u2b50'\n",
    "    },\n",
    "    {\n",
    "        'Format': 'CSV',\n",
    "        'Use Case': 'Simple data, spreadsheet compatibility',\n",
    "        'Pros': 'Universal support, human-readable',\n",
    "        'Cons': 'No type preservation, inefficient',\n",
    "        'Speed': '\u2b50\u2b50\u2b50'\n",
    "    },\n",
    "    {\n",
    "        'Format': 'JSON Lines',\n",
    "        'Use Case': 'Streaming data, append operations',\n",
    "        'Pros': 'Streamable, one record per line',\n",
    "        'Cons': 'Larger file size than Parquet',\n",
    "        'Speed': '\u2b50\u2b50\u2b50\u2b50'\n",
    "    },\n",
    "    {\n",
    "        'Format': 'JSON',\n",
    "        'Use Case': 'Web APIs, small datasets',\n",
    "        'Pros': 'Human-readable, nested data support',\n",
    "        'Cons': 'Not efficient for large data',\n",
    "        'Speed': '\u2b50\u2b50'\n",
    "    },\n",
    "    {\n",
    "        'Format': 'Pickle',\n",
    "        'Use Case': 'Quick Python prototyping only',\n",
    "        'Pros': 'Fast for Python, preserves types',\n",
    "        'Cons': 'Python-only, security risk',\n",
    "        'Speed': '\u2b50\u2b50\u2b50\u2b50'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\ud83d\udccb Format Recommendations:\")\n",
    "print(\"=\" * 80)\n",
    "print(recommendations.to_string(index=False))\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Quick Decision Guide:\")\n",
    "print(\"   \u2705 Production/Large Data \u2192 Parquet\")\n",
    "print(\"   \u2705 Streaming/Logging \u2192 JSON Lines\")\n",
    "print(\"   \u2705 Spreadsheet/Simple \u2192 CSV\")\n",
    "print(\"   \u2705 Web APIs \u2192 JSON\")\n",
    "print(\"   \u26a0\ufe0f  Python Quick Test Only \u2192 Pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Dataset Processing & Transformation\n",
    "\n",
    "Once loaded, you'll often need to process and transform your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our example dataset\n",
    "dataset = csv_dataset['train']\n",
    "print(f\"\ud83d\udcca Working with dataset: {len(dataset)} examples\")\n",
    "print(f\"\ud83d\udd27 Original features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# 1. Filtering examples\n",
    "print(\"\\n\ud83d\udd0d Filtering: Keep only 'offensive' and 'neither' classes...\")\n",
    "filtered = dataset.filter(lambda x: x['label'] in [1, 2])\n",
    "print(f\"   Kept {len(filtered)} out of {len(dataset)} examples\")\n",
    "\n",
    "# 2. Mapping - Add new features\n",
    "print(\"\\n\ud83d\uddfa\ufe0f  Mapping: Adding text length feature...\")\n",
    "def add_text_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "dataset_with_length = dataset.map(add_text_length)\n",
    "print(f\"   New features: {list(dataset_with_length.features.keys())}\")\n",
    "print(f\"   Sample text length: {dataset_with_length[0]['text_length']}\")\n",
    "\n",
    "# 3. Batch processing for efficiency\n",
    "print(\"\\n\u26a1 Batch processing: Converting text to lowercase...\")\n",
    "def lowercase_batch(batch):\n",
    "    batch['text_lower'] = [text.lower() for text in batch['text']]\n",
    "    return batch\n",
    "\n",
    "dataset_lowercase = dataset.map(lowercase_batch, batched=True, batch_size=4)\n",
    "print(f\"   Processed in batches of 4\")\n",
    "print(f\"   Original: {dataset[0]['text'][:50]}...\")\n",
    "print(f\"   Lowercase: {dataset_lowercase[0]['text_lower'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Example\n",
    "\n",
    "Let's tokenize our dataset using a hate speech detection model tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for hate speech detection (preferred model)\n",
    "print(\"\ud83d\udd24 Loading tokenizer for hate speech detection...\")\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"\u2705 Loaded tokenizer: {model_name}\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text with truncation and padding.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"\\n\u26a1 Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=['text', 'class_name']  # Remove text columns after tokenization\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Tokenization complete!\")\n",
    "print(f\"\ud83d\udcca Tokenized features: {list(tokenized_dataset.features.keys())}\")\n",
    "print(f\"\\n\ud83d\udcdd Sample tokenized example:\")\n",
    "sample_tokenized = tokenized_dataset[0]\n",
    "print(f\"   Input IDs (first 10): {sample_tokenized['input_ids'][:10]}\")\n",
    "print(f\"   Attention mask (first 10): {sample_tokenized['attention_mask'][:10]}\")\n",
    "print(f\"   Label: {sample_tokenized['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Saving Processed Datasets\n",
    "\n",
    "After processing, you'll want to save your datasets for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(temp_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\ud83d\udcc1 Created output directory: {output_dir}\")\n",
    "\n",
    "# 1. Save as Arrow format (recommended by HF)\n",
    "arrow_path = os.path.join(output_dir, 'arrow_dataset')\n",
    "tokenized_dataset.save_to_disk(arrow_path)\n",
    "print(f\"\\n\ud83d\udcbe Saved as Arrow format: {arrow_path}\")\n",
    "print(f\"   This is the recommended format for HF datasets\")\n",
    "\n",
    "# 2. Save as Parquet\n",
    "parquet_path = os.path.join(output_dir, 'dataset.parquet')\n",
    "tokenized_dataset.to_parquet(parquet_path)\n",
    "print(f\"\\n\ud83d\udcbe Saved as Parquet: {parquet_path}\")\n",
    "\n",
    "# 3. Save as JSON\n",
    "json_path = os.path.join(output_dir, 'dataset.json')\n",
    "tokenized_dataset.to_json(json_path)\n",
    "print(f\"\\n\ud83d\udcbe Saved as JSON: {json_path}\")\n",
    "\n",
    "# 4. Save as CSV\n",
    "csv_path = os.path.join(output_dir, 'dataset.csv')\n",
    "tokenized_dataset.to_csv(csv_path)\n",
    "print(f\"\\n\ud83d\udcbe Saved as CSV: {csv_path}\")\n",
    "\n",
    "print(\"\\n\u2705 All formats saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading the saved dataset\n",
    "print(\"\ud83d\udce5 Loading saved Arrow dataset...\")\n",
    "reloaded_dataset = load_from_disk(arrow_path)\n",
    "\n",
    "print(f\"\\n\u2705 Reloaded successfully!\")\n",
    "print(f\"\ud83d\udcca Number of examples: {len(reloaded_dataset)}\")\n",
    "print(f\"\ud83d\udd27 Features: {list(reloaded_dataset.features.keys())}\")\n",
    "\n",
    "# Verify data integrity\n",
    "original_sample = tokenized_dataset[0]\n",
    "reloaded_sample = reloaded_dataset[0]\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Data integrity check:\")\n",
    "print(f\"   Input IDs match: {original_sample['input_ids'] == reloaded_sample['input_ids']}\")\n",
    "print(f\"   Labels match: {original_sample['label'] == reloaded_sample['label']}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Arrow format benefits:\")\n",
    "print(\"   \u2705 Fast loading and saving\")\n",
    "print(\"   \u2705 Preserves all data types\")\n",
    "print(\"   \u2705 Memory-mapped for large datasets\")\n",
    "print(\"   \u2705 Native format for HF datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Real-World Example - Combining Multiple Sources\n",
    "\n",
    "Let's combine datasets from different sources into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd17 Combining datasets from multiple sources...\\n\")\n",
    "\n",
    "# Source 1: CSV file\n",
    "source1 = load_dataset('csv', data_files=csv_file, split='train')\n",
    "print(f\"\ud83d\udce5 Source 1 (CSV): {len(source1)} examples\")\n",
    "\n",
    "# Source 2: JSON file  \n",
    "source2 = load_dataset('json', data_files=jsonl_file, split='train')\n",
    "print(f\"\ud83d\udce5 Source 2 (JSON Lines): {len(source2)} examples\")\n",
    "\n",
    "# Source 3: Parquet file\n",
    "source3 = load_dataset('parquet', data_files=parquet_file, split='train')\n",
    "print(f\"\ud83d\udce5 Source 3 (Parquet): {len(source3)} examples\")\n",
    "\n",
    "# Combine all sources\n",
    "combined_dataset = concatenate_datasets([source1, source2, source3])\n",
    "\n",
    "print(f\"\\n\ufffd\ufffd Combined dataset:\")\n",
    "print(f\"   Total examples: {len(combined_dataset)}\")\n",
    "print(f\"   Features: {list(combined_dataset.features.keys())}\")\n",
    "\n",
    "# Remove duplicates based on text (using repository seed=16)\n",
    "print(f\"\\n\ud83e\uddf9 Removing duplicates...\")\n",
    "# Note: unique() doesn't exist in older versions, so we'll use a workaround\n",
    "seen_texts = set()\n",
    "unique_indices = []\n",
    "for i, example in enumerate(combined_dataset):\n",
    "    if example['text'] not in seen_texts:\n",
    "        seen_texts.add(example['text'])\n",
    "        unique_indices.append(i)\n",
    "\n",
    "deduplicated = combined_dataset.select(unique_indices)\n",
    "print(f\"   After deduplication: {len(deduplicated)} examples\")\n",
    "print(f\"   Removed: {len(combined_dataset) - len(deduplicated)} duplicates\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Real-world scenario:\")\n",
    "print(\"   \u2705 Data from multiple teams/formats\")\n",
    "print(\"   \u2705 Combine for larger training set\")\n",
    "print(\"   \u2705 Deduplicate for data quality\")\n",
    "print(\"   \u2705 Reproducible with seed=16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Performance Tips & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a larger dataset for benchmarking\n",
    "large_data = sample_data * 100  # 800 examples\n",
    "large_df = pd.DataFrame(large_data)\n",
    "large_csv = os.path.join(temp_dir, \"large_dataset.csv\")\n",
    "large_df.to_csv(large_csv, index=False)\n",
    "\n",
    "print(\"\u26a1 Performance Comparison: Loading Methods\\n\")\n",
    "\n",
    "# Method 1: Load entire dataset\n",
    "start = time.time()\n",
    "full_load = load_dataset('csv', data_files=large_csv, split='train')\n",
    "full_load_time = time.time() - start\n",
    "print(f\"\ud83d\udce5 Full load: {full_load_time:.4f}s ({len(full_load)} examples)\")\n",
    "\n",
    "# Method 2: Load with streaming\n",
    "start = time.time()\n",
    "stream_load = load_dataset('csv', data_files=large_csv, split='train', streaming=True)\n",
    "# Take first 100 examples from stream\n",
    "stream_sample = []\n",
    "for i, example in enumerate(stream_load):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    stream_sample.append(example)\n",
    "stream_load_time = time.time() - start\n",
    "print(f\"\ud83d\udce5 Streaming (100 examples): {stream_load_time:.4f}s\")\n",
    "\n",
    "print(f\"\\n\u26a1 Speedup: {full_load_time/stream_load_time:.2f}x faster for partial data\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 When to use streaming:\")\n",
    "print(\"   \u2705 Dataset too large for memory\")\n",
    "print(\"   \u2705 Only need a subset of data\")\n",
    "print(\"   \u2705 Processing data in chunks\")\n",
    "print(\"   \u2705 Real-time data pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tips summary\n",
    "tips = [\n",
    "    (\"1. Use Parquet for large datasets\", \"Most efficient format for size and speed\"),\n",
    "    (\"2. Use batched=True in map()\", \"Process multiple examples at once\"),\n",
    "    (\"3. Use streaming for huge datasets\", \"Don't load everything into memory\"),\n",
    "    (\"4. Cache processed datasets\", \"Reuse tokenized data across runs\"),\n",
    "    (\"5. Remove unused columns\", \"Save memory after tokenization\"),\n",
    "    (\"6. Use seed=16 for reproducibility\", \"Repository standard for all random ops\"),\n",
    "    (\"7. Save to Arrow format\", \"Native HF format, fastest reload\"),\n",
    "    (\"8. Use num_proc for parallel processing\", \"Speed up map() operations\"),\n",
    "]\n",
    "\n",
    "print(\"\ud83c\udfaf Top Performance Tips & Best Practices\")\n",
    "print(\"=\" * 70)\n",
    "for tip, description in tips:\n",
    "    print(f\"\\n{tip}\")\n",
    "    print(f\"   \u2192 {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\u2705 Follow these practices for optimal dataset handling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"\ud83e\uddf9 Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Could not clean up temp directory: {e}\")\n",
    "\n",
    "print(\"\\n\u2705 Notebook execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udccb Summary\n",
    "\n",
    "### \ud83d\udd11 Key Concepts Mastered\n",
    "- **HF Hub Datasets**: Loading datasets directly from Hugging Face Hub with `load_dataset()`\n",
    "- **Multiple Formats**: Working with Parquet, CSV, TSV, JSON, JSON Lines, and Pickle formats\n",
    "- **Dataset Processing**: Filtering, mapping, and transforming datasets efficiently\n",
    "- **Tokenization**: Converting text to model-ready format with proper batching\n",
    "- **Data Persistence**: Saving and loading processed datasets in various formats\n",
    "- **Best Practices**: Performance optimization and reproducibility with seed=16\n",
    "\n",
    "### \ud83d\udcc8 Format Recommendations\n",
    "- **Production/Large Data** \u2192 Apache Parquet (most efficient)\n",
    "- **Streaming/Logging** \u2192 JSON Lines (one record per line)\n",
    "- **Simple/Spreadsheet** \u2192 CSV/TSV (universal compatibility)\n",
    "- **Web APIs** \u2192 JSON (standard for APIs)\n",
    "- **HF Native** \u2192 Arrow format (fastest for HF ecosystem)\n",
    "- **Quick Testing Only** \u2192 Pickle (Python-specific, security risk)\n",
    "\n",
    "### \ud83d\ude80 Next Steps\n",
    "- **Notebook 05**: Fine-tuning models with the Trainer API\n",
    "- **Documentation**: [HF Datasets Documentation](https://huggingface.co/docs/datasets)\n",
    "- **Course**: [HF Course Chapter 5](https://huggingface.co/learn/llm-course/chapter5)\n",
    "- **Practice**: Try loading your own datasets in different formats\n",
    "\n",
    "### \ud83d\udca1 Key Takeaways\n",
    "1. The `datasets` library provides a unified API for all data formats\n",
    "2. Parquet is the most efficient format for large-scale NLP datasets\n",
    "3. Use streaming for datasets too large for memory\n",
    "4. Always use `seed=16` for reproducibility (repository standard)\n",
    "5. Cache processed datasets to avoid redundant computation\n",
    "6. Batch processing is significantly faster than single-example processing\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- \ud83c\udf10 **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- \ud83d\udcbc **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- \ud83d\udcbb **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}