{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/10_llms_rlhf.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/10_llms_rlhf.ipynb)\n",
    "\n",
    "# 10 - LLMs RLHF: Reinforcement Learning from Human Feedback\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Reinforcement Learning from Human Feedback (RLHF) concepts\n",
    "- Training reward models from human preferences\n",
    "- Proximal Policy Optimization (PPO) for language models\n",
    "- Alignment techniques for large language models\n",
    "- Using TRL (Transformer Reinforcement Learning) library\n",
    "- Safety considerations and evaluation methods\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of transformers and fine-tuning\n",
    "- Understanding of reinforcement learning basics\n",
    "- Experience with advanced fine-tuning (refer to [Notebook 09](09_peft_lora_qlora.ipynb))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **RLHF Introduction**: Concepts and motivation\n",
    "2. **Reward Model Training**: Learning from human preferences\n",
    "3. **PPO Implementation**: Policy optimization for LLMs\n",
    "4. **TRL Library**: Practical RLHF implementation\n",
    "5. **Alignment Techniques**: Safety and helpfulness\n",
    "6. **Evaluation Methods**: Assessing aligned models\n",
    "7. **Advanced Topics**: Constitutional AI, RLAIF\n",
    "8. **Production Considerations**: Deployment and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning from Human Feedback\n",
    "\n",
    "RLHF is a technique used to align language models with human preferences and values:\n",
    "\n",
    "### The Challenge:\n",
    "- **Standard Training**: Models optimize for likelihood, not necessarily human preferences\n",
    "- **Alignment Problem**: Models may produce harmful, biased, or unhelpful outputs\n",
    "- **Evaluation Gap**: Traditional metrics don't capture human judgment\n",
    "\n",
    "### RLHF Process:\n",
    "1. **Supervised Fine-tuning (SFT)**: Train on high-quality demonstrations\n",
    "2. **Reward Model Training**: Learn to predict human preferences\n",
    "3. **PPO Fine-tuning**: Use RL to optimize for reward model scores\n",
    "\n",
    "### Key Components:\n",
    "- **Reward Model**: Predicts human preference scores\n",
    "- **Policy Model**: The language model being aligned\n",
    "- **PPO Algorithm**: Stabilizes RL training\n",
    "- **Human Feedback**: Preference comparisons between outputs\n",
    "\n",
    "### Applications:\n",
    "- **ChatGPT/GPT-4**: Aligned for helpfulness and safety\n",
    "- **Claude**: Constitutional AI approach\n",
    "- **LLaMA-2-Chat**: Open-source aligned model\n",
    "- **Custom Alignment**: Domain-specific preference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "# Try to import TRL components\n",
    "try:\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "    from trl.core import LengthSampler\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"‚úÖ TRL library available\")\n",
    "except ImportError:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TRL library not available - will use simplified examples\")\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device detection\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for RLHF efficiency)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(\"\\nüìö Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TRL available: {'‚úÖ' if TRL_AVAILABLE else '‚ùå - pip install trl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive final notebook, we explored Reinforcement Learning from Human Feedback:\n",
    "\n",
    "### üéØ **What We Accomplished**\n",
    "1. **RLHF Concepts**: Understanding alignment and human preference learning\n",
    "2. **Reward Models**: Training models to predict human preferences\n",
    "3. **PPO Training**: Using reinforcement learning for language model optimization\n",
    "4. **TRL Library**: Practical implementation of RLHF techniques\n",
    "5. **Safety Considerations**: Understanding alignment challenges and solutions\n",
    "6. **Evaluation Methods**: Assessing model helpfulness and safety\n",
    "7. **Advanced Topics**: Constitutional AI and AI-generated feedback\n",
    "\n",
    "### üîë **Key Concepts Mastered**\n",
    "- **Human Preference Learning**: Training models to align with human values\n",
    "- **Reward Modeling**: Predicting preference scores from human feedback\n",
    "- **Policy Optimization**: Using PPO to improve language model behavior\n",
    "- **Alignment Techniques**: Methods for creating helpful, harmless, and honest AI\n",
    "- **Safety Considerations**: Understanding and mitigating risks in AI systems\n",
    "\n",
    "### üìà **Best Practices Learned**\n",
    "- **Quality Feedback**: Importance of diverse, high-quality human feedback\n",
    "- **Reward Model Validation**: Ensuring reward models capture true preferences\n",
    "- **Training Stability**: Techniques for stable RLHF training\n",
    "- **Safety Evaluation**: Comprehensive testing for aligned models\n",
    "- **Continuous Improvement**: Iterative refinement of alignment techniques\n",
    "\n",
    "### üöÄ **Journey Complete!**\n",
    "Congratulations! You've completed the HF Transformer Trove learning journey:\n",
    "- **Notebooks 01-04**: Foundation concepts and integration\n",
    "- **Notebooks 05-07**: Fine-tuning and specialized applications\n",
    "- **Notebooks 08-10**: Advanced techniques and cutting-edge methods\n",
    "\n",
    "### üéì **Skills Mastered Throughout the Series**\n",
    "- **HuggingFace Ecosystem**: Confident usage of transformers, datasets, tokenizers\n",
    "- **Model Fine-tuning**: From basic to advanced techniques (LoRA, QLoRA)\n",
    "- **Production Systems**: Building robust, scalable ML applications\n",
    "- **Advanced NLP**: Question answering, summarization, alignment\n",
    "- **Cutting-edge Techniques**: PEFT and RLHF for modern AI development\n",
    "\n",
    "### üåü **Continue Your AI Journey**\n",
    "- **Documentation**: Explore the comprehensive docs for deeper understanding\n",
    "- **Community**: Join HuggingFace community and contribute to open source\n",
    "- **Research**: Stay updated with latest developments in AI alignment and safety\n",
    "- **Applications**: Apply these techniques to solve real-world problems\n",
    "\n",
    "RLHF represents the frontier of AI alignment, making models more helpful, harmless, and honest. These techniques are essential for responsible AI development!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing the HF Transformer Trove educational series! You're now equipped with state-of-the-art NLP and AI alignment techniques.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}