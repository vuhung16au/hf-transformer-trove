{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/05_fine_tuning_trainer.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/05_fine_tuning_trainer.ipynb)\n",
    "\n",
    "# 05 - Fine-Tuning with Trainer API\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to fine-tune pre-trained models using the Trainer API\n",
    "- Setting up training arguments and configurations\n",
    "- Implementing custom evaluation metrics\n",
    "- Monitoring training progress with callbacks\n",
    "- Saving and loading fine-tuned models\n",
    "- Best practices for fine-tuning\n",
    "\n",
    "The Trainer API is Hugging Face's high-level interface for training and fine-tuning transformer models. It handles most of the complexity while providing flexibility for customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from evaluate import load as load_metric\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env.local loading\")\n",
    "\n",
    "# Credential management function\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Get API key from environment or Colab secrets.\"\"\"\n",
    "    try:\n",
    "        # Try to import Colab userdata (only available in Colab)\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(key_name)\n",
    "    except (ImportError, Exception):\n",
    "        # Fall back to local environment variable\n",
    "        api_key = os.getenv(key_name)\n",
    "        if not api_key:\n",
    "            print(f\"Warning: {key_name} not found. Some features may be limited.\")\n",
    "            print(f\"   For Colab: Add {key_name} to Colab secrets\")\n",
    "            print(f\"   For local use: Add {key_name} to .env.local\")\n",
    "            return None\n",
    "        return api_key\n",
    "\n",
    "# Device detection function\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for training/inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup authentication and device\n",
    "hf_token = get_api_key('HF_TOKEN')\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"Hugging Face token configured\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"\\n=== Setup Information ===\")\n",
    "print(f\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif device.type == 'mps':\n",
    "    print(\"Apple Silicon GPU (MPS) detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation for Fine-tuning\n",
    "\n",
    "Let's prepare a dataset for fine-tuning. We'll use a sentiment analysis task as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use smaller subset for demonstration\n",
    "train_size = 2500\n",
    "eval_size = 500\n",
    "\n",
    "small_train_dataset = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
    "small_eval_dataset = dataset['test'].shuffle(seed=42).select(range(eval_size))\n",
    "\n",
    "print(f\"Training examples: {len(small_train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(small_eval_dataset)}\")\n",
    "\n",
    "# Examine the data\n",
    "example = small_train_dataset[0]\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Text: {example['text'][:200]}...\")\n",
    "print(f\"Label: {example['label']} ({'positive' if example['label'] == 1 else 'negative'})\")\n",
    "\n",
    "# Check label distribution\n",
    "from collections import Counter\n",
    "train_labels = [ex['label'] for ex in small_train_dataset]\n",
    "eval_labels = [ex['label'] for ex in small_eval_dataset]\n",
    "\n",
    "print(f\"\\nTraining label distribution: {Counter(train_labels)}\")\n",
    "print(f\"Evaluation label distribution: {Counter(eval_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model for fine-tuning\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "num_labels = 2  # Binary classification\n",
    "\n",
    "print(f\"Loading model and tokenizer: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model for classification with error handling\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "        label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "    )\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"âœ“ Model loaded successfully with {model.num_parameters():,} parameters\")\n",
    "    print(f\"âœ“ Model moved to device: {device}\")\n",
    "    print(f\"\\nModel configuration:\")\n",
    "    print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "    print(f\"  Number of attention heads: {model.config.num_attention_heads}\")\n",
    "    print(f\"  Number of layers: {model.config.num_hidden_layers}\")\n",
    "    print(f\"  Max position embeddings: {model.config.max_position_embeddings}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check if we need to add a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ“ Added pad token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare text for training\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,  # We'll use dynamic padding\n",
    "        max_length=256  # Reduced for faster training\n",
    "    )\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = small_train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text to save memory\n",
    ")\n",
    "\n",
    "tokenized_eval = small_eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "# Rename label column to match trainer expectations\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(f\"Tokenized training features: {tokenized_train.features}\")\n",
    "print(f\"Example tokenized input: {tokenized_train[0]}\")\n",
    "\n",
    "# Analyze token length distribution\n",
    "token_lengths = [len(ex['input_ids']) for ex in tokenized_train]\n",
    "print(f\"\\nToken length statistics:\")\n",
    "print(f\"  Mean: {np.mean(token_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(token_lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(token_lengths, 95):.1f}\")\n",
    "print(f\"  Max: {max(token_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation Metrics Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted classes\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Test the metrics function\n",
    "print(\"Testing metrics computation...\")\n",
    "dummy_predictions = np.array([[0.3, 0.7], [0.8, 0.2], [0.1, 0.9]])\n",
    "dummy_labels = np.array([1, 0, 1])\n",
    "test_metrics = compute_metrics((dummy_predictions, dummy_labels))\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments with device-aware settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Device-aware settings\n",
    "    seed=42,\n",
    "    fp16=(device.type == 'cuda'),  # Use mixed precision only on CUDA\n",
    "    bf16=(device.type == 'cuda' and torch.cuda.is_bf16_supported()),  # Use bfloat16 if available\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for this demo\n",
    "    \n",
    "    # For demonstration - remove if you want full training\n",
    "    max_steps=1000,  # Limit steps for demo\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training Configuration ===\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (train): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  Mixed precision (fp16): {training_args.fp16}\")\n",
    "print(f\"  BFloat16: {training_args.bf16}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Custom Callbacks and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom callback for monitoring\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to monitor training progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.eval_accuracies = []\n",
    "        self.learning_rates = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Called when logging occurs\"\"\"\n",
    "        if logs is not None:\n",
    "            if 'loss' in logs:\n",
    "                self.training_losses.append(logs['loss'])\n",
    "            if 'eval_loss' in logs:\n",
    "                self.eval_losses.append(logs['eval_loss'])\n",
    "            if 'eval_accuracy' in logs:\n",
    "                self.eval_accuracies.append(logs['eval_accuracy'])\n",
    "            if 'learning_rate' in logs:\n",
    "                self.learning_rates.append(logs['learning_rate'])\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, model=None, eval_dataloader=None, **kwargs):\n",
    "        \"\"\"Called after evaluation\"\"\"\n",
    "        print(f\"Evaluation completed at step {state.global_step}\")\n",
    "    \n",
    "    def get_training_history(self):\n",
    "        \"\"\"Return training history for visualization\"\"\"\n",
    "        return {\n",
    "            'training_losses': self.training_losses,\n",
    "            'eval_losses': self.eval_losses,\n",
    "            'eval_accuracies': self.eval_accuracies,\n",
    "            'learning_rates': self.learning_rates\n",
    "        }\n",
    "\n",
    "# Initialize custom callback\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "# Set up data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Custom callback and data collator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Initialize Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        custom_callback,\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(f\"Device: {trainer.args.device}\")\n",
    "\n",
    "# Run initial evaluation\n",
    "print(\"\\nRunning initial evaluation...\")\n",
    "initial_eval = trainer.evaluate()\n",
    "print(f\"Initial evaluation results:\")\n",
    "for key, value in initial_eval.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Print training results\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    print(f\"Training steps: {train_result.metrics['train_steps']}\")\n",
    "    print(f\"Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"This might be due to memory constraints or missing dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "final_eval = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nFinal evaluation results:\")\n",
    "for key, value in final_eval.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Compare initial vs final performance\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"  Accuracy: {initial_eval['eval_accuracy']:.4f} â†’ {final_eval['eval_accuracy']:.4f} (Î”{final_eval['eval_accuracy'] - initial_eval['eval_accuracy']:+.4f})\")\n",
    "print(f\"  F1 Score: {initial_eval['eval_f1']:.4f} â†’ {final_eval['eval_f1']:.4f} (Î”{final_eval['eval_f1'] - initial_eval['eval_f1']:+.4f})\")\n",
    "print(f\"  Loss: {initial_eval['eval_loss']:.4f} â†’ {final_eval['eval_loss']:.4f} (Î”{final_eval['eval_loss'] - initial_eval['eval_loss']:+.4f})\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(tokenized_eval)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Fine-tuning Results', fontsize=16)\n",
    "\n",
    "# Training history (if available)\n",
    "history = custom_callback.get_training_history()\n",
    "if history['training_losses']:\n",
    "    axes[0, 0].plot(history['training_losses'], label='Training Loss')\n",
    "    if history['eval_losses']:\n",
    "        # Align eval losses with training steps\n",
    "        eval_steps = np.linspace(0, len(history['training_losses']), len(history['eval_losses']))\n",
    "        axes[0, 0].plot(eval_steps, history['eval_losses'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy over time\n",
    "if history['eval_accuracies']:\n",
    "    axes[0, 1].plot(history['eval_accuracies'], 'g-o')\n",
    "    axes[0, 1].set_title('Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Evaluation Steps')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Confusion Matrix')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# Learning rate schedule\n",
    "if history['learning_rates']:\n",
    "    axes[1, 1].plot(history['learning_rates'], 'r-')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Model Inference and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on new examples\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"Predict sentiment for a given text\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    \n",
    "    # Move to device if using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "    # Get confidence\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # Convert to labels\n",
    "    label = \"Positive\" if predicted_class.item() == 1 else \"Negative\"\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': probabilities[0].cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"I absolutely loved this movie! Best film I've seen all year.\",\n",
    "    \"This was terrible. Waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"Amazing performance by the actors. Highly recommend!\",\n",
    "    \"Boring and predictable. Fell asleep halfway through.\",\n",
    "    \"A masterpiece of cinema. Perfect in every way.\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model on new examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "for example in test_examples:\n",
    "    result = predict_sentiment(example, trainer.model, tokenizer, device)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\nText: {example}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"Probabilities: Negative={result['probabilities'][0]:.3f}, Positive={result['probabilities'][1]:.3f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "labels = [r['label'] for r in results]\n",
    "confidences = [r['confidence'] for r in results]\n",
    "colors = ['red' if label == 'Negative' else 'green' for label in labels]\n",
    "\n",
    "bars = ax.bar(range(len(results)), confidences, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Example')\n",
    "ax.set_ylabel('Confidence')\n",
    "ax.set_title('Model Predictions on Test Examples')\n",
    "ax.set_xticks(range(len(results)))\n",
    "ax.set_xticklabels([f'Ex {i+1}' for i in range(len(results))])\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, label, conf) in enumerate(zip(bars, labels, confidences)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{label}\\n{conf:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_path = \"./fine-tuned-sentiment-model\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Save training arguments and metrics\n",
    "with open(f\"{save_path}/training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args.to_dict(), f, indent=2)\n",
    "\n",
    "with open(f\"{save_path}/final_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_eval, f, indent=2)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# List saved files\n",
    "import os\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"Saved files: {saved_files}\")\n",
    "\n",
    "# Test loading the saved model\n",
    "print(\"\\nTesting model loading...\")\n",
    "try:\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "    \n",
    "    # Test loaded model\n",
    "    test_text = \"This is a great product!\"\n",
    "    result_original = predict_sentiment(test_text, trainer.model, tokenizer, device)\n",
    "    result_loaded = predict_sentiment(test_text, loaded_model, loaded_tokenizer, device)\n",
    "    \n",
    "    print(f\"Original model: {result_original['label']} ({result_original['confidence']:.3f})\")\n",
    "    print(f\"Loaded model: {result_loaded['label']} ({result_loaded['confidence']:.3f})\")\n",
    "    print(f\"Results match: {np.allclose(result_original['probabilities'], result_loaded['probabilities'], rtol=1e-5)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Fine-tuning Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary\n",
    "def create_fine_tuning_summary():\n",
    "    \"\"\"Create a summary of fine-tuning best practices and results\"\"\"\n",
    "    \n",
    "    print(\"FINE-TUNING BEST PRACTICES SUMMARY\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ KEY LEARNINGS:\")\n",
    "    print(\"  â€¢ Start with a pre-trained model close to your domain\")\n",
    "    print(\"  â€¢ Use appropriate learning rates (2e-5 to 5e-5 for most cases)\")\n",
    "    print(\"  â€¢ Implement early stopping to prevent overfitting\")\n",
    "    print(\"  â€¢ Monitor both training and validation metrics\")\n",
    "    print(\"  â€¢ Use mixed precision training for efficiency\")\n",
    "    print(\"  â€¢ Save the best model based on validation metrics\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š TRAINING CONFIGURATION:\")\n",
    "    print(f\"  â€¢ Model: {model_name}\")\n",
    "    print(f\"  â€¢ Training examples: {len(tokenized_train):,}\")\n",
    "    print(f\"  â€¢ Validation examples: {len(tokenized_eval):,}\")\n",
    "    print(f\"  â€¢ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  â€¢ Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  â€¢ Max steps: {training_args.max_steps}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ PERFORMANCE RESULTS:\")\n",
    "    print(f\"  â€¢ Initial accuracy: {initial_eval['eval_accuracy']:.4f}\")\n",
    "    print(f\"  â€¢ Final accuracy: {final_eval['eval_accuracy']:.4f}\")\n",
    "    print(f\"  â€¢ Improvement: {final_eval['eval_accuracy'] - initial_eval['eval_accuracy']:+.4f}\")\n",
    "    print(f\"  â€¢ Final F1 score: {final_eval['eval_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nâš¡ OPTIMIZATION TIPS:\")\n",
    "    print(\"  â€¢ Use dynamic padding with DataCollatorWithPadding\")\n",
    "    print(\"  â€¢ Implement gradient accumulation for larger effective batch sizes\")\n",
    "    print(\"  â€¢ Use learning rate scheduling\")\n",
    "    print(\"  â€¢ Consider freezing early layers for domain adaptation\")\n",
    "    print(\"  â€¢ Monitor GPU memory usage and adjust batch size accordingly\")\n",
    "    \n",
    "    print(\"\\nðŸš€ PRODUCTION CONSIDERATIONS:\")\n",
    "    print(\"  â€¢ Validate on held-out test set\")\n",
    "    print(\"  â€¢ Test model robustness on edge cases\")\n",
    "    print(\"  â€¢ Consider model compression techniques\")\n",
    "    print(\"  â€¢ Implement proper error handling\")\n",
    "    print(\"  â€¢ Document training configuration and metrics\")\n",
    "    \n",
    "    print(\"\\nâœ… CHECKLIST FOR FINE-TUNING:\")\n",
    "    checklist = [\n",
    "        \"Data preprocessing and tokenization\",\n",
    "        \"Train/validation/test split\",\n",
    "        \"Appropriate model architecture\",\n",
    "        \"Hyperparameter tuning\",\n",
    "        \"Evaluation metrics definition\",\n",
    "        \"Training monitoring and callbacks\",\n",
    "        \"Model saving and versioning\",\n",
    "        \"Performance validation\",\n",
    "        \"Error analysis\",\n",
    "        \"Documentation\"\n",
    "    ]\n",
    "    \n",
    "    for i, item in enumerate(checklist, 1):\n",
    "        print(f\"  {i:2d}. {item}\")\n",
    "\n",
    "create_fine_tuning_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Fine-tuning tutorial completed successfully!\")\n",
    "print(\"Your model is ready for production use.\")\n",
    "print(\"Next: Try Notebook 06 for fine-tuning from scratch!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered the complete fine-tuning workflow using the Trainer API:\n",
    "\n",
    "### ðŸŽ¯ **What We Accomplished**\n",
    "1. **Data Preparation**: Loaded and preprocessed text data for classification\n",
    "2. **Model Setup**: Configured pre-trained model for fine-tuning\n",
    "3. **Training Configuration**: Set up comprehensive training arguments\n",
    "4. **Custom Metrics**: Implemented evaluation metrics and callbacks\n",
    "5. **Model Training**: Fine-tuned model with monitoring\n",
    "6. **Performance Analysis**: Evaluated and visualized results\n",
    "7. **Model Deployment**: Saved and tested the fine-tuned model\n",
    "\n",
    "### ï¿½ï¿½ **Key Concepts Mastered**\n",
    "- **Trainer API**: High-level interface for model training\n",
    "- **Training Arguments**: Comprehensive configuration options\n",
    "- **Callbacks**: Custom monitoring and early stopping\n",
    "- **Evaluation Metrics**: Custom metric computation\n",
    "- **Model Saving**: Proper model persistence and loading\n",
    "\n",
    "### ðŸ“ˆ **Best Practices Learned**\n",
    "- Start with appropriate learning rates (2e-5 to 5e-5)\n",
    "- Use validation data for model selection\n",
    "- Implement early stopping to prevent overfitting\n",
    "- Monitor training progress with callbacks\n",
    "- Use mixed precision training for efficiency\n",
    "- Save models with proper versioning\n",
    "\n",
    "### ðŸš€ **Next Steps**\n",
    "- **Notebook 06**: Fine-tuning from scratch with custom training loops\n",
    "- **Notebook 07**: Specialized applications (summarization)\n",
    "- **Notebook 08**: Question answering systems\n",
    "- **Notebook 09**: Advanced techniques (LoRA, QLoRA)\n",
    "\n",
    "The Trainer API provides an excellent balance between ease of use and flexibility, making it the go-to choice for most fine-tuning tasks. You now have the foundation to fine-tune models for any classification task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
