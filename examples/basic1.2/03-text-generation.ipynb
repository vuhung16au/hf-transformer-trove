{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/03-text-generation.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/03-text-generation.ipynb)\n",
    "\n",
    "# Text Generation with Hugging Face Transformers\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to use the text generation pipeline for creating text\n",
    "- Different text generation models and their characteristics\n",
    "- Key parameters for controlling text generation quality\n",
    "- Best practices for text generation applications\n",
    "- How to handle different use cases and scenarios\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of transformer architectures\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Basic Text Generation**: Simple pipeline usage\n",
    "2. **Model Selection**: Different models for various use cases\n",
    "3. **Parameter Control**: Understanding generation parameters\n",
    "4. **Advanced Techniques**: Sampling strategies and control methods\n",
    "5. **Real-world Applications**: Practical examples and use cases\n",
    "6. **Performance & Optimization**: Memory management and speed considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch datasets tokenizers matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    COLAB_AVAILABLE = True\n",
    "    print(\"üìç Running in Google Colab\")\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    print(\"üíª Running in local environment\")\n",
    "\n",
    "# Load environment variables for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"‚úÖ Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  python-dotenv not installed, skipping .env.local loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Set up device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Text Generation\n",
    "\n",
    "Let's start with the simple example from the issue. This demonstrates the most basic way to generate text using Hugging Face transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation as specified in the issue\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline (uses default GPT-2 model)\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Generate text with the provided prompt\n",
    "result = generator(\"In this course, we will teach you how to\")\n",
    "\n",
    "print(\"üìù Basic Text Generation Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Basic Pipeline\n",
    "\n",
    "The simple `pipeline(\"text-generation\")` call does several things automatically:\n",
    "- Downloads and loads the default GPT-2 model (~500MB)\n",
    "- Sets up tokenization and model inference\n",
    "- Uses default generation parameters\n",
    "\n",
    "Let's explore what's happening under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what model the pipeline is using\n",
    "print(f\"üìä Pipeline Model Information:\")\n",
    "print(f\"Model: {generator.model.config.name_or_path if hasattr(generator.model.config, 'name_or_path') else 'gpt2'}\")\n",
    "print(f\"Model type: {generator.model.config.model_type}\")\n",
    "print(f\"Vocabulary size: {generator.model.config.vocab_size:,}\")\n",
    "print(f\"Maximum position embeddings: {generator.model.config.n_positions:,}\")\n",
    "print(f\"Number of parameters: {generator.model.num_parameters():,}\")\n",
    "\n",
    "# Show tokenizer information\n",
    "print(f\"\\nüî§ Tokenizer Information:\")\n",
    "print(f\"Tokenizer type: {type(generator.tokenizer).__name__}\")\n",
    "print(f\"Vocabulary size: {len(generator.tokenizer):,}\")\n",
    "print(f\"Special tokens: {generator.tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Selection and Comparison\n",
    "\n",
    "Different models have different characteristics. Let's explore various text generation models and compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different models for text generation\n",
    "models_to_test = {\n",
    "    \"GPT-2 (Small)\": \"gpt2\",  # 117M parameters\n",
    "    \"DistilGPT-2\": \"distilgpt2\",  # 82M parameters (faster, smaller)\n",
    "    \"GPT-2 Medium\": \"gpt2-medium\",  # 345M parameters (better quality)\n",
    "}\n",
    "\n",
    "# Initialize pipelines with different models\n",
    "generators = {}\n",
    "load_times = {}\n",
    "\n",
    "for name, model_name in models_to_test.items():\n",
    "    print(f\"üì• Loading {name} ({model_name})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        generators[name] = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "        )\n",
    "        load_time = time.time() - start_time\n",
    "        load_times[name] = load_time\n",
    "        print(f\"   ‚úÖ Loaded in {load_time:.2f}s\")\n",
    "        \n",
    "        # Print model info\n",
    "        model = generators[name].model\n",
    "        print(f\"   üìä Parameters: {model.num_parameters():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to load: {e}\")\n",
    "        # Remove from models_to_test if failed\n",
    "        if name in models_to_test:\n",
    "            del models_to_test[name]\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison with Same Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models with the same prompt\n",
    "test_prompt = \"In this course, we will teach you how to\"\n",
    "print(f\"üéØ Testing with prompt: \\\"{test_prompt}\\\"\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate text with each model\n",
    "results = {}\n",
    "generation_times = {}\n",
    "\n",
    "for name, gen in generators.items():\n",
    "    print(f\"\\nü§ñ {name}:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate text with consistent parameters\n",
    "        output = gen(\n",
    "            test_prompt,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gen.tokenizer.eos_token_id  # Avoid warning\n",
    "        )\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        generation_times[name] = generation_time\n",
    "        results[name] = output[0]['generated_text']\n",
    "        \n",
    "        print(f\"   ‚è±Ô∏è  Generated in {generation_time:.2f}s\")\n",
    "        print(f\"   üìù Output: {output[0]['generated_text']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Generation failed: {e}\")\n",
    "        results[name] = f\"Error: {e}\"\n",
    "        generation_times[name] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Generation Parameters\n",
    "\n",
    "Text generation quality heavily depends on the parameters used. Let's explore the key parameters and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first available generator for parameter exploration\n",
    "demo_generator = list(generators.values())[0]\n",
    "demo_model_name = list(generators.keys())[0]\n",
    "\n",
    "print(f\"üîß Exploring parameters with {demo_model_name}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define parameter sets to test\n",
    "parameter_sets = {\n",
    "    \"Conservative (Low Temperature)\": {\n",
    "        \"temperature\": 0.3,\n",
    "        \"do_sample\": True,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Balanced (Medium Temperature)\": {\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Creative (High Temperature)\": {\n",
    "        \"temperature\": 1.2,\n",
    "        \"do_sample\": True,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Deterministic (Greedy)\": {\n",
    "        \"do_sample\": False,\n",
    "        \"max_length\": 80\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "print(f\"üéØ Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "for param_name, params in parameter_sets.items():\n",
    "    print(f\"üéõÔ∏è  {param_name}:\")\n",
    "    print(f\"   Parameters: {params}\")\n",
    "    \n",
    "    try:\n",
    "        output = demo_generator(\n",
    "            prompt,\n",
    "            **params,\n",
    "            pad_token_id=demo_generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        print(f\"   üìù Result: {output[0]['generated_text']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Explanation\n",
    "\n",
    "Let's understand what each parameter does:\n",
    "\n",
    "- **`temperature`**: Controls randomness (0.1-2.0)\n",
    "  - Low (0.1-0.5): More focused, conservative\n",
    "  - Medium (0.6-0.8): Balanced creativity\n",
    "  - High (0.9-2.0): More random, creative\n",
    "\n",
    "- **`do_sample`**: Whether to use sampling vs. greedy decoding\n",
    "  - `True`: Uses sampling (more diverse)\n",
    "  - `False`: Uses greedy decoding (deterministic)\n",
    "\n",
    "- **`max_length`**: Maximum number of tokens to generate\n",
    "- **`num_return_sequences`**: Number of different outputs to generate\n",
    "- **`top_p`**: Nucleus sampling threshold\n",
    "- **`top_k`**: Top-k sampling parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Generation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced sampling techniques\n",
    "print(\"üß† Advanced Sampling Techniques\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "advanced_techniques = {\n",
    "    \"Top-k Sampling (k=50)\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Nucleus Sampling (p=0.9)\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Combined (top_k=40, top_p=0.95)\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_length\": 80\n",
    "    },\n",
    "    \"Multiple Sequences (n=3)\": {\n",
    "        \"do_sample\": True,\n",
    "        \"num_return_sequences\": 3,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_length\": 60\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"To build a successful AI application, you need to\"\n",
    "print(f\"üéØ Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "for technique_name, params in advanced_techniques.items():\n",
    "    print(f\"‚ö° {technique_name}:\")\n",
    "    \n",
    "    try:\n",
    "        outputs = demo_generator(\n",
    "            prompt,\n",
    "            **params,\n",
    "            pad_token_id=demo_generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if isinstance(outputs, list) and len(outputs) > 1:\n",
    "            for i, output in enumerate(outputs, 1):\n",
    "                print(f\"   üìù Sequence {i}: {output['generated_text']}\")\n",
    "        else:\n",
    "            print(f\"   üìù Result: {outputs[0]['generated_text']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real-world Applications\n",
    "\n",
    "Let's explore practical applications of text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationHelper:\n",
    "    \"\"\"Helper class for different text generation applications.\"\"\"\n",
    "    \n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def creative_writing(self, prompt: str, style: str = \"balanced\") -> str:\n",
    "        \"\"\"Generate creative writing content.\"\"\"\n",
    "        style_params = {\n",
    "            \"conservative\": {\"temperature\": 0.5, \"top_p\": 0.8},\n",
    "            \"balanced\": {\"temperature\": 0.8, \"top_p\": 0.9},\n",
    "            \"creative\": {\"temperature\": 1.1, \"top_p\": 0.95}\n",
    "        }\n",
    "        \n",
    "        params = style_params.get(style, style_params[\"balanced\"])\n",
    "        \n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_length=150,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.generator.tokenizer.eos_token_id,\n",
    "            **params\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    \n",
    "    def business_content(self, prompt: str) -> str:\n",
    "        \"\"\"Generate professional business content.\"\"\"\n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_length=120,\n",
    "            temperature=0.6,  # More conservative for professional content\n",
    "            do_sample=True,\n",
    "            top_p=0.85,\n",
    "            pad_token_id=self.generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    \n",
    "    def educational_content(self, prompt: str) -> str:\n",
    "        \"\"\"Generate educational content.\"\"\"\n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_length=140,\n",
    "            temperature=0.5,  # Conservative for accuracy\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            pad_token_id=self.generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "\n",
    "# Create helper instance\n",
    "helper = TextGenerationHelper(demo_generator)\n",
    "\n",
    "# Test different applications\n",
    "applications = {\n",
    "    \"Creative Writing\": {\n",
    "        \"prompt\": \"The old lighthouse stood alone on the cliff, its beam cutting through the fog\",\n",
    "        \"method\": helper.creative_writing\n",
    "    },\n",
    "    \"Business Content\": {\n",
    "        \"prompt\": \"Our company's strategic vision for the next quarter focuses on\",\n",
    "        \"method\": helper.business_content\n",
    "    },\n",
    "    \"Educational Content\": {\n",
    "        \"prompt\": \"Machine learning is a subset of artificial intelligence that\",\n",
    "        \"method\": helper.educational_content\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üé® Real-world Application Examples\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for app_name, config in applications.items():\n",
    "    print(f\"\\nüìã {app_name}:\")\n",
    "    print(f\"   Prompt: \\\"{config['prompt']}\\\"\")\n",
    "    \n",
    "    try:\n",
    "        result = config['method'](config['prompt'])\n",
    "        print(f\"   üìù Generated: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "if len(generation_times) > 1:  # Only if we have multiple models\n",
    "    # Create performance visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Model loading times\n",
    "    model_names = list(load_times.keys())\n",
    "    load_time_values = list(load_times.values())\n",
    "    \n",
    "    ax1.bar(model_names, load_time_values, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Model Loading Times', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(load_time_values):\n",
    "        ax1.text(i, v + 0.1, f'{v:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # Generation times\n",
    "    gen_model_names = list(generation_times.keys())\n",
    "    gen_time_values = list(generation_times.values())\n",
    "    \n",
    "    ax2.bar(gen_model_names, gen_time_values, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Text Generation Times', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(gen_time_values):\n",
    "        if v > 0:  # Only show if not error\n",
    "            ax2.text(i, v + 0.01, f'{v:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nüìä Performance Summary:\")\n",
    "    print(\"=\" * 30)\n",
    "    for model in model_names:\n",
    "        if model in load_times and model in generation_times:\n",
    "            print(f\"{model}:\")\n",
    "            print(f\"  Loading: {load_times[model]:.2f}s\")\n",
    "            print(f\"  Generation: {generation_times[model]:.2f}s\")\n",
    "            print(f\"  Total: {load_times[model] + generation_times[model]:.2f}s\")\n",
    "            print()\nelse:\n    print(\"üìä Performance analysis requires multiple models to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Best Practices and Troubleshooting\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Use DistilGPT-2 for speed, GPT-2 medium/large for quality\n",
    "   - Consider memory constraints when choosing model size\n",
    "\n",
    "2. **Parameter Tuning**\n",
    "   - Start with temperature=0.7 for balanced output\n",
    "   - Use `top_p=0.9` for nucleus sampling\n",
    "   - Set appropriate `max_length` to avoid truncation\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Use GPU acceleration when available\n",
    "   - Batch multiple generations for efficiency\n",
    "   - Cache models to avoid repeated loading\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Memory Issues**: Large models may cause out-of-memory errors\n",
    "2. **Repetitive Output**: Too low temperature can cause repetition\n",
    "3. **Inconsistent Quality**: Very high temperature leads to nonsensical output\n",
    "4. **Padding Token Warnings**: Always set `pad_token_id` for clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_generate_text(generator, prompt: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Safe text generation with error handling and best practices.\n",
    "    \n",
    "    Args:\n",
    "        generator: HuggingFace pipeline\n",
    "        prompt: Input text prompt\n",
    "        **kwargs: Generation parameters\n",
    "    \n",
    "    Returns:\n",
    "        Generated text or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set default parameters if not provided\n",
    "        default_params = {\n",
    "            'max_length': 100,\n",
    "            'temperature': 0.7,\n",
    "            'do_sample': True,\n",
    "            'pad_token_id': generator.tokenizer.eos_token_id,\n",
    "            'num_return_sequences': 1\n",
    "        }\n",
    "        \n",
    "        # Update with user parameters\n",
    "        params = {**default_params, **kwargs}\n",
    "        \n",
    "        # Validate parameters\n",
    "        if params['max_length'] > 512:\n",
    "            print(\"‚ö†Ô∏è  Warning: max_length > 512 may cause memory issues\")\n",
    "        \n",
    "        if params['temperature'] < 0.1 or params['temperature'] > 2.0:\n",
    "            print(f\"‚ö†Ô∏è  Warning: temperature {params['temperature']} is outside recommended range (0.1-2.0)\")\n",
    "        \n",
    "        # Generate text\n",
    "        start_time = time.time()\n",
    "        result = generator(prompt, **params)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Generated in {generation_time:.2f}s\")\n",
    "        return result\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"‚ùå GPU out of memory. Try reducing max_length or using a smaller model.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Demonstrate safe generation\n",
    "print(\"üõ°Ô∏è  Safe Text Generation Example:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "result = safe_generate_text(\n",
    "    demo_generator,\n",
    "    \"The key to successful machine learning projects is\",\n",
    "    max_length=120,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"üìù Generated text: {result[0]['generated_text']}\")\nelse:\n    print(\"‚ùå Generation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"üîã GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "    else:\n",
    "        print(\"üíª Running on CPU - no GPU memory to track\")\n",
    "\n",
    "# Check memory usage\n",
    "print(\"üìä Current Memory Usage:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Optional: Clear GPU cache if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nüßπ GPU cache cleared\")\n    print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Basic Text Generation**: Used `pipeline(\"text-generation\")` for simple text generation\n",
    "- **Model Selection**: Compared different models (GPT-2, DistilGPT-2) for various use cases\n",
    "- **Parameter Control**: Learned how temperature, sampling, and other parameters affect output quality\n",
    "- **Advanced Techniques**: Explored top-k, nucleus sampling, and multiple sequence generation\n",
    "- **Real-world Applications**: Applied text generation to creative writing, business, and educational content\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- **Device Awareness**: Automatically detect and use optimal compute device (CUDA/MPS/CPU)\n",
    "- **Parameter Optimization**: Balance creativity and coherence through proper parameter tuning\n",
    "- **Error Handling**: Implement robust error handling for production applications\n",
    "- **Memory Management**: Monitor and manage GPU memory usage effectively\n",
    "- **Performance Monitoring**: Track loading and generation times for optimization\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Notebook 04**: Mini Project - building complete NLP applications\n",
    "- **Notebook 05**: Fine-tuning models for specific domains\n",
    "- **Advanced Topics**: Explore larger models like GPT-3.5 and GPT-4 via APIs\n",
    "- **Documentation**: [Hugging Face Transformers Text Generation Guide](https://huggingface.co/docs/transformers/task_summary#text-generation)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}