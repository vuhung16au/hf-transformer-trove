{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/07-summarization.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/07-summarization.ipynb)\n",
    "\n",
    "# üìù Text Summarization: Creating Concise Summaries\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What is text summarization and why it's important\n",
    "- The difference between extractive and abstractive summarization\n",
    "- How to use pre-trained summarization models with Hugging Face\n",
    "- Practical applications of text summarization\n",
    "- Best practices for implementing summarization systems\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. Introduction to Text Summarization\n",
    "2. Setting up the Environment\n",
    "3. Using Pre-trained Summarization Models\n",
    "4. Practical Examples and Applications\n",
    "5. Best Practices and Tips\n",
    "6. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Text Summarization\n",
    "\n",
    "**Text summarization** is the task of creating a shorter version of a text while preserving the most important information. It's one of the most useful NLP applications.\n",
    "\n",
    "### üîç Types of Summarization:\n",
    "\n",
    "**Extractive Summarization:**\n",
    "- Selects and combines existing sentences from the original text\n",
    "- Maintains original phrasing and vocabulary\n",
    "- Example: Highlighting key sentences in a document\n",
    "\n",
    "**Abstractive Summarization:**\n",
    "- Generates new text that captures the essence of the original\n",
    "- Can rephrase and restructure information\n",
    "- More human-like but computationally complex\n",
    "\n",
    "### üåü Real-world Applications:\n",
    "- News article summaries\n",
    "- Research paper abstracts\n",
    "- Meeting notes condensation\n",
    "- Email and message summaries\n",
    "- Document analysis for legal/medical fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# For visualization and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get optimal device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Pre-trained Summarization Models\n",
    "\n",
    "Let's start with the easiest approach - using Hugging Face's pipeline API for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize summarization pipeline\n",
    "print(\"üîÑ Loading summarization model...\")\n",
    "print(\"This may take a few minutes on first run (downloading model)\")\n",
    "\n",
    "try:\n",
    "    # Use BART model fine-tuned on CNN/DailyMail dataset\n",
    "    # This is one of the best models for news summarization\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=\"facebook/bart-large-cnn\",\n",
    "        device=0 if device.type == 'cuda' else -1  # Use GPU if available\n",
    "    )\n",
    "    print(\"‚úÖ Summarization model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üí° Trying alternative model...\")\n",
    "    \n",
    "    # Fallback to a smaller model\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "        device=0 if device.type == 'cuda' else -1\n",
    "    )\n",
    "    print(\"‚úÖ Alternative summarization model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Your First Summarization\n",
    "\n",
    "Let's try summarizing a sample article to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article for demonstration\n",
    "sample_article = \"\"\"\n",
    "Artificial Intelligence (AI) has become one of the most transformative technologies of the 21st century, \n",
    "revolutionizing industries from healthcare to transportation. Machine learning algorithms, particularly \n",
    "deep neural networks, have achieved remarkable breakthroughs in tasks that were once considered \n",
    "impossible for computers to perform.\n",
    "\n",
    "In healthcare, AI systems can now diagnose diseases from medical images with accuracy matching or \n",
    "exceeding human specialists. For instance, Google's AI can detect diabetic retinopathy from eye scans, \n",
    "potentially preventing blindness in millions of people worldwide. Similarly, AI-powered drug discovery \n",
    "platforms are accelerating the development of new medications, reducing the time and cost required \n",
    "to bring life-saving treatments to market.\n",
    "\n",
    "The transportation industry has been equally transformed by autonomous vehicle technology. Companies \n",
    "like Tesla, Waymo, and Uber have invested billions in developing self-driving cars that promise to \n",
    "reduce traffic accidents, improve fuel efficiency, and provide mobility solutions for elderly and \n",
    "disabled populations.\n",
    "\n",
    "However, the rapid advancement of AI also raises important concerns about job displacement, privacy, \n",
    "and the need for proper regulation. As AI systems become more capable, society must carefully \n",
    "consider how to harness their benefits while mitigating potential risks.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"üìÑ Original Article:\")\n",
    "print(\"-\" * 50)\n",
    "print(sample_article)\n",
    "print(f\"\\nüìä Article length: {len(sample_article.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "print(\"üîÑ Generating summary...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Generate summary with specific parameters\n",
    "    summary_result = summarizer(\n",
    "        sample_article,\n",
    "        max_length=100,      # Maximum length of summary\n",
    "        min_length=30,       # Minimum length of summary\n",
    "        do_sample=False,     # Use deterministic generation\n",
    "        truncation=True      # Truncate if input is too long\n",
    "    )\n",
    "    \n",
    "    # Extract summary text\n",
    "    summary = summary_result[0]['summary_text']\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(\"‚úÖ Summary generated successfully!\")\n",
    "    print(f\"‚è±Ô∏è Generation time: {generation_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating summary: {e}\")\n",
    "    summary = \"Error occurred during summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and analyze the summary\n",
    "print(\"üìù Generated Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(summary)\n",
    "\n",
    "# Calculate summary statistics\n",
    "original_words = len(sample_article.split())\n",
    "summary_words = len(summary.split())\n",
    "compression_ratio = original_words / summary_words if summary_words > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "print(f\"  Original length: {original_words} words\")\n",
    "print(f\"  Summary length: {summary_words} words\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"  Reduction: {((original_words - summary_words) / original_words * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building a More Advanced Summarization System\n",
    "\n",
    "Let's create a more sophisticated summarization system with better error handling and customization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    \"\"\"\n",
    "    A comprehensive text summarization system with educational focus.\n",
    "    \n",
    "    This class provides an easy-to-use interface for text summarization\n",
    "    with built-in error handling, preprocessing, and analysis features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer with a specific model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pipeline = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the summarization model with error handling.\"\"\"\n",
    "        try:\n",
    "            print(f\"üîÑ Loading model: {self.model_name}\")\n",
    "            self.pipeline = pipeline(\n",
    "                \"summarization\",\n",
    "                model=self.model_name,\n",
    "                device=0 if device.type == 'cuda' else -1\n",
    "            )\n",
    "            print(\"‚úÖ Model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            print(\"üí° Trying fallback model...\")\n",
    "            \n",
    "            # Fallback to smaller model\n",
    "            self.model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "            self.pipeline = pipeline(\n",
    "                \"summarization\",\n",
    "                model=self.model_name,\n",
    "                device=0 if device.type == 'cuda' else -1\n",
    "            )\n",
    "            print(\"‚úÖ Fallback model loaded successfully\")\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text before summarization.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to preprocess\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed text ready for summarization\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            raise ValueError(\"Input text is empty\")\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Check minimum length (most models need at least 30-50 words)\n",
    "        words = text.split()\n",
    "        if len(words) < 30:\n",
    "            print(f\"‚ö†Ô∏è Warning: Text is quite short ({len(words)} words). \"\n",
    "                  f\"Summarization works best with longer texts.\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def summarize(\n",
    "        self, \n",
    "        text: str, \n",
    "        max_length: int = 130, \n",
    "        min_length: int = 30,\n",
    "        num_beams: int = 4\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a summary of the input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize\n",
    "            max_length: Maximum length of the summary\n",
    "            min_length: Minimum length of the summary\n",
    "            num_beams: Number of beams for beam search (higher = more thorough)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing summary and analysis\n",
    "        \"\"\"\n",
    "        # Preprocess input\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        \n",
    "        try:\n",
    "            # Generate summary\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = self.pipeline(\n",
    "                processed_text,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            summary = result[0]['summary_text']\n",
    "            \n",
    "            # Calculate statistics\n",
    "            original_words = len(text.split())\n",
    "            summary_words = len(summary.split())\n",
    "            compression_ratio = original_words / summary_words if summary_words > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'summary': summary,\n",
    "                'original_length': original_words,\n",
    "                'summary_length': summary_words,\n",
    "                'compression_ratio': compression_ratio,\n",
    "                'generation_time': generation_time,\n",
    "                'model_used': self.model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during summarization: {e}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'original_text': text,\n",
    "                'summary': None\n",
    "            }\n",
    "\n",
    "# Initialize our advanced summarizer\n",
    "advanced_summarizer = TextSummarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Examples with Different Text Types\n",
    "\n",
    "Let's test our summarizer with different types of content to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of content for testing\n",
    "test_texts = {\n",
    "    \"news\": \"\"\"\n",
    "    Scientists at MIT have developed a revolutionary new battery technology that could dramatically \n",
    "    extend the range of electric vehicles while reducing charging time to just five minutes. The \n",
    "    breakthrough involves using lithium metal anodes combined with a solid ceramic electrolyte, \n",
    "    which eliminates the safety concerns and performance limitations of current lithium-ion batteries.\n",
    "    \n",
    "    The new battery design can store up to three times more energy than conventional batteries while \n",
    "    maintaining stability over thousands of charge cycles. Initial testing shows that electric vehicles \n",
    "    equipped with these batteries could travel over 1,000 miles on a single charge, compared to the \n",
    "    300-400 mile range of current EVs.\n",
    "    \n",
    "    The research team, led by Dr. Sarah Chen from University of Sydney, expects the technology to be commercially available \n",
    "    within five years. Major automakers including Tesla, Ford, and Toyota have already expressed \n",
    "    interest in licensing the technology. The development could accelerate the global transition \n",
    "    to electric vehicles and significantly reduce carbon emissions from transportation.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"scientific\": \"\"\"\n",
    "    Recent advances in quantum computing have brought us closer to achieving quantum supremacy in \n",
    "    practical applications. Researchers have successfully demonstrated quantum error correction using \n",
    "    a 70-qubit processor, marking a significant milestone in fault-tolerant quantum computing.\n",
    "    \n",
    "    The experiment involved creating logical qubits that are protected from environmental noise \n",
    "    through sophisticated error correction codes. By encoding quantum information across multiple \n",
    "    physical qubits, the system can detect and correct errors without destroying the quantum state.\n",
    "    \n",
    "    This breakthrough addresses one of the fundamental challenges in quantum computing: maintaining \n",
    "    coherent quantum states long enough to perform complex calculations. The implications for \n",
    "    cryptography, drug discovery, and materials science are profound, as quantum computers could \n",
    "    solve certain problems exponentially faster than classical computers.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"business\": \"\"\"\n",
    "    Global e-commerce giant Amazon announced record-breaking quarterly profits, driven by strong \n",
    "    growth in cloud computing services and advertising revenue. The company reported net income of \n",
    "    $15.3 billion for the third quarter, exceeding analyst expectations by 22%.\n",
    "    \n",
    "    Amazon Web Services (AWS), the company's cloud computing division, saw revenue increase by 35% \n",
    "    year-over-year as more businesses accelerate their digital transformation efforts. The advertising \n",
    "    business also performed exceptionally well, with revenue growing 58% as retailers increased their \n",
    "    marketing spend during the peak shopping season.\n",
    "    \n",
    "    CEO Andy Jassy attributed the success to continued innovation in artificial intelligence and \n",
    "    machine learning capabilities, which have improved operational efficiency and customer experience. \n",
    "    The company plans to invest heavily in expanding its logistics network and developing new AI-powered \n",
    "    services for business customers.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"üß™ Testing summarization on different content types...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization on different content types\n",
    "results = {}\n",
    "\n",
    "for content_type, text in test_texts.items():\n",
    "    print(f\"\\nüìù {content_type.upper()} ARTICLE:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Generate summary\n",
    "    result = advanced_summarizer.summarize(\n",
    "        text.strip(),\n",
    "        max_length=80,  # Shorter summaries for comparison\n",
    "        min_length=25\n",
    "    )\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"üìÑ Original ({result['original_length']} words):\")\n",
    "        print(text.strip()[:200] + \"...\" if len(text.strip()) > 200 else text.strip())\n",
    "        \n",
    "        print(f\"\\nüìù Summary ({result['summary_length']} words):\")\n",
    "        print(result['summary'])\n",
    "        \n",
    "        print(f\"\\nüìä Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Compression: {result['compression_ratio']:.1f}x\")\n",
    "        print(f\"  ‚Ä¢ Generation time: {result['generation_time']:.2f}s\")\n",
    "        \n",
    "        results[content_type] = result\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully summarized {len(results)} different content types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Understanding Summarization Parameters\n",
    "\n",
    "Let's explore how different parameters affect the quality and style of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameter configurations\n",
    "parameter_tests = [\n",
    "    {\"name\": \"Short & Concise\", \"max_length\": 50, \"min_length\": 20, \"num_beams\": 4},\n",
    "    {\"name\": \"Balanced\", \"max_length\": 100, \"min_length\": 40, \"num_beams\": 4},\n",
    "    {\"name\": \"Detailed\", \"max_length\": 150, \"min_length\": 60, \"num_beams\": 6},\n",
    "    {\"name\": \"High Quality (More Beams)\", \"max_length\": 100, \"min_length\": 40, \"num_beams\": 8}\n",
    "]\n",
    "\n",
    "# Use the first test text for comparison\n",
    "test_text = test_texts[\"news\"]\n",
    "\n",
    "print(\"üéõÔ∏è Testing Different Summarization Parameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÑ Source text: {len(test_text.split())} words\")\n",
    "print()\n",
    "\n",
    "for config in parameter_tests:\n",
    "    print(f\"\\nüîß Configuration: {config['name']}\")\n",
    "    print(f\"   Max length: {config['max_length']}, Min length: {config['min_length']}, Beams: {config['num_beams']}\")\n",
    "    \n",
    "    result = advanced_summarizer.summarize(\n",
    "        test_text.strip(),\n",
    "        max_length=config['max_length'],\n",
    "        min_length=config['min_length'],\n",
    "        num_beams=config['num_beams']\n",
    "    )\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"üìù Summary ({result['summary_length']} words): {result['summary']}\")\n",
    "        print(f\"‚è±Ô∏è Time: {result['generation_time']:.2f}s, Ratio: {result['compression_ratio']:.1f}x\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Best Practices and Tips\n",
    "\n",
    "Here are some important guidelines for effective text summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for text summarization.\n",
    "    \"\"\"\n",
    "    print(\"üí° Best Practices for Text Summarization\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            \"title\": \"üéØ Choose the Right Model\",\n",
    "            \"description\": \"Different models work better for different content types\",\n",
    "            \"examples\": [\n",
    "                \"‚Ä¢ facebook/bart-large-cnn: Best for news articles\",\n",
    "                \"‚Ä¢ google/pegasus-xsum: Good for very short summaries\",\n",
    "                \"‚Ä¢ t5-small: Lighter model for resource-constrained environments\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"üìè Optimize Length Parameters\",\n",
    "            \"description\": \"Set appropriate length based on your use case\",\n",
    "            \"examples\": [\n",
    "                \"‚Ä¢ Headlines: max_length=30-50\",\n",
    "                \"‚Ä¢ Social media: max_length=50-100\",\n",
    "                \"‚Ä¢ Executive summaries: max_length=100-200\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"üîß Tune Generation Parameters\",\n",
    "            \"description\": \"Adjust parameters for quality vs speed tradeoffs\",\n",
    "            \"examples\": [\n",
    "                \"‚Ä¢ More beams (4-8): Higher quality, slower generation\",\n",
    "                \"‚Ä¢ Fewer beams (1-3): Faster generation, potentially lower quality\",\n",
    "                \"‚Ä¢ Length penalty: Controls summary length adherence\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"‚ö†Ô∏è Handle Edge Cases\",\n",
    "            \"description\": \"Always validate inputs and handle errors gracefully\",\n",
    "            \"examples\": [\n",
    "                \"‚Ä¢ Check minimum text length (30+ words recommended)\",\n",
    "                \"‚Ä¢ Handle empty or very short inputs\",\n",
    "                \"‚Ä¢ Implement fallback models for reliability\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['title']}\")\n",
    "        print(f\"{practice['description']}\")\n",
    "        for example in practice['examples']:\n",
    "            print(f\"  {example}\")\n",
    "    \n",
    "    print(\"\\nüö® Common Pitfalls to Avoid:\")\n",
    "    pitfalls = [\n",
    "        \"‚Ä¢ Using summarization on very short texts (< 30 words)\",\n",
    "        \"‚Ä¢ Setting min_length too close to max_length\",\n",
    "        \"‚Ä¢ Ignoring model token limits (typically 1024 tokens)\",\n",
    "        \"‚Ä¢ Not preprocessing text (removing unnecessary formatting)\",\n",
    "        \"‚Ä¢ Expecting perfect summaries without domain-specific fine-tuning\"\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  {pitfall}\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully learned the fundamentals of text summarization with Hugging Face transformers.\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Text Summarization Basics**: Understanding extractive vs abstractive approaches\n",
    "- **Model Usage**: Working with pre-trained summarization models\n",
    "- **Pipeline API**: Using Hugging Face's simple pipeline interface\n",
    "- **Parameter Tuning**: Optimizing length, quality, and performance\n",
    "- **Best Practices**: Handling different content types and edge cases\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Choose models appropriate for your content type and use case\n",
    "- Optimize length parameters based on desired output format\n",
    "- Implement proper error handling for production systems\n",
    "- Consider computational resources when selecting models and parameters\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced Fine-tuning**: Learn to fine-tune models on domain-specific data\n",
    "- **Evaluation Metrics**: Explore ROUGE scores and other summarization metrics  \n",
    "- **Multi-document Summarization**: Summarize multiple documents together\n",
    "- **Real-time Applications**: Build web services and APIs for summarization\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}