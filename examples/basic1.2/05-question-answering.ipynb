{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/05-question-answering.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/05-question-answering.ipynb)\n",
    "\n",
    "# 05 - Basic Question Answering with Transformers\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to use Hugging Face pipelines for question answering\n",
    "- Basic question-answering workflow with transformers\n",
    "- How to extract information from text using QA models\n",
    "- Working with context and question pairs\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Setup**: Import libraries and set up environment\n",
    "2. **Basic QA Pipeline**: Using transformers pipeline for question answering\n",
    "3. **Custom Examples**: Testing with different contexts and questions\n",
    "4. **Understanding Results**: Interpreting confidence scores and answers\n",
    "5. **Summary**: Key takeaways and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only if not already installed)\n",
    "# !pip install transformers torch\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the optimal device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Question Answering Pipeline\n",
    "\n",
    "Question Answering (QA) is a natural language processing task where we extract answers from a given context based on a question. The Hugging Face `transformers` library provides a simple pipeline interface for this task.\n",
    "\n",
    "Let's start with the basic example from the problem statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question-answering pipeline\n",
    "# This will automatically download a pre-trained QA model (e.g., DistilBERT)\n",
    "print(\"üì• Loading question-answering pipeline...\")\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "print(\"‚úÖ Pipeline loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic question answering example from the problem statement\n",
    "result = question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Sydney\",\n",
    ")\n",
    "\n",
    "print(\"üìã QUESTION ANSWERING RESULT\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Question: {result.get('question', 'Where do I work?')}\")\n",
    "print(f\"Context: My name is Sylvain and I work at Hugging Face in Sydney\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\")\n",
    "print(f\"Answer Position: {result['start']}-{result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More Examples with Different Contexts\n",
    "\n",
    "Let's explore more examples to understand how the question-answering pipeline works with different types of questions and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Company information\n",
    "context_1 = \"\"\"\n",
    "Hugging Face is a French company with offices in Sydney, Australia that develops tools for building \n",
    "applications using machine learning. It is most notable for its transformers library built \n",
    "for natural language processing applications and its platform that allows users to share \n",
    "machine learning models and datasets.\n",
    "\"\"\"\n",
    "\n",
    "questions_1 = [\n",
    "    \"Where is Hugging Face based?\",\n",
    "    \"What is Hugging Face most notable for?\",\n",
    "    \"What can users share on the platform?\"\n",
    "]\n",
    "\n",
    "print(\"üìã EXAMPLE 1: Company Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Context: {context_1.strip()}\")\n",
    "print(\"\\nüîç Questions and Answers:\")\n",
    "\n",
    "for i, question in enumerate(questions_1, 1):\n",
    "    result = question_answerer(question=question, context=context_1)\n",
    "    print(f\"\\n  {i}. Question: {question}\")\n",
    "    print(f\"     Answer: {result['answer']}\")\n",
    "    print(f\"     Confidence: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Scientific information\n",
    "context_2 = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence (AI) that focuses on algorithms \n",
    "that can learn and improve from data without being explicitly programmed. Deep learning, \n",
    "a subset of machine learning, uses neural networks with multiple layers to model and \n",
    "understand complex patterns in data. Transformers are a type of deep learning architecture \n",
    "that has revolutionized natural language processing tasks.\n",
    "\"\"\"\n",
    "\n",
    "questions_2 = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What type of networks does deep learning use?\",\n",
    "    \"What have transformers revolutionized?\"\n",
    "]\n",
    "\n",
    "print(\"üìã EXAMPLE 2: Scientific Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Context: {context_2.strip()}\")\n",
    "print(\"\\nüîç Questions and Answers:\")\n",
    "\n",
    "for i, question in enumerate(questions_2, 1):\n",
    "    result = question_answerer(question=question, context=context_2)\n",
    "    print(f\"\\n  {i}. Question: {question}\")\n",
    "    print(f\"     Answer: {result['answer']}\")\n",
    "    print(f\"     Confidence: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Personal information (like the original example)\n",
    "context_3 = \"\"\"\n",
    "Alice is a data scientist who works at Atlassian in Sydney, Australia. She has been \n",
    "working there for 3 years and specializes in natural language processing and computer vision. \n",
    "Alice graduated from University of Sydney with a PhD in Computer Science in 2020.\n",
    "\"\"\"\n",
    "\n",
    "questions_3 = [\n",
    "    \"Where does Alice work?\",\n",
    "    \"What does Alice specialize in?\",\n",
    "    \"When did Alice graduate?\",\n",
    "    \"How long has Alice been working at Atlassian?\"\n",
    "]\n",
    "\n",
    "print(\"üìã EXAMPLE 3: Personal Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Context: {context_3.strip()}\")\n",
    "print(\"\\nüîç Questions and Answers:\")\n",
    "\n",
    "for i, question in enumerate(questions_3, 1):\n",
    "    result = question_answerer(question=question, context=context_3)\n",
    "    print(f\"\\n  {i}. Question: {question}\")\n",
    "    print(f\"     Answer: {result['answer']}\")\n",
    "    print(f\"     Confidence: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding the Results\n",
    "\n",
    "Let's examine what the question-answering pipeline returns and how to interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of a QA result\n",
    "sample_context = \"The quick brown fox jumps over the lazy dog. The fox is very clever and agile.\"\n",
    "sample_question = \"What animal jumps over the dog?\"\n",
    "\n",
    "result = question_answerer(question=sample_question, context=sample_context)\n",
    "\n",
    "print(\"üìä DETAILED RESULT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"\\nüîç Result Details:\")\n",
    "print(f\"  ‚Ä¢ Answer: '{result['answer']}'\")\n",
    "print(f\"  ‚Ä¢ Confidence Score: {result['score']:.6f}\")\n",
    "print(f\"  ‚Ä¢ Start Position: {result['start']} (character index)\")\n",
    "print(f\"  ‚Ä¢ End Position: {result['end']} (character index)\")\n",
    "\n",
    "# Show the exact text span\n",
    "answer_span = sample_context[result['start']:result['end']]\n",
    "print(f\"  ‚Ä¢ Extracted Span: '{answer_span}'\")\n",
    "\n",
    "# Confidence interpretation\n",
    "confidence = result['score']\n",
    "if confidence > 0.9:\n",
    "    confidence_level = \"Very High\"\n",
    "elif confidence > 0.7:\n",
    "    confidence_level = \"High\"\n",
    "elif confidence > 0.5:\n",
    "    confidence_level = \"Medium\"\n",
    "else:\n",
    "    confidence_level = \"Low\"\n",
    "\n",
    "print(f\"  ‚Ä¢ Confidence Level: {confidence_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Question Answering Function\n",
    "\n",
    "Let's create a helper function to make question answering more interactive and educational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, context, show_details=True):\n",
    "    \"\"\"\n",
    "    Ask a question about a given context and display the results.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        context (str): The context to search for answers\n",
    "        show_details (bool): Whether to show detailed analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: The question answering result\n",
    "    \"\"\"\n",
    "    result = question_answerer(question=question, context=context)\n",
    "    \n",
    "    if show_details:\n",
    "        print(\"‚ùì Question:\", question)\n",
    "        print(\"üí° Answer:\", result['answer'])\n",
    "        print(f\"üéØ Confidence: {result['score']:.4f}\")\n",
    "        \n",
    "        # Confidence emoji\n",
    "        if result['score'] > 0.8:\n",
    "            confidence_emoji = \"üü¢\"\n",
    "        elif result['score'] > 0.5:\n",
    "            confidence_emoji = \"üü°\"\n",
    "        else:\n",
    "            confidence_emoji = \"üî¥\"\n",
    "            \n",
    "        print(f\"{confidence_emoji} Reliability: {'High' if result['score'] > 0.8 else 'Medium' if result['score'] > 0.5 else 'Low'}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "test_context = \"Python is a high-level programming language created by Guido van Rossum in 1991. It emphasizes code readability and simplicity.\"\n",
    "\n",
    "print(\"üß™ TESTING INTERACTIVE FUNCTION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Context: {test_context}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "ask_question(\"Who created Python?\", test_context)\n",
    "ask_question(\"When was Python created?\", test_context)\n",
    "ask_question(\"What does Python emphasize?\", test_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Information\n",
    "\n",
    "Let's explore what model is being used by default in our question-answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the model being used\n",
    "print(\"üîç MODEL INFORMATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# The default QA pipeline typically uses DistilBERT\n",
    "print(\"Default Model: distilbert-base-cased-distilled-squad\")\n",
    "print(\"Model Type: DistilBERT (distilled BERT)\")\n",
    "print(\"Training Dataset: SQuAD (Stanford Question Answering Dataset)\")\n",
    "print(\"Model Size: ~66M parameters\")\n",
    "print(\"Use Case: Extractive Question Answering\")\n",
    "\n",
    "print(\"\\nüìä MODEL CHARACTERISTICS:\")\n",
    "print(\"‚Ä¢ Fast inference speed (distilled model)\")\n",
    "print(\"‚Ä¢ Good accuracy on reading comprehension tasks\")\n",
    "print(\"‚Ä¢ Extractive approach (finds answer spans in context)\")\n",
    "print(\"‚Ä¢ Pre-trained on large text corpus and fine-tuned on SQuAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Question Answering Pipeline**: Used Hugging Face's pipeline interface for easy QA tasks\n",
    "- **Extractive QA**: Understanding how models find answer spans within given context\n",
    "- **Confidence Scoring**: Interpreting model confidence levels for answer reliability\n",
    "- **Context Processing**: Working with different types of text contexts and questions\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use clear, specific questions for better results\n",
    "- Provide sufficient context containing the answer information\n",
    "- Check confidence scores to assess answer reliability\n",
    "- Understand that extractive QA finds spans, not generated answers\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced QA**: Explore more sophisticated question-answering models\n",
    "- **Custom Fine-tuning**: Learn to fine-tune QA models on specific domains\n",
    "- **Documentation**: Check [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/task_summary#question-answering)\n",
    "- **Related Notebooks**: Explore other notebooks in this series for more NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}