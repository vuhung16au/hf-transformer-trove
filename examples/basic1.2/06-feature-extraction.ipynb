{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/06-feature-extraction.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/06-feature-extraction.ipynb)\n",
    "\n",
    "# 06 - Feature Extraction: Extract Vector Representations of Text\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to extract meaningful vector representations from text using transformers\n",
    "- Different types of text embeddings: token-level, sentence-level, and document-level\n",
    "- Techniques for pooling transformer outputs into fixed-size vectors\n",
    "- Applications of text embeddings for similarity search and semantic analysis\n",
    "- Comparing different models for feature extraction tasks\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals\n",
    "- Understanding of transformer architectures (refer to previous notebooks)\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Introduction to Feature Extraction**: Understanding text embeddings and their applications\n",
    "2. **Basic Feature Extraction**: Using pre-trained models to extract embeddings\n",
    "3. **Pooling Strategies**: Different methods to create sentence-level representations\n",
    "4. **Model Comparison**: Comparing different architectures for feature extraction\n",
    "5. **Similarity and Search**: Practical applications using cosine similarity\n",
    "6. **Advanced Techniques**: Using specialized sentence transformer models\n",
    "7. **Visualization**: Plotting embeddings in 2D space\n",
    "8. **Summary and Best Practices**: Key takeaways and recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Feature Extraction\n",
    "\n",
    "**Feature extraction** from text involves converting human-readable text into numerical vectors (embeddings) that capture semantic meaning. These vectors can be used for:\n",
    "\n",
    "- **Semantic Search**: Finding similar documents or sentences\n",
    "- **Clustering**: Grouping similar texts together\n",
    "- **Classification**: Using embeddings as input features for downstream tasks\n",
    "- **Recommendation Systems**: Finding similar content based on text descriptions\n",
    "\n",
    "### Why Use Transformer Models for Feature Extraction?\n",
    "\n",
    "- **Contextual Understanding**: Transformers understand word meaning based on context\n",
    "- **Rich Representations**: Capture complex semantic relationships\n",
    "- **Transfer Learning**: Pre-trained models work well across different domains\n",
    "- **Flexible Output**: Can extract features at token, sentence, or document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch numpy matplotlib scikit-learn sentence-transformers\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer, \n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    COLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Running in Colab: {COLAB_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Set up device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Feature Extraction with BERT\n",
    "\n",
    "Let's start with extracting features using BERT, one of the most popular transformer models for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "print(f\"üì• Loading {model_name}...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully\")\n",
    "    print(f\"üìä Model size: {model.num_parameters():,} parameters\")\n",
    "    print(f\"üéØ Max sequence length: {tokenizer.model_max_length}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üí° Check internet connection or try a different model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for feature extraction\n",
    "sample_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on the rug.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Artificial intelligence transforms technology.\"\n",
    "]\n",
    "\n",
    "print(\"üìù Sample texts for feature extraction:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"  {i}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_basic(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Extract basic features from text using a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        model: Transformer model\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different types of embeddings\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Extract features without gradient computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get different representations\n",
    "    # last_hidden_state: [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    # pooler_output: [batch_size, hidden_size] (for BERT, this is CLS token representation)\n",
    "    pooler_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else None\n",
    "    \n",
    "    return {\n",
    "        'last_hidden_state': last_hidden_state.cpu(),\n",
    "        'pooler_output': pooler_output.cpu() if pooler_output is not None else None,\n",
    "        'tokens': tokenizer.tokenize(text),\n",
    "        'input_ids': inputs['input_ids'].cpu()\n",
    "    }\n",
    "\n",
    "# Extract features for first sample text\n",
    "sample_text = sample_texts[0]\n",
    "features = extract_features_basic(sample_text, model, tokenizer, device)\n",
    "\n",
    "print(f\"üìä Feature extraction results for: '{sample_text}'\")\n",
    "print(f\"  Tokens: {features['tokens']}\")\n",
    "print(f\"  Token count: {len(features['tokens'])}\")\n",
    "print(f\"  Last hidden state shape: {features['last_hidden_state'].shape}\")\n",
    "if features['pooler_output'] is not None:\n",
    "    print(f\"  Pooler output shape: {features['pooler_output'].shape}\")\n",
    "    print(f\"  Pooler output (first 10 dims): {features['pooler_output'][0][:10].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Pooling Strategies for Sentence Embeddings\n",
    "\n",
    "When working with transformer models, we often need to convert the variable-length token representations into fixed-size sentence embeddings. Here are several popular pooling strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pooling_strategies(last_hidden_state, attention_mask):\n",
    "    \"\"\"\n",
    "    Apply different pooling strategies to create sentence embeddings.\n",
    "    \n",
    "    Args:\n",
    "        last_hidden_state: Tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        attention_mask: Tensor of shape [batch_size, seq_len]\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with different pooled representations\n",
    "    \"\"\"\n",
    "    # 1. CLS Token (first token) - common for BERT\n",
    "    cls_embedding = last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "    \n",
    "    # 2. Mean Pooling - average over all tokens (excluding padding)\n",
    "    # Expand attention mask to match hidden_state dimensions\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    \n",
    "    # Apply mask and compute mean\n",
    "    masked_embeddings = last_hidden_state * attention_mask_expanded\n",
    "    sum_embeddings = torch.sum(masked_embeddings, 1)\n",
    "    sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_embedding = sum_embeddings / sum_mask\n",
    "    \n",
    "    # 3. Max Pooling - take maximum value across sequence dimension\n",
    "    # Set padded positions to large negative value before max pooling\n",
    "    masked_embeddings_max = last_hidden_state.clone()\n",
    "    masked_embeddings_max[attention_mask_expanded == 0] = -1e9\n",
    "    max_embedding = torch.max(masked_embeddings_max, 1)[0]\n",
    "    \n",
    "    # 4. Min Pooling - take minimum value across sequence dimension\n",
    "    masked_embeddings_min = last_hidden_state.clone()\n",
    "    masked_embeddings_min[attention_mask_expanded == 0] = 1e9\n",
    "    min_embedding = torch.min(masked_embeddings_min, 1)[0]\n",
    "    \n",
    "    return {\n",
    "        'cls_token': cls_embedding,\n",
    "        'mean_pooling': mean_embedding,\n",
    "        'max_pooling': max_embedding,\n",
    "        'min_pooling': min_embedding\n",
    "    }\n",
    "\n",
    "# Test pooling strategies\n",
    "def extract_sentence_embeddings(texts, model, tokenizer, device, pooling_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Extract sentence embeddings using specified pooling strategy.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        model: Transformer model\n",
    "        tokenizer: Tokenizer\n",
    "        device: Device\n",
    "        pooling_strategy: 'cls', 'mean', 'max', or 'min'\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of sentence embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Apply pooling\n",
    "        pooled = apply_pooling_strategies(\n",
    "            outputs.last_hidden_state, \n",
    "            inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Select requested strategy\n",
    "        if pooling_strategy == 'cls':\n",
    "            embedding = pooled['cls_token']\n",
    "        elif pooling_strategy == 'mean':\n",
    "            embedding = pooled['mean_pooling']\n",
    "        elif pooling_strategy == 'max':\n",
    "            embedding = pooled['max_pooling']\n",
    "        elif pooling_strategy == 'min':\n",
    "            embedding = pooled['min_pooling']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "        \n",
    "        all_embeddings.append(embedding.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Compare different pooling strategies\n",
    "pooling_strategies = ['cls', 'mean', 'max', 'min']\n",
    "pooling_results = {}\n",
    "\n",
    "print(\"üîÑ Comparing pooling strategies...\")\n",
    "for strategy in pooling_strategies:\n",
    "    embeddings = extract_sentence_embeddings(\n",
    "        sample_texts, \n",
    "        model, \n",
    "        tokenizer, \n",
    "        device, \n",
    "        pooling_strategy=strategy\n",
    "    )\n",
    "    pooling_results[strategy] = embeddings\n",
    "    print(f\"  {strategy.upper()} pooling shape: {embeddings.shape}\")\n",
    "\n",
    "print(\"‚úÖ Pooling strategies comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Similarity Analysis\n",
    "\n",
    "Now let's use the extracted features to compute semantic similarity between texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity matrix between all pairs of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape [n_texts, embedding_dim]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Similarity matrix of shape [n_texts, n_texts]\n",
    "    \"\"\"\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "def visualize_similarity_matrix(similarity_matrix, texts, strategy_name):\n",
    "    \"\"\"\n",
    "    Visualize similarity matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: Similarity matrix\n",
    "        texts: List of text strings for labels\n",
    "        strategy_name: Name of pooling strategy\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create labels (truncated for readability)\n",
    "    labels = [text[:30] + '...' if len(text) > 30 else text for text in texts]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        similarity_matrix, \n",
    "        annot=True, \n",
    "        fmt='.3f', \n",
    "        cmap='viridis', \n",
    "        xticklabels=labels, \n",
    "        yticklabels=labels,\n",
    "        cbar_kws={'label': 'Cosine Similarity'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Text Similarity Matrix - {strategy_name.upper()} Pooling', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Texts', fontweight='bold')\n",
    "    plt.ylabel('Texts', fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze similarity for mean pooling (typically most effective)\n",
    "mean_embeddings = pooling_results['mean']\n",
    "similarity_matrix = compute_similarity_matrix(mean_embeddings)\n",
    "\n",
    "print(\"üìä Similarity Analysis (Mean Pooling):\")\n",
    "print(\"\\nSimilarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Find most similar pairs\n",
    "n_texts = len(sample_texts)\n",
    "for i in range(n_texts):\n",
    "    for j in range(i+1, n_texts):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        print(f\"\\nTexts {i+1} & {j+1}: {similarity:.3f}\")\n",
    "        print(f\"  '{sample_texts[i]}'\")\n",
    "        print(f\"  '{sample_texts[j]}'\")\n",
    "\n",
    "# Visualize similarity matrix\n",
    "visualize_similarity_matrix(similarity_matrix, sample_texts, 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing Different Models\n",
    "\n",
    "Different transformer models can produce different quality embeddings. Let's compare several popular models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    'BERT Base': 'bert-base-uncased',\n",
    "    'DistilBERT': 'distilbert-base-uncased',\n",
    "    'RoBERTa': 'roberta-base',\n",
    "}\n",
    "\n",
    "def load_model_safely(model_name):\n",
    "    \"\"\"\n",
    "    Load model with error handling.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model) or (None, None) if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load models and extract embeddings\n",
    "model_embeddings = {}\n",
    "model_objects = {}\n",
    "\n",
    "print(\"üì• Loading and comparing different models...\")\n",
    "for model_display_name, model_name in models_to_compare.items():\n",
    "    print(f\"\\nüîÑ Processing {model_display_name} ({model_name})...\")\n",
    "    \n",
    "    tokenizer, model_obj = load_model_safely(model_name)\n",
    "    if tokenizer is None or model_obj is None:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {model_display_name} due to loading error\")\n",
    "        continue\n",
    "    \n",
    "    # Extract embeddings\n",
    "    try:\n",
    "        embeddings = extract_sentence_embeddings(\n",
    "            sample_texts, \n",
    "            model_obj, \n",
    "            tokenizer, \n",
    "            device, \n",
    "            pooling_strategy='mean'\n",
    "        )\n",
    "        model_embeddings[model_display_name] = embeddings\n",
    "        model_objects[model_display_name] = (tokenizer, model_obj)\n",
    "        \n",
    "        print(f\"  ‚úÖ {model_display_name}: {embeddings.shape}\")\n",
    "        print(f\"  üìä Parameters: {model_obj.num_parameters():,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to extract embeddings: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Successfully loaded {len(model_embeddings)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity patterns across different models\n",
    "def compare_model_similarities(model_embeddings, sample_texts):\n",
    "    \"\"\"\n",
    "    Compare similarity patterns across different models.\n",
    "    \n",
    "    Args:\n",
    "        model_embeddings: Dict of model embeddings\n",
    "        sample_texts: List of sample texts\n",
    "    \"\"\"\n",
    "    print(\"üîç Comparing similarity patterns across models:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Compare specific text pairs across models\n",
    "    text_pairs_to_compare = [\n",
    "        (0, 1),  # \"cat sat\" vs \"feline rested\" - should be similar\n",
    "        (0, 2),  # \"cat sat\" vs \"dog ran\" - different animals\n",
    "        (3, 4),  # \"machine learning\" vs \"artificial intelligence\" - similar domain\n",
    "    ]\n",
    "    \n",
    "    for i, j in text_pairs_to_compare:\n",
    "        print(f\"\\nüìù Comparing:\")\n",
    "        print(f\"  Text {i+1}: '{sample_texts[i]}'\")\n",
    "        print(f\"  Text {j+1}: '{sample_texts[j]}'\")\n",
    "        print(f\"  Similarities by model:\")\n",
    "        \n",
    "        for model_name, embeddings in model_embeddings.items():\n",
    "            similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            print(f\"    {model_name:12}: {similarity:.4f}\")\n",
    "\n",
    "# Run model comparison\n",
    "if model_embeddings:\n",
    "    compare_model_similarities(model_embeddings, sample_texts)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models successfully loaded for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Feature Extraction with Sentence Transformers\n",
    "\n",
    "For the best sentence embeddings, we can use models specifically trained for this task, such as those from the `sentence-transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use sentence-transformers if available\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"‚úÖ sentence-transformers library available\")\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  sentence-transformers not available. Installing...\")\n",
    "    print(\"üí° Run: pip install sentence-transformers\")\n",
    "\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    # Load a sentence transformer model\n",
    "    sentence_model_name = 'all-MiniLM-L6-v2'  # Fast and good quality\n",
    "    print(f\"üì• Loading sentence transformer: {sentence_model_name}\")\n",
    "    \n",
    "    try:\n",
    "        sentence_model = SentenceTransformer(sentence_model_name)\n",
    "        \n",
    "        # Extract sentence embeddings\n",
    "        sentence_embeddings = sentence_model.encode(sample_texts)\n",
    "        \n",
    "        print(f\"‚úÖ Sentence embeddings shape: {sentence_embeddings.shape}\")\n",
    "        print(f\"üìä Model max sequence length: {sentence_model.max_seq_length}\")\n",
    "        \n",
    "        # Compare with our manual approach\n",
    "        st_similarity_matrix = compute_similarity_matrix(sentence_embeddings)\n",
    "        \n",
    "        print(\"\\nüîç Sentence Transformer similarities:\")\n",
    "        for i in range(len(sample_texts)):\n",
    "            for j in range(i+1, len(sample_texts)):\n",
    "                similarity = st_similarity_matrix[i, j]\n",
    "                print(f\"  Texts {i+1} & {j+1}: {similarity:.4f}\")\n",
    "        \n",
    "        # Visualize sentence transformer similarities\n",
    "        visualize_similarity_matrix(st_similarity_matrix, sample_texts, 'Sentence Transformer')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with sentence transformer: {e}\")\n",
    "        SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"üìù Sentence transformers demo skipped - using manual approach instead\")\n",
    "    print(\"üí° Sentence transformers are specifically optimized for semantic similarity tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Practical Application - Semantic Search\n",
    "\n",
    "Let's implement a simple semantic search system using our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSemanticSearch:\n",
    "    \"\"\"\n",
    "    Simple semantic search system using pre-computed embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embeddings):\n",
    "        \"\"\"\n",
    "        Initialize with documents and their embeddings.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents\n",
    "            embeddings: Corresponding embeddings array\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def search(self, query_embedding, top_k=3):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Embedding of the query\n",
    "            top_k: Number of top results to return\n",
    "        \n",
    "        Returns:\n",
    "            list: List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Return results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.documents[idx], similarities[idx]))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create a larger document collection for search\n",
    "document_collection = [\n",
    "    \"The cat sat on the mat in the living room.\",\n",
    "    \"A feline rested comfortably on the soft rug.\",\n",
    "    \"The dog ran quickly through the green park.\",\n",
    "    \"Machine learning algorithms can solve complex problems.\",\n",
    "    \"Artificial intelligence is transforming modern technology.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    \"Climate change affects global weather patterns.\",\n",
    "    \"Python is a popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "print(\"üìö Document collection for semantic search:\")\n",
    "for i, doc in enumerate(document_collection, 1):\n",
    "    print(f\"  {i:2d}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for document collection\n",
    "print(\"üîÑ Extracting embeddings for document collection...\")\n",
    "\n",
    "# Use the best available model (sentence transformer if available, otherwise BERT)\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    doc_embeddings = sentence_model.encode(document_collection)\n",
    "    search_model_name = \"Sentence Transformer\"\n",
    "    \n",
    "    def encode_query(query):\n",
    "        return sentence_model.encode([query])[0]\n",
    "        \n",
    "elif 'BERT Base' in model_objects:\n",
    "    tokenizer, bert_model = model_objects['BERT Base']\n",
    "    doc_embeddings = extract_sentence_embeddings(\n",
    "        document_collection, bert_model, tokenizer, device, 'mean'\n",
    "    )\n",
    "    search_model_name = \"BERT Base\"\n",
    "    \n",
    "    def encode_query(query):\n",
    "        return extract_sentence_embeddings(\n",
    "            [query], bert_model, tokenizer, device, 'mean'\n",
    "        )[0]\n",
    "        \n",
    "else:\n",
    "    # Fallback to the original model\n",
    "    doc_embeddings = extract_sentence_embeddings(\n",
    "        document_collection, model, tokenizer, device, 'mean'\n",
    "    )\n",
    "    search_model_name = \"BERT Base (fallback)\"\n",
    "    \n",
    "    def encode_query(query):\n",
    "        return extract_sentence_embeddings(\n",
    "            [query], model, tokenizer, device, 'mean'\n",
    "        )[0]\n",
    "\n",
    "print(f\"‚úÖ Document embeddings extracted using {search_model_name}\")\n",
    "print(f\"üìä Embeddings shape: {doc_embeddings.shape}\")\n",
    "\n",
    "# Create search system\n",
    "search_system = SimpleSemanticSearch(document_collection, doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search with different queries\n",
    "test_queries = [\n",
    "    \"animal sitting on carpet\",\n",
    "    \"AI and machine intelligence\",\n",
    "    \"sunny weather conditions\",\n",
    "    \"programming languages for data analysis\"\n",
    "]\n",
    "\n",
    "print(\"üîç Semantic Search Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = encode_query(query)\n",
    "    \n",
    "    # Search\n",
    "    results = search_system.search(query_embedding, top_k=3)\n",
    "    \n",
    "    print(\"   Top matches:\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"   {i}. Score: {score:.4f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualizing Embeddings in 2D Space\n",
    "\n",
    "Let's visualize the high-dimensional embeddings in 2D space using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions for visualization\n",
    "def visualize_embeddings_2d(embeddings, texts, title=\"Text Embeddings Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional embeddings in 2D space using PCA.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: High-dimensional embeddings\n",
    "        texts: Corresponding text labels\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot points\n",
    "    scatter = plt.scatter(\n",
    "        embeddings_2d[:, 0], \n",
    "        embeddings_2d[:, 1], \n",
    "        c=range(len(texts)), \n",
    "        cmap='tab10', \n",
    "        s=100, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, txt in enumerate(texts):\n",
    "        # Truncate long texts for readability\n",
    "        label = txt[:40] + '...' if len(txt) > 40 else txt\n",
    "        plt.annotate(\n",
    "            f\"{i+1}: {label}\", \n",
    "            (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "            xytext=(5, 5), \n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(f'PC1 (explained variance: {pca.explained_variance_ratio_[0]:.2%})', \n",
    "               fontweight='bold')\n",
    "    plt.ylabel(f'PC2 (explained variance: {pca.explained_variance_ratio_[1]:.2%})', \n",
    "               fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"üìà Total explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Visualize document collection embeddings\n",
    "visualize_embeddings_2d(\n",
    "    doc_embeddings, \n",
    "    document_collection, \n",
    "    f\"Document Embeddings ({search_model_name}) - 2D Visualization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Best Practices and Tips\n",
    "\n",
    "Let's summarize the key best practices for feature extraction with transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for feature extraction.\n",
    "    \"\"\"\n",
    "    print(\"üéØ FEATURE EXTRACTION BEST PRACTICES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = {\n",
    "        \"Model Selection\": [\n",
    "            \"Use sentence-transformers models for semantic similarity tasks\",\n",
    "            \"BERT/RoBERTa are good general-purpose choices\",\n",
    "            \"DistilBERT offers good speed-quality trade-off\",\n",
    "            \"Consider domain-specific models for specialized tasks\"\n",
    "        ],\n",
    "        \"Pooling Strategies\": [\n",
    "            \"Mean pooling usually works better than CLS token for similarity\",\n",
    "            \"CLS token is good for classification tasks\",\n",
    "            \"Max pooling can capture important features but is less stable\",\n",
    "            \"Try different strategies and evaluate on your specific task\"\n",
    "        ],\n",
    "        \"Processing Tips\": [\n",
    "            \"Normalize embeddings for cosine similarity tasks\",\n",
    "            \"Batch process multiple texts for efficiency\",\n",
    "            \"Handle padding and attention masks properly\",\n",
    "            \"Use torch.no_grad() for inference to save memory\"\n",
    "        ],\n",
    "        \"Performance\": [\n",
    "            \"Move models to GPU when available\",\n",
    "            \"Consider model quantization for deployment\",\n",
    "            \"Cache embeddings for frequently used texts\",\n",
    "            \"Use appropriate batch sizes to avoid OOM errors\"\n",
    "        ],\n",
    "        \"Evaluation\": [\n",
    "            \"Test similarity results on known similar/dissimilar pairs\",\n",
    "            \"Use downstream task performance to validate embeddings\",\n",
    "            \"Visualize embeddings to understand model behavior\",\n",
    "            \"Compare multiple models on your specific use case\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\nüîπ {category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"   ‚Ä¢ {tip}\")\n",
    "\n",
    "# Example of proper embedding normalization\n",
    "def normalize_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Normalize embeddings to unit length for cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of embeddings\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized embeddings\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "# Demonstrate normalization effect\n",
    "print(\"üìä Embedding Normalization Example:\")\n",
    "sample_embedding = doc_embeddings[:3]  # First 3 embeddings\n",
    "normalized_embedding = normalize_embeddings(sample_embedding)\n",
    "\n",
    "print(f\"Original norms: {[np.linalg.norm(emb) for emb in sample_embedding]}\")\n",
    "print(f\"Normalized norms: {[np.linalg.norm(emb) for emb in normalized_embedding]}\")\n",
    "print(\"‚úÖ All normalized embeddings have unit length (norm = 1.0)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "\n",
    "- **Feature Extraction**: Converting text to meaningful vector representations using transformer models\n",
    "- **Pooling Strategies**: Different methods (CLS, mean, max, min) to create sentence-level embeddings\n",
    "- **Similarity Analysis**: Using cosine similarity to measure semantic similarity between texts\n",
    "- **Model Comparison**: Understanding trade-offs between different transformer architectures\n",
    "- **Practical Applications**: Implementing semantic search and embedding visualization\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "\n",
    "- **Model Selection**: Choose task-appropriate models (sentence-transformers for similarity, BERT for general use)\n",
    "- **Pooling Choice**: Mean pooling generally works better than CLS token for semantic similarity\n",
    "- **Normalization**: Normalize embeddings for cosine similarity tasks\n",
    "- **Efficiency**: Use batch processing, GPU acceleration, and proper memory management\n",
    "- **Evaluation**: Test on known similar/dissimilar pairs and visualize results\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Advanced Models**: Explore domain-specific transformer models\n",
    "- **Fine-tuning**: Fine-tune models for your specific similarity tasks\n",
    "- **Scalability**: Implement approximate similarity search with FAISS or similar libraries\n",
    "- **Applications**: Build recommendation systems, document clustering, or question-answering systems\n",
    "\n",
    "### üìö Further Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [Hugging Face Transformers Guide](https://huggingface.co/docs/transformers/index)\n",
    "- [Understanding BERT Embeddings](https://jalammar.github.io/illustrated-bert/)\n",
    "- [Text Embeddings and Similarity Search](https://www.pinecone.io/learn/vector-embeddings/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}