{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/01-sentiment-analysis.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/01-sentiment-analysis.ipynb)\n",
    "\n",
    "# Basic Sentiment Analysis with Hugging Face Transformers\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What sentiment analysis is and why it's important\n",
    "- How to use Hugging Face transformers pipeline for sentiment analysis\n",
    "- The structure of sentiment analysis outputs\n",
    "- Basic text preprocessing considerations\n",
    "- How transformers perform sentiment classification\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python programming\n",
    "- Basic knowledge of Natural Language Processing (NLP) concepts\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Introduction**: What is sentiment analysis?\n",
    "2. **Setup**: Installing and importing required libraries\n",
    "3. **Core Implementation**: Using transformers pipeline\n",
    "4. **Understanding Output**: Interpreting results and confidence scores\n",
    "5. **Advanced Examples**: Testing different text inputs\n",
    "6. **Summary**: Key takeaways and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Sentiment Analysis\n",
    "\n",
    "**Sentiment Analysis** is a fundamental Natural Language Processing task that determines the emotional tone or attitude expressed in text. It's widely used in:\n",
    "\n",
    "- **Social Media Monitoring**: Understanding public opinion about brands or products\n",
    "- **Customer Feedback**: Analyzing reviews and support tickets\n",
    "- **Market Research**: Gauging consumer sentiment\n",
    "- **Content Moderation**: Identifying negative or harmful content\n",
    "\n",
    "### How Does It Work?\n",
    "\n",
    "Modern sentiment analysis uses transformer models that:\n",
    "1. **Tokenize** the input text into smaller pieces\n",
    "2. **Encode** these tokens into numerical representations\n",
    "3. **Process** the sequence using attention mechanisms\n",
    "4. **Classify** the overall sentiment with a confidence score\n",
    "\n",
    "### Common Sentiment Categories\n",
    "- **POSITIVE**: Expresses happiness, satisfaction, or approval\n",
    "- **NEGATIVE**: Expresses sadness, dissatisfaction, or disapproval\n",
    "- **NEUTRAL**: Expresses neither positive nor negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install transformers torch datasets\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device detection for optimal performance\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Detect the best available device\n",
    "device = get_device()\n",
    "\n",
    "print(\"\\nüìö Setup completed successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Implementation: Using Transformers Pipeline\n",
    "\n",
    "The Hugging Face `pipeline` function provides the easiest way to perform sentiment analysis. It handles:\n",
    "- Model loading and configuration\n",
    "- Text preprocessing and tokenization\n",
    "- Model inference and postprocessing\n",
    "- Result formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# This uses a default model: distilbert-base-uncased-finetuned-sst-2-english\n",
    "print(\"üîÑ Loading sentiment analysis model...\")\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìä Model: {classifier.model.config.name_or_path}\")\n",
    "print(f\"üéØ Task: {classifier.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Required Example\n",
    "\n",
    "Let's test our sentiment analysis pipeline with the specific example from the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the required example\n",
    "test_text = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "print(f\"üìù Input text: '{test_text}'\")\n",
    "print(\"\\nüîç Performing sentiment analysis...\")\n",
    "\n",
    "# Get the sentiment prediction\n",
    "result = classifier(test_text)\n",
    "\n",
    "print(f\"\\nüìä Result: {result}\")\n",
    "\n",
    "# Extract and display details\n",
    "sentiment = result[0]\n",
    "label = sentiment['label']\n",
    "score = sentiment['score']\n",
    "\n",
    "print(f\"\\n‚ú® Analysis:\")\n",
    "print(f\"   Sentiment: {label}\")\n",
    "print(f\"   Confidence: {score:.4f} ({score*100:.1f}%)\")\n",
    "print(f\"   Interpretation: The model is {score*100:.1f}% confident this text expresses {label.lower()} sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output Structure\n",
    "\n",
    "Let's break down what the sentiment analysis pipeline returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the output structure\n",
    "print(\"üîç Understanding the output structure:\")\n",
    "print(f\"   Type: {type(result)}\")\n",
    "print(f\"   Length: {len(result)}\")\n",
    "print(f\"   First item type: {type(result[0])}\")\n",
    "print(f\"   Keys: {result[0].keys()}\")\n",
    "\n",
    "print(\"\\nüìã Output Format Explanation:\")\n",
    "print(\"   ‚Ä¢ Returns a list containing one dictionary per input text\")\n",
    "print(\"   ‚Ä¢ Each dictionary has two keys:\")\n",
    "print(\"     - 'label': The predicted sentiment class (POSITIVE/NEGATIVE)\")\n",
    "print(\"     - 'score': The confidence score (0.0 to 1.0)\")\n",
    "print(\"   ‚Ä¢ Higher scores indicate higher confidence in the prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Examples: Testing Different Text Types\n",
    "\n",
    "Let's explore how the model handles various types of text to better understand its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various text examples\n",
    "test_examples = [\n",
    "    \"This movie is absolutely amazing! I loved every minute of it.\",\n",
    "    \"The service at this restaurant was terrible and the food was cold.\",\n",
    "    \"The weather today is okay, nothing special.\",\n",
    "    \"I can't believe how wonderful this product is!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The meeting is scheduled for 3 PM tomorrow.\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing multiple examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    result = classifier(text)\n",
    "    sentiment = result[0]\n",
    "    \n",
    "    # Use emoji for visual representation\n",
    "    emoji = \"üòä\" if sentiment['label'] == 'POSITIVE' else \"üòû\"\n",
    "    \n",
    "    print(f\"\\n{i}. Text: '{text}'\")\n",
    "    print(f\"   Result: {sentiment['label']} {emoji} (confidence: {sentiment['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "The pipeline can also process multiple texts at once, which is more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example\n",
    "batch_texts = [\n",
    "    \"I love this new feature!\",\n",
    "    \"This is disappointing.\",\n",
    "    \"Not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Batch processing example:\")\n",
    "print(\"Input texts:\", batch_texts)\n",
    "\n",
    "# Process all texts at once\n",
    "batch_results = classifier(batch_texts)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for text, result in zip(batch_texts, batch_results):\n",
    "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòû\"\n",
    "    print(f\"  '{text}' ‚Üí {result['label']} {emoji} ({result['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Information and Technical Details\n",
    "\n",
    "Let's explore some technical details about the model being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model details\n",
    "print(\"üîß Model Technical Details:\")\n",
    "print(f\"   Model Name: {classifier.model.config.name_or_path}\")\n",
    "print(f\"   Model Type: {classifier.model.config.model_type}\")\n",
    "print(f\"   Number of Labels: {classifier.model.config.num_labels}\")\n",
    "print(f\"   Label Mapping: {classifier.model.config.id2label}\")\n",
    "\n",
    "# Model size information\n",
    "total_params = sum(p.numel() for p in classifier.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in classifier.model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Size:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   Model Size: ~{total_params/1_000_000:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Considerations\n",
    "\n",
    "Understanding performance characteristics helps in production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance timing\n",
    "def time_inference(texts, num_runs=5):\n",
    "    \"\"\"Time the inference process for performance analysis.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = classifier(texts)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Test with single text\n",
    "single_text = \"This is a test for performance measurement.\"\n",
    "single_times = time_inference(single_text)\n",
    "\n",
    "# Test with batch\n",
    "batch_text = [single_text] * 10\n",
    "batch_times = time_inference(batch_text)\n",
    "\n",
    "print(\"‚è±Ô∏è Performance Analysis:\")\n",
    "print(f\"   Single text (avg): {sum(single_times)/len(single_times):.3f}s\")\n",
    "print(f\"   Batch of 10 (avg): {sum(batch_times)/len(batch_times):.3f}s\")\n",
    "print(f\"   Speedup factor: {(sum(single_times)*10)/(sum(batch_times)*len(batch_times)):.1f}x\")\n",
    "print(\"\\nüí° Tip: Batch processing is more efficient for multiple texts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Sentiment Analysis**: Understanding how to classify text emotional tone using transformers\n",
    "- **Hugging Face Pipeline**: Using the high-level pipeline API for quick sentiment analysis\n",
    "- **Output Interpretation**: Understanding confidence scores and label predictions\n",
    "- **Batch Processing**: Efficiently processing multiple texts at once\n",
    "- **Performance Considerations**: Understanding timing and efficiency trade-offs\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use batch processing for multiple texts to improve efficiency\n",
    "- Always check confidence scores to understand prediction reliability\n",
    "- Consider the model's training data and potential biases in real-world applications\n",
    "- Monitor performance characteristics for production deployment\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced Notebooks**: Explore custom model fine-tuning for specific domains\n",
    "- **Model Comparison**: Compare different sentiment analysis models\n",
    "- **Documentation**: Review [Hugging Face Transformers documentation](https://huggingface.co/docs/transformers/) for advanced usage\n",
    "- **External Resources**: Explore the [Model Hub](https://huggingface.co/models?pipeline_tag=text-classification) for specialized models\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}