{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/04-mask-filling.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.2/04-mask-filling.ipynb)\n",
    "\n",
    "# 04 - Mask Filling with Hugging Face Transformers\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What mask filling is and how it works\n",
    "- How to use the fill-mask pipeline in Hugging Face\n",
    "- Different models suitable for mask filling tasks\n",
    "- How to interpret and work with prediction results\n",
    "- Advanced mask filling techniques and use cases\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of transformer architectures\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. Introduction to Mask Filling\n",
    "2. Basic Fill-Mask Pipeline\n",
    "3. Working with Different Models\n",
    "4. Advanced Techniques\n",
    "5. Practical Applications\n",
    "6. Performance Considerations\n",
    "\n",
    "## What is Mask Filling?\n",
    "\n",
    "Mask filling (also called masked language modeling) is a task where the model predicts missing words in a sentence. The missing words are represented by special `<mask>` tokens. This technique is fundamental to how models like BERT were pre-trained and is useful for:\n",
    "\n",
    "- **Text completion**: Suggesting words to complete sentences\n",
    "- **Error correction**: Finding and correcting typos or grammatical errors\n",
    "- **Creative writing**: Generating alternative word choices\n",
    "- **Language understanding**: Testing model comprehension of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only once)\n",
    "# !pip install transformers torch datasets\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device detection function\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the optimal device\n",
    "device = get_device()\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Fill-Mask Pipeline\n",
    "\n",
    "The simplest way to perform mask filling is using the Hugging Face pipeline. Let's start with the basic example from the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fill-mask pipeline\n",
    "# This will automatically download and use a default model (usually BERT-based)\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "# Basic mask filling example\n",
    "text = \"This course will teach you all about <mask> models.\"\n",
    "result = unmasker(text, top_k=2)\n",
    "\n",
    "print(\"Basic Mask Filling Example:\")\n",
    "print(f\"Input: {text}\")\n",
    "print(\"\\nPredictions:\")\n",
    "for i, prediction in enumerate(result, 1):\n",
    "    print(f\"{i}. Token: '{prediction['token_str']}' | Score: {prediction['score']:.4f}\")\n",
    "    print(f\"   Complete sentence: {prediction['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "Let's break down what each field in the result means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of results\n",
    "def analyze_mask_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and explain mask filling results in detail.\n",
    "    \n",
    "    Args:\n",
    "        results: Output from fill-mask pipeline\n",
    "    \"\"\"\n",
    "    print(\"üìä DETAILED RESULT ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nPrediction #{i}:\")\n",
    "        print(f\"  üéØ Token: '{result['token_str']}'\")\n",
    "        print(f\"  üìà Confidence Score: {result['score']:.4f} ({result['score']*100:.2f}%)\")\n",
    "        print(f\"  üî¢ Token ID: {result['token']}\")\n",
    "        print(f\"  üìù Complete Sentence: {result['sequence']}\")\n",
    "        \n",
    "        # Confidence level interpretation\n",
    "        if result['score'] > 0.5:\n",
    "            confidence = \"Very High\"\n",
    "        elif result['score'] > 0.2:\n",
    "            confidence = \"High\"\n",
    "        elif result['score'] > 0.1:\n",
    "            confidence = \"Medium\"\n",
    "        else:\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        print(f\"  üí° Confidence Level: {confidence}\")\n",
    "\n",
    "# Analyze our previous results\n",
    "analyze_mask_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploring Different Contexts\n",
    "\n",
    "Let's try mask filling with various types of sentences to see how context affects predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different context examples\n",
    "examples = [\n",
    "    \"The weather today is very <mask>.\",\n",
    "    \"I need to buy some <mask> from the grocery store.\",\n",
    "    \"The capital of France is <mask>.\",\n",
    "    \"Machine learning is a subset of <mask> intelligence.\",\n",
    "    \"The <mask> is the largest planet in our solar system.\",\n",
    "    \"Python is a popular <mask> language.\"\n",
    "]\n",
    "\n",
    "print(\"üîç EXPLORING DIFFERENT CONTEXTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"\\n{i}. Input: {example}\")\n",
    "    \n",
    "    try:\n",
    "        predictions = unmasker(example, top_k=3)\n",
    "        print(\"   Top predictions:\")\n",
    "        \n",
    "        for j, pred in enumerate(predictions, 1):\n",
    "            print(f\"      {j}. {pred['token_str']} (score: {pred['score']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with Different Models\n",
    "\n",
    "Different models have different strengths. Let's compare a few popular models for mask filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to compare\n",
    "models_to_try = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\", \n",
    "    \"roberta-base\",\n",
    "    \"albert-base-v2\"\n",
    "]\n",
    "\n",
    "test_sentence = \"The best way to learn <mask> is through practice.\"\n",
    "\n",
    "print(\"üèÜ MODEL COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Test sentence: {test_sentence}\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name in models_to_try:\n",
    "    print(f\"\\nüì± Testing {model_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Create pipeline with specific model\n",
    "        model_unmasker = pipeline(\n",
    "            \"fill-mask\", \n",
    "            model=model_name,\n",
    "            device=0 if device.type == 'cuda' else -1  # Use GPU if available\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model_unmasker(test_sentence, top_k=2)\n",
    "        model_results[model_name] = predictions\n",
    "        \n",
    "        print(\"   Top predictions:\")\n",
    "        for i, pred in enumerate(predictions, 1):\n",
    "            print(f\"      {i}. '{pred['token_str']}' (score: {pred['score']:.4f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error with {model_name}: {e}\")\n",
    "        print(\"   üí° This might be due to memory constraints or model availability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis\n",
    "\n",
    "Let's analyze the differences between models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model predictions\n",
    "if model_results:\n",
    "    print(\"üìä MODEL PREDICTION COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, predictions in model_results.items():\n",
    "        if predictions:  # Check if we got results\n",
    "            top_prediction = predictions[0]\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Top Prediction': top_prediction['token_str'],\n",
    "                'Confidence': f\"{top_prediction['score']:.4f}\",\n",
    "                'Percentage': f\"{top_prediction['score']*100:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    if comparison_data:\n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        print(df_comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüéØ Key Observations:\")\n",
    "        print(\"- Different models may predict different words\")\n",
    "        print(\"- Confidence scores vary between models\")\n",
    "        print(\"- Some models are more confident in their predictions\")\n",
    "        print(\"- Context understanding differs across architectures\")\n",
    "else:\n",
    "    print(\"No model results to compare. This might be due to resource constraints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Mask Filling Techniques\n",
    "\n",
    "### Multiple Masks\n",
    "\n",
    "Some models can handle multiple masks in a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple masks example (note: not all models support this well)\n",
    "print(\"üé≠ MULTIPLE MASKS EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Single mask first for comparison\n",
    "single_mask = \"I love <mask> programming.\"\n",
    "print(f\"Single mask: {single_mask}\")\n",
    "\n",
    "try:\n",
    "    single_result = unmasker(single_mask, top_k=3)\n",
    "    print(\"Predictions:\")\n",
    "    for i, pred in enumerate(single_result, 1):\n",
    "        print(f\"  {i}. {pred['token_str']} (score: {pred['score']:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Note about multiple masks\n",
    "print(\"\\nüí° Note about multiple masks:\")\n",
    "print(\"Most fill-mask models are designed for single mask prediction.\")\n",
    "print(\"For multiple masks, you typically need to:\")\n",
    "print(\"1. Process one mask at a time\")\n",
    "print(\"2. Use specialized multi-mask models\")\n",
    "print(\"3. Use text generation models instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Processing Function\n",
    "\n",
    "Let's create a more sophisticated mask filling function with additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_mask_filling(text, model_name=\"bert-base-uncased\", top_k=5, min_score=0.01):\n",
    "    \"\"\"\n",
    "    Advanced mask filling with filtering and analysis.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with <mask> token\n",
    "        model_name: Model to use for prediction\n",
    "        top_k: Number of top predictions to return\n",
    "        min_score: Minimum confidence score to include\n",
    "    \n",
    "    Returns:\n",
    "        Filtered and analyzed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create pipeline\n",
    "        pipe = pipeline(\n",
    "            \"fill-mask\", \n",
    "            model=model_name,\n",
    "            device=0 if device.type == 'cuda' else -1\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = pipe(text, top_k=top_k)\n",
    "        \n",
    "        # Filter by minimum score\n",
    "        filtered_predictions = [\n",
    "            pred for pred in predictions \n",
    "            if pred['score'] >= min_score\n",
    "        ]\n",
    "        \n",
    "        # Add analysis\n",
    "        total_score = sum(pred['score'] for pred in filtered_predictions)\n",
    "        \n",
    "        results = {\n",
    "            'input': text,\n",
    "            'model': model_name,\n",
    "            'predictions': filtered_predictions,\n",
    "            'total_predictions': len(filtered_predictions),\n",
    "            'confidence_sum': total_score,\n",
    "            'top_prediction_confidence': filtered_predictions[0]['score'] if filtered_predictions else 0\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test the advanced function\n",
    "test_text = \"Artificial <mask> is transforming many industries.\"\n",
    "advanced_result = advanced_mask_filling(test_text, top_k=5, min_score=0.05)\n",
    "\n",
    "print(\"üöÄ ADVANCED MASK FILLING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'error' not in advanced_result:\n",
    "    print(f\"Input: {advanced_result['input']}\")\n",
    "    print(f\"Model: {advanced_result['model']}\")\n",
    "    print(f\"Total valid predictions: {advanced_result['total_predictions']}\")\n",
    "    print(f\"Sum of confidence scores: {advanced_result['confidence_sum']:.4f}\")\n",
    "    \n",
    "    print(\"\\nFiltered predictions (min score 0.05):\")\n",
    "    for i, pred in enumerate(advanced_result['predictions'], 1):\n",
    "        print(f\"{i:2d}. '{pred['token_str']:12}' | Score: {pred['score']:.4f} | {pred['sequence']}\")\nelse:\n    print(f\"Error: {advanced_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Applications\n",
    "\n",
    "Let's explore some real-world applications of mask filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical application examples\n",
    "practical_examples = {\n",
    "    \"Text Completion\": [\n",
    "        \"The meeting is scheduled for <mask> morning.\",\n",
    "        \"Please send the report by <mask>.\",\n",
    "        \"The new <mask> will improve our productivity.\"\n",
    "    ],\n",
    "    \"Creative Writing\": [\n",
    "        \"The mysterious <mask> appeared at midnight.\",\n",
    "        \"She walked through the <mask> forest.\",\n",
    "        \"The ancient <mask> held many secrets.\"\n",
    "    ],\n",
    "    \"Educational Context\": [\n",
    "        \"The process of photosynthesis occurs in <mask>.\",\n",
    "        \"Newton's <mask> law states that every action has an equal and opposite reaction.\",\n",
    "        \"The <mask> War lasted from 1914 to 1918.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üéØ PRACTICAL APPLICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, examples in practical_examples.items():\n",
    "    print(f\"\\nüìö {category.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. {example}\")\n",
    "        \n",
    "        try:\n",
    "            predictions = unmasker(example, top_k=2)\n",
    "            print(\"   Suggestions:\")\n",
    "            for j, pred in enumerate(predictions, 1):\n",
    "                print(f\"      {j}. {pred['token_str']} (confidence: {pred['score']:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation\n",
    "\n",
    "Mask filling can help understand how context affects word meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word sense disambiguation examples\n",
    "context_examples = [\n",
    "    (\"The bank of the river was muddy.\", \"Financial vs. Geographic\"),\n",
    "    (\"He went to the <mask> to deposit money.\", \"Should predict 'bank' (financial)\"),\n",
    "    (\"They sat on the <mask> of the river.\", \"Should predict 'bank' (geographic)\"),\n",
    "    (\"The bat flew through the cave.\", \"Animal vs. Sports equipment\"),\n",
    "    (\"He swung the <mask> at the cricket ball.\", \"Should predict 'bat' (cricket)\"),\n",
    "    (\"The <mask> hung upside down from the tree.\", \"Should predict 'bat' (animal)\")\n",
    "]\n",
    "\n",
    "print(\"üß† WORD SENSE DISAMBIGUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for example, explanation in context_examples:\n",
    "    if \"<mask>\" in example:\n",
    "        print(f\"\\nExample: {example}\")\n",
    "        print(f\"Expected: {explanation}\")\n",
    "        \n",
    "        try:\n",
    "            predictions = unmasker(example, top_k=3)\n",
    "            print(\"Predictions:\")\n",
    "            for i, pred in enumerate(predictions, 1):\n",
    "                print(f\"  {i}. {pred['token_str']} (score: {pred['score']:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nReference: {example} | {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance and Optimization\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "For multiple texts, batch processing can be more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Batch processing example\n",
    "batch_texts = [\n",
    "    \"The weather is <mask> today.\",\n",
    "    \"I love eating <mask> for breakfast.\",\n",
    "    \"The movie was <mask> entertaining.\",\n",
    "    \"She works as a <mask> engineer.\",\n",
    "    \"The book is very <mask> to read.\"\n",
    "]\n",
    "\n",
    "print(\"‚ö° BATCH PROCESSING PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Individual processing\n",
    "print(\"\\n1. Individual Processing:\")\n",
    "start_time = time.time()\n",
    "\n",
    "individual_results = []\n",
    "for text in batch_texts:\n",
    "    try:\n",
    "        result = unmasker(text, top_k=1)\n",
    "        individual_results.append((text, result[0]['token_str'], result[0]['score']))\n",
    "    except Exception as e:\n",
    "        individual_results.append((text, \"ERROR\", 0.0))\n",
    "\n",
    "individual_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Time taken: {individual_time:.3f} seconds\")\n",
    "print(\"   Results:\")\n",
    "for text, prediction, score in individual_results:\n",
    "    print(f\"     '{prediction}' for '{text}' (score: {score:.3f})\")\n",
    "\n",
    "# Note about batch processing\n",
    "print(\"\\nüí° Note about batch processing:\")\n",
    "print(\"- The basic pipeline processes one text at a time\")\n",
    "print(\"- For true batch processing, you'd need to use the model directly\")\n",
    "print(\"- Batch processing is more memory efficient for large datasets\")\n",
    "print(f\"- Current processing speed: {len(batch_texts)/individual_time:.1f} texts/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tips and memory management\n",
    "print(\"üîß PERFORMANCE OPTIMIZATION TIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. üöÄ Model Selection:\")\n",
    "print(\"   - DistilBERT: Faster, smaller (66M params)\")\n",
    "print(\"   - BERT-base: Better accuracy (110M params)\")\n",
    "print(\"   - RoBERTa: Often better performance (125M params)\")\n",
    "print(\"   - ALBERT: More efficient architecture (12M params)\")\n",
    "\n",
    "print(\"\\n2. üíæ Memory Management:\")\n",
    "print(\"   - Use device=0 for GPU acceleration\")\n",
    "print(\"   - Use torch_dtype=torch.float16 for half precision\")\n",
    "print(\"   - Clear GPU cache: torch.cuda.empty_cache()\")\n",
    "\n",
    "print(\"\\n3. ‚ö° Speed Optimization:\")\n",
    "print(\"   - Reduce top_k for faster inference\")\n",
    "print(\"   - Use smaller models for real-time applications\")\n",
    "print(\"   - Cache models to avoid reloading\")\n",
    "\n",
    "# Memory usage check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nüìä Current GPU Memory Usage:\")\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    cached = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"   Allocated: {allocated:.2f}GB\")\n",
    "    print(f\"   Cached: {cached:.2f}GB\")\n",
    "    \n",
    "    # Clear cache if needed\n",
    "    if cached > 1.0:  # If using more than 1GB\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"   üßπ Cleared GPU cache\")\n",
    "else:\n",
    "    print(\"\\nüìä Using CPU - consider upgrading to GPU for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "\n",
    "- **Mask Filling Fundamentals**: Understanding how masked language models predict missing words using context\n",
    "- **Pipeline Usage**: Using the `fill-mask` pipeline for quick and easy mask filling\n",
    "- **Model Comparison**: Different models (BERT, DistilBERT, RoBERTa, ALBERT) have different strengths and performance characteristics\n",
    "- **Result Interpretation**: Understanding confidence scores and how to filter and analyze predictions\n",
    "- **Context Sensitivity**: How surrounding words influence predictions and word sense disambiguation\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "\n",
    "- **Model Selection**: Choose models based on your speed vs. accuracy requirements\n",
    "- **Device Optimization**: Use GPU acceleration when available for better performance\n",
    "- **Result Filtering**: Apply minimum confidence thresholds to get more reliable predictions\n",
    "- **Error Handling**: Always implement proper error handling for robust applications\n",
    "- **Memory Management**: Monitor and optimize memory usage, especially with large models\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Advanced Applications**: Explore using mask filling for text correction and completion systems\n",
    "- **Custom Training**: Learn how to fine-tune models for domain-specific mask filling\n",
    "- **Integration**: Combine mask filling with other NLP tasks for comprehensive applications\n",
    "- **Production Deployment**: Scale mask filling systems for real-world applications\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "- Mask filling is a powerful technique for text completion, error correction, and understanding language context\n",
    "- Different models excel in different scenarios - choose based on your specific needs\n",
    "- Context is crucial - the same mask can have very different predictions based on surrounding words\n",
    "- Performance optimization involves balancing model size, accuracy, and computational resources\n",
    "- Always consider confidence scores when using predictions in real applications\n",
    "\n",
    "Mask filling is a fundamental NLP technique that demonstrates the power of transformer models to understand and generate human language. The concepts learned here form the foundation for more advanced NLP applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}