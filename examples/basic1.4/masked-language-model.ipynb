{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/masked-language-model.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/masked-language-model.ipynb)\n",
    "\n",
    "# Masked Language Model Demonstration\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What masked language modeling is and how it works\n",
    "- How to use Hugging Face transformers for mask filling\n",
    "- How to interpret model predictions and confidence scores\n",
    "- Basic applications of masked language models\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Setup**: Import libraries and set up environment\n",
    "2. **Basic Masked Language Modeling**: Using the fill-mask pipeline\n",
    "3. **Understanding Results**: Interpreting predictions and confidence scores\n",
    "4. **Practical Examples**: Testing with different contexts\n",
    "5. **Summary**: Key takeaways and next steps\n",
    "\n",
    "## What is Masked Language Modeling?\n",
    "\n",
    "Masked Language Modeling (MLM) is a self-supervised learning task where the model learns to predict missing words in a sentence. The missing words are represented by a special `<mask>` token. This is how models like BERT were pre-trained to understand language context and semantics.\n",
    "\n",
    "**Key Applications:**\n",
    "- Text completion and suggestion\n",
    "- Grammar and spell checking\n",
    "- Language understanding evaluation\n",
    "- Creative writing assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install transformers torch\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the optimal device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Masked Language Modeling\n",
    "\n",
    "The Hugging Face `transformers` library provides a simple pipeline interface for masked language modeling. Let's start with a basic example.\n",
    "\n",
    "The fill-mask pipeline automatically loads a pre-trained model (typically BERT-based) that's been trained to predict masked tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fill-mask pipeline\n",
    "# This automatically downloads and uses a default masked language model\n",
    "print(\"üì• Loading fill-mask pipeline...\")\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "print(f\"‚úÖ Pipeline loaded successfully!\")\n",
    "print(f\"üìä Default model: {unmasker.model.config.name_or_path}\")\n",
    "print(f\"üè∑Ô∏è  Model type: {unmasker.model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic masked language modeling example\n",
    "text = \"The capital of France is <mask>.\"\n",
    "\n",
    "print(f\"üéØ Input text: {text}\")\n",
    "print(\"\\nü§ñ Model predictions:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Get predictions (top 3 most likely words)\n",
    "predictions = unmasker(text, top_k=3)\n",
    "\n",
    "# Display results\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"{i}. '{pred['token_str']}' (confidence: {pred['score']:.3f})\")\n",
    "    print(f\"   Complete sentence: {pred['sequence']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Results\n",
    "\n",
    "Let's examine what the model returns and how to interpret the confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_results(predictions):\n",
    "    \"\"\"\n",
    "    Analyze and explain masked language model predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dictionaries from fill-mask pipeline\n",
    "    \"\"\"\n",
    "    print(\"üìä PREDICTION ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    total_confidence = sum(pred['score'] for pred in predictions)\n",
    "    \n",
    "    print(f\"Number of predictions: {len(predictions)}\")\n",
    "    print(f\"Total confidence across top predictions: {total_confidence:.3f}\")\n",
    "    print(f\"Average confidence: {total_confidence/len(predictions):.3f}\\n\")\n",
    "    \n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        confidence_level = (\n",
    "            \"Very High\" if pred['score'] > 0.5 else\n",
    "            \"High\" if pred['score'] > 0.2 else\n",
    "            \"Medium\" if pred['score'] > 0.1 else\n",
    "            \"Low\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Prediction #{i}:\")\n",
    "        print(f\"  Token: '{pred['token_str']}'\")\n",
    "        print(f\"  Score: {pred['score']:.4f} ({pred['score']*100:.2f}%)\")\n",
    "        print(f\"  Confidence Level: {confidence_level}\")\n",
    "        print(f\"  Token ID: {pred['token']}\")\n",
    "        print()\n",
    "\n",
    "# Analyze our previous predictions\n",
    "analyze_prediction_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Examples\n",
    "\n",
    "Let's test the masked language model with different types of sentences to see how context affects predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various test examples\n",
    "test_examples = [\n",
    "    \"I love to eat <mask> for breakfast.\",\n",
    "    \"The weather today is very <mask>.\",\n",
    "    \"She works as a <mask> in the hospital.\",\n",
    "    \"The <mask> is shining brightly today.\",\n",
    "    \"Machine learning is a subset of <mask> intelligence.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING DIFFERENT CONTEXTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\n{i}. Input: {example}\")\n",
    "    \n",
    "    try:\n",
    "        predictions = unmasker(example, top_k=2)\n",
    "        print(\"   Top predictions:\")\n",
    "        \n",
    "        for j, pred in enumerate(predictions, 1):\n",
    "            print(f\"      {j}. {pred['token_str']} (score: {pred['score']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Sensitivity Example\n",
    "\n",
    "Let's demonstrate how the same word can have different meanings based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context sensitivity examples\n",
    "context_examples = [\n",
    "    (\"He went to the <mask> to deposit money.\", \"Financial context\"),\n",
    "    (\"They sat on the <mask> of the river.\", \"Geographic context\"),\n",
    "    (\"The <mask> flew through the dark cave.\", \"Animal context\"),\n",
    "    (\"He swung the <mask> at the baseball.\", \"Sports context\")\n",
    "]\n",
    "\n",
    "print(\"üß† CONTEXT SENSITIVITY DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for example, context_type in context_examples:\n",
    "    print(f\"\\nContext: {context_type}\")\n",
    "    print(f\"Sentence: {example}\")\n",
    "    \n",
    "    predictions = unmasker(example, top_k=3)\n",
    "    print(\"Predictions:\")\n",
    "    \n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['token_str']} ({pred['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Masked Language Modeling**: Understanding how models predict missing words using context\n",
    "- **Fill-Mask Pipeline**: Using Hugging Face transformers for simple mask filling tasks\n",
    "- **Prediction Interpretation**: Understanding confidence scores and model uncertainty\n",
    "- **Context Sensitivity**: How surrounding words influence predictions\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use the fill-mask pipeline for quick masked language modeling tasks\n",
    "- Consider confidence scores when evaluating predictions\n",
    "- Test with various contexts to understand model behavior\n",
    "- Use device detection for optimal performance across different hardware\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Notebook 05**: Explore question answering with transformers\n",
    "- **Advanced Topics**: Custom model fine-tuning for specific domains\n",
    "- **Documentation**: [Hugging Face Fill-Mask Documentation](https://huggingface.co/docs/transformers/task_summary#masked-language-modeling)\n",
    "\n",
    "### üí° Key Takeaways\n",
    "> **Masked Language Modeling** is a powerful technique that helps models understand language context and semantics. The fill-mask pipeline provides an easy way to leverage pre-trained models for text completion and understanding tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "code_language": "python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}