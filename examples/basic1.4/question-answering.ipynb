{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/question-answering.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/question-answering.ipynb)\n",
        "\n",
        "# Simple Question Answering with BERT\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- How question answering works with BERT models\n",
        "- Using Hugging Face pipelines for question answering\n",
        "- How to extract answers from text contexts\n",
        "- Understanding confidence scores in QA tasks\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with Python and PyTorch\n",
        "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
        "\n",
        "## üìö What We'll Cover\n",
        "1. **Setup**: Import libraries and prepare environment\n",
        "2. **Basic QA Pipeline**: Using BERT for question answering\n",
        "3. **Simple Examples**: Testing with context and questions\n",
        "4. **Understanding Results**: Interpreting answers and confidence scores\n",
        "5. **Summary**: Key takeaways and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Question Answering\n",
        "\n",
        "**Question Answering (QA)** is a natural language processing task where we extract answers from a given text based on questions. BERT-based models excel at this task because they can:\n",
        "\n",
        "- **Understand Context**: Process the relationship between question and text\n",
        "- **Find Relevant Spans**: Identify the most relevant text segments\n",
        "- **Extract Answers**: Pull out specific answer spans from the context\n",
        "\n",
        "### How BERT Does Question Answering\n",
        "\n",
        "BERT for question answering adds a **span classification head** on top of the base BERT model. This linear layer:\n",
        "1. Accepts the final hidden states from BERT\n",
        "2. Performs linear transformation to compute **span start and end logits**\n",
        "3. Uses cross-entropy loss to find the most likely text span\n",
        "4. Returns the span as the answer with a confidence score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup and Environment\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if running for the first time)\n",
        "# !pip install transformers torch\n",
        "\n",
        "# Import essential libraries\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device detection for optimal performance\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Automatically detect and return the best available device.\n",
        "    \n",
        "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for current hardware\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Detect the best available device\n",
        "device = get_device()\n",
        "\n",
        "print(\"\\nüìö Setup completed successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic Question Answering with BERT\n",
        "\n",
        "The Hugging Face `pipeline` function makes question answering simple. By default, it uses a BERT-based model trained on the SQuAD dataset for extractive question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a question-answering pipeline\n",
        "# This will use a BERT-based model by default (e.g., distilbert-base-cased-distilled-squad)\n",
        "print(\"üîÑ Loading question answering model...\")\n",
        "qa_pipeline = pipeline(\"question-answering\", device=0 if device.type == \"cuda\" else -1)\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(\"üìä Ready for question answering with BERT!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Simple Question Answering Examples\n",
        "\n",
        "Let's test our BERT-based QA model with some simple examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a simple context about BERT and transformers\n",
        "context = \"\"\"\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based \n",
        "machine learning technique for natural language processing developed by Google. BERT was \n",
        "published in 2018 and stands for Bidirectional Encoder Representations from Transformers. \n",
        "Unlike previous models that read text sequentially, BERT reads the entire sequence of words \n",
        "at once, allowing it to learn the context of a word based on all of its surroundings. \n",
        "This makes BERT particularly effective for question answering tasks.\n",
        "\"\"\"\n",
        "\n",
        "# Define some questions about the context\n",
        "questions = [\n",
        "    \"What does BERT stand for?\",\n",
        "    \"Who developed BERT?\",\n",
        "    \"When was BERT published?\",\n",
        "    \"Why is BERT effective for question answering?\"\n",
        "]\n",
        "\n",
        "print(\"üîç QUESTION ANSWERING EXAMPLES\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Context: {context.strip()}\")\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "# Process each question\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n‚ùì Question {i}: {question}\")\n",
        "    \n",
        "    # Get answer using BERT\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    \n",
        "    print(f\"üí° Answer: {result['answer']}\")\n",
        "    print(f\"üìä Confidence: {result['score']:.4f}\")\n",
        "    print(f\"üìç Position: characters {result['start']}-{result['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding the Results\n",
        "\n",
        "Let's explore what the BERT model returns and how to interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine a single question in detail\n",
        "sample_question = \"What does BERT stand for?\"\n",
        "result = qa_pipeline(question=sample_question, context=context)\n",
        "\n",
        "print(\"üîç DETAILED RESULT ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Question: {sample_question}\")\n",
        "print(f\"\\nResult dictionary: {result}\")\n",
        "\n",
        "print(f\"\\nüìä EXPLANATION:\")\n",
        "print(f\"‚Ä¢ Answer: '{result['answer']}' - The extracted text span\")\n",
        "print(f\"‚Ä¢ Score: {result['score']:.4f} - Confidence level (0-1, higher is better)\")\n",
        "print(f\"‚Ä¢ Start: {result['start']} - Character position where answer begins\")\n",
        "print(f\"‚Ä¢ End: {result['end']} - Character position where answer ends\")\n",
        "\n",
        "# Show the context with the answer highlighted\n",
        "print(f\"\\nüéØ ANSWER IN CONTEXT:\")\n",
        "start_pos = result['start']\n",
        "end_pos = result['end']\n",
        "before = context[:start_pos]\n",
        "answer_span = context[start_pos:end_pos]\n",
        "after = context[end_pos:]\n",
        "\n",
        "print(f\"{before}[**{answer_span}**]{after}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing with Different Examples\n",
        "\n",
        "Let's try some additional examples to see how BERT handles different types of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different context about machine learning\n",
        "ml_context = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that enables computers to learn \n",
        "and make decisions without being explicitly programmed. It was first coined by Arthur Samuel \n",
        "in 1959. There are three main types of machine learning: supervised learning, unsupervised \n",
        "learning, and reinforcement learning. Deep learning is a specialized subset of machine learning \n",
        "that uses neural networks with multiple layers to analyze data.\n",
        "\"\"\"\n",
        "\n",
        "ml_questions = [\n",
        "    \"Who coined the term machine learning?\",\n",
        "    \"What are the three main types of machine learning?\",\n",
        "    \"What is deep learning?\"\n",
        "]\n",
        "\n",
        "print(\"üß† MACHINE LEARNING QA EXAMPLES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for question in ml_questions:\n",
        "    result = qa_pipeline(question=question, context=ml_context)\n",
        "    print(f\"\\n‚ùì Q: {question}\")\n",
        "    print(f\"üí° A: {result['answer']} (confidence: {result['score']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Information\n",
        "\n",
        "Let's understand what BERT model we're actually using for question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get information about the model being used\n",
        "print(\"üîç MODEL INFORMATION\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# The default QA pipeline typically uses DistilBERT (a compressed version of BERT)\n",
        "print(\"Default Model: distilbert-base-cased-distilled-squad\")\n",
        "print(\"Model Type: DistilBERT (distilled BERT)\")\n",
        "print(\"Training Dataset: SQuAD (Stanford Question Answering Dataset)\")\n",
        "print(\"Model Size: ~66M parameters (vs 110M for base BERT)\")\n",
        "print(\"Use Case: Extractive Question Answering\")\n",
        "\n",
        "print(\"\\nüìä MODEL CHARACTERISTICS:\")\n",
        "print(\"‚Ä¢ Bidirectional: Reads text in both directions\")\n",
        "print(\"‚Ä¢ Attention-based: Uses self-attention mechanisms\")\n",
        "print(\"‚Ä¢ Pre-trained: Trained on large text corpora\")\n",
        "print(\"‚Ä¢ Fine-tuned: Specialized for QA on SQuAD dataset\")\n",
        "print(\"‚Ä¢ Extractive: Finds answer spans within the given context\")\n",
        "\n",
        "print(\"\\nüí° HOW IT WORKS:\")\n",
        "print(\"1. Tokenizes question + context together\")\n",
        "print(\"2. BERT encoder processes the combined input\")\n",
        "print(\"3. Linear layers predict start/end positions\")\n",
        "print(\"4. Extracts text span with highest probability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we learned about question answering with BERT:\n",
        "\n",
        "### üîë Key Concepts Mastered\n",
        "- **Question Answering**: Extracting answers from text contexts using questions\n",
        "- **BERT for QA**: How BERT uses span classification for answer extraction\n",
        "- **Pipeline Usage**: Simple interface for question answering tasks\n",
        "- **Result Interpretation**: Understanding answers, confidence scores, and positions\n",
        "\n",
        "### üìà Best Practices Learned\n",
        "- Use clear, specific questions for better results\n",
        "- Provide sufficient context containing the answer information\n",
        "- Check confidence scores to assess answer reliability\n",
        "- Understand that extractive QA finds spans, not generated answers\n",
        "\n",
        "### üöÄ Next Steps\n",
        "- **Advanced QA**: Explore more sophisticated question-answering models\n",
        "- **Custom Fine-tuning**: Learn to fine-tune QA models on specific domains\n",
        "- **Documentation**: Check [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/task_summary#question-answering)\n",
        "- **Related Notebooks**: Explore other notebooks in this series for more NLP tasks\n",
        "\n",
        "üí° **Notice how easy it is to use BERT for different tasks once it's been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!**\n",
        "\n",
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}