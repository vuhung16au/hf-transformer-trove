{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/EmissionTracker.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/EmissionTracker.ipynb)\n",
        "\n",
        "# Carbon Emission Tracking for Model Inference with CodeCarbon\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- Why tracking carbon emissions in ML is important for sustainable AI\n",
        "- How to use CodeCarbon's EmissionTracker to monitor model inference\n",
        "- Different patterns for integrating emission tracking in ML workflows\n",
        "- How to analyze and compare carbon footprints of different models\n",
        "- Best practices for sustainable machine learning development\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with Hugging Face transformers\n",
        "- Knowledge of model inference processes\n",
        "- Understanding of environmental sustainability concepts\n",
        "\n",
        "## üìö What We'll Cover\n",
        "1. **Introduction**: Understanding AI's environmental impact\n",
        "2. **Setup**: Installing and configuring CodeCarbon\n",
        "3. **Basic Usage**: Simple emission tracking for model inference\n",
        "4. **Advanced Patterns**: Context managers and decorators\n",
        "5. **Model Comparison**: Comparing carbon footprints across models\n",
        "6. **Analysis**: Visualizing and interpreting emission data\n",
        "7. **Best Practices**: Guidelines for sustainable ML development\n",
        "8. **Summary**: Key takeaways and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "## 1. Introduction to Carbon Tracking in AI\n",
        "\n",
        "**Why Track Carbon Emissions in ML?**\n",
        "\n",
        "Machine learning models, especially large language models, consume significant computational resources, leading to substantial carbon emissions. Understanding and tracking these emissions is crucial for:\n",
        "\n",
        "- **Environmental Responsibility**: Reducing the carbon footprint of AI systems\n",
        "- **Cost Optimization**: Energy-efficient models often have lower operational costs\n",
        "- **Regulatory Compliance**: Meeting environmental reporting requirements\n",
        "- **Research Insights**: Understanding the relationship between model performance and environmental impact\n",
        "- **Sustainable Development**: Building AI systems that align with climate goals\n",
        "\n",
        "**About CodeCarbon**\n",
        "\n",
        "CodeCarbon is a Python library that tracks the carbon emissions produced by computing workloads. It provides:\n",
        "- Real-time emission tracking\n",
        "- Multiple integration patterns (decorators, context managers, explicit tracking)\n",
        "- Detailed reporting with hardware and location information\n",
        "- Integration with popular ML frameworks\n",
        "\n",
        "Let's learn how to use it effectively!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 2. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "# Install required packages if not already available\n",
        "# Uncomment the line below if running in a fresh environment\n",
        "# !pip install codecarbon>=2.3.0 transformers torch datasets matplotlib seaborn pandas\n",
        "\n",
        "# Import essential libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Hugging Face imports\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM, pipeline\n",
        ")\n",
        "\n",
        "# CodeCarbon imports\n",
        "try:\n",
        "    from codecarbon import EmissionsTracker, track_emissions\n",
        "    CODECARBON_AVAILABLE = True\n",
        "    print(\"‚úÖ CodeCarbon library loaded successfully\")\n",
        "except ImportError:\n",
        "    CODECARBON_AVAILABLE = False\n",
        "    print(\"‚ùå CodeCarbon not available. Please install with: pip install codecarbon>=2.3.0\")\n",
        "    print(\"üîÑ Continuing with mock implementation for demonstration...\")\n",
        "\n",
        "# Device detection function\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Get the best available device for PyTorch operations.\n",
        "    \n",
        "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for current hardware\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "print(f\"\\nüìö Libraries loaded successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CodeCarbon available: {'‚úÖ' if CODECARBON_AVAILABLE else '‚ùå'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mock_tracker_section"
      },
      "source": [
        "## 3. Mock Implementation and Basic Patterns\n",
        "\n",
        "When CodeCarbon is not available, we'll use a mock implementation to demonstrate the concepts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mock_implementation"
      },
      "outputs": [],
      "source": [
        "# Mock EmissionTracker for demonstration when CodeCarbon is not available\n",
        "class MockEmissionsTracker:\n",
        "    \"\"\"Mock implementation for demonstration purposes.\"\"\"\n",
        "    def __init__(self, project_name=None, experiment_name=None, output_dir=None, output_file=None):\n",
        "        self.project_name = project_name or \"demo_project\"\n",
        "        self.experiment_name = experiment_name or \"demo_experiment\"\n",
        "        self.output_dir = output_dir or \"./emissions/\"\n",
        "        self.output_file = output_file or \"emissions.csv\"\n",
        "        self.start_time = None\n",
        "        self.emissions = 0.0\n",
        "        print(f\"üé≠ Mock EmissionsTracker initialized: {self.project_name}/{self.experiment_name}\")\n",
        "    \n",
        "    def start(self):\n",
        "        self.start_time = time.time()\n",
        "        print(f\"üü¢ Mock emission tracking started for {self.experiment_name}\")\n",
        "    \n",
        "    def stop(self):\n",
        "        if self.start_time:\n",
        "            duration = time.time() - self.start_time\n",
        "            # Mock calculation: base emission + complexity factor\n",
        "            self.emissions = max(0.001, duration * 0.0001)  # Mock emission in kg CO2\n",
        "            print(f\"üõë Mock emission tracking stopped for {self.experiment_name}\")\n",
        "            print(f\"üìä Mock emissions: {self.emissions:.6f} kg CO2\")\n",
        "            return self.emissions\n",
        "        return 0.0\n",
        "    \n",
        "    def __enter__(self):\n",
        "        self.start()\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        return self.stop()\n",
        "\n",
        "# Mock decorator\n",
        "def mock_track_emissions(project_name=None, experiment_name=None, output_dir=None, output_file=None):\n",
        "    def decorator(func):\n",
        "        def wrapper(*func_args, **func_kwargs):\n",
        "            print(f\"üé≠ Mock emission tracking decorator for function: {func.__name__}\")\n",
        "            start_time = time.time()\n",
        "            result = func(*func_args, **func_kwargs)\n",
        "            duration = time.time() - start_time\n",
        "            mock_emissions = max(0.001, duration * 0.0001)\n",
        "            print(f\"üìä Mock emissions for {func.__name__}: {mock_emissions:.6f} kg CO2\")\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Use real or mock tracker\n",
        "TrackerClass = EmissionsTracker if CODECARBON_AVAILABLE else MockEmissionsTracker\n",
        "emissions_decorator = track_emissions if CODECARBON_AVAILABLE else mock_track_emissions\n",
        "\n",
        "print(\"üìã Emission Tracking Patterns Available:\")\n",
        "print(\"  1. Explicit start/stop tracking\")\n",
        "print(\"  2. Context manager (with statement)\")\n",
        "print(\"  3. Decorator pattern\")\n",
        "print(\"\\nüéØ Let's explore each pattern with model inference examples!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading_section"
      },
      "source": [
        "## 4. Load Model for Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_loading"
      },
      "outputs": [],
      "source": [
        "# Load a small model for demonstration\n",
        "print(\"üîÑ Loading sentiment analysis model...\")\n",
        "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(f\"‚úÖ Model loaded successfully on {device}\")\n",
        "    \n",
        "    # Count parameters\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"üìä Model parameters: {param_count:,}\")\n",
        "    \n",
        "    # Test data for inference\n",
        "    test_texts = [\n",
        "        \"I love this product! It's amazing and works perfectly.\",\n",
        "        \"This is terrible. Completely disappointed with the quality.\",\n",
        "        \"It's okay, nothing special but not bad either.\",\n",
        "        \"Absolutely fantastic! Exceeded all my expectations.\",\n",
        "        \"Waste of money. Would not recommend to anyone.\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"üìù Prepared {len(test_texts)} test texts for inference\")\n",
        "    \n",
        "    MODEL_LOADED = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"üîÑ This might be due to network issues or memory constraints\")\n",
        "    print(\"üìù Will continue with demonstration using mock data\")\n",
        "    MODEL_LOADED = False\n",
        "    test_texts = [\n",
        "        \"Sample positive text for demonstration\",\n",
        "        \"Sample negative text for demonstration\",\n",
        "        \"Sample neutral text for demonstration\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pattern1_section"
      },
      "source": [
        "## 5. Pattern 1: Explicit Start/Stop Tracking\n",
        "\n",
        "The most basic pattern where you manually start and stop emission tracking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pattern1_demo"
      },
      "outputs": [],
      "source": [
        "# Pattern 1: Explicit Start/Stop Tracking\n",
        "print(\"üéØ Pattern 1: Explicit Start/Stop Emission Tracking\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Ensure emissions directory exists\n",
        "os.makedirs('./emissions/', exist_ok=True)\n",
        "\n",
        "# Initialize the emission tracker\n",
        "tracker = TrackerClass(\n",
        "    project_name=\"hf_sentiment_inference\",\n",
        "    experiment_name=\"explicit_tracking\",\n",
        "    output_dir=\"./emissions/\",\n",
        "    output_file=\"explicit_tracking.csv\"\n",
        ")\n",
        "\n",
        "# Start tracking\n",
        "print(\"\\nüü¢ Starting emission tracking...\")\n",
        "tracker.start()\n",
        "\n",
        "# Perform model inference\n",
        "results = []\n",
        "inference_start = time.time()\n",
        "\n",
        "try:\n",
        "    for i, text in enumerate(test_texts):\n",
        "        print(f\"\\nüìù Processing text {i+1}/{len(test_texts)}: {text[:50]}...\")\n",
        "        \n",
        "        if MODEL_LOADED:\n",
        "            # Real model inference\n",
        "            inputs = tokenizer(\n",
        "                text, \n",
        "                return_tensors=\"pt\", \n",
        "                padding=True, \n",
        "                truncation=True, \n",
        "                max_length=512\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "                confidence = torch.max(predictions).item()\n",
        "            \n",
        "            label = \"POSITIVE\" if predicted_class == 1 else \"NEGATIVE\"\n",
        "        else:\n",
        "            # Mock inference for demonstration\n",
        "            label = \"POSITIVE\" if \"love\" in text.lower() or \"fantastic\" in text.lower() else \"NEGATIVE\"\n",
        "            confidence = 0.85 + np.random.random() * 0.14  # Mock confidence\n",
        "        \n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'prediction': label,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "        \n",
        "        print(f\"   üéØ Prediction: {label} (confidence: {confidence:.3f})\")\n",
        "        \n",
        "        # Small delay to simulate processing time\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    inference_time = time.time() - inference_start\n",
        "    print(f\"\\n‚è±Ô∏è  Total inference time: {inference_time:.2f} seconds\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during inference: {e}\")\n",
        "    inference_time = time.time() - inference_start\n",
        "\n",
        "# Stop tracking and get emissions\n",
        "print(\"\\nüõë Stopping emission tracking...\")\n",
        "emissions = tracker.stop()\n",
        "\n",
        "print(f\"\\nüìä Emission Tracking Results:\")\n",
        "print(f\"   Carbon emissions: {emissions:.6f} kg CO2\")\n",
        "print(f\"   Inference time: {inference_time:.2f} seconds\")\n",
        "print(f\"   Texts processed: {len(results)}\")\n",
        "if len(results) > 0 and emissions > 0:\n",
        "    print(f\"   Emissions per text: {emissions/len(results):.6f} kg CO2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pattern2_section"
      },
      "source": [
        "## 6. Pattern 2: Context Manager (Recommended)\n",
        "\n",
        "The context manager pattern is more pythonic and automatically handles start/stop operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pattern2_demo"
      },
      "outputs": [],
      "source": [
        "# Pattern 2: Context Manager Tracking\n",
        "print(\"üéØ Pattern 2: Context Manager Emission Tracking\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Test with a different batch of texts\n",
        "batch_texts = [\n",
        "    \"The weather is beautiful today!\",\n",
        "    \"I'm feeling quite frustrated with this situation.\",\n",
        "    \"This movie was absolutely incredible!\",\n",
        "    \"The service was disappointing and slow.\",\n",
        "    \"I have mixed feelings about this decision.\"\n",
        "]\n",
        "\n",
        "# Use context manager for automatic tracking\n",
        "with TrackerClass(\n",
        "    project_name=\"hf_sentiment_inference\",\n",
        "    experiment_name=\"context_manager\",\n",
        "    output_dir=\"./emissions/\",\n",
        "    output_file=\"context_manager.csv\"\n",
        ") as tracker:\n",
        "    \n",
        "    print(f\"\\nüîÑ Processing {len(batch_texts)} texts with context manager...\")\n",
        "    \n",
        "    batch_results = []\n",
        "    batch_start = time.time()\n",
        "    \n",
        "    try:\n",
        "        if MODEL_LOADED:\n",
        "            # Batch processing for efficiency\n",
        "            print(\"\\nüì¶ Using batch processing for better efficiency...\")\n",
        "            \n",
        "            # Tokenize all texts together\n",
        "            batch_inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "            batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
        "            \n",
        "            # Perform batch inference\n",
        "            with torch.no_grad():\n",
        "                batch_outputs = model(**batch_inputs)\n",
        "                batch_predictions = torch.nn.functional.softmax(batch_outputs.logits, dim=-1)\n",
        "                predicted_classes = torch.argmax(batch_predictions, dim=-1)\n",
        "                confidences = torch.max(batch_predictions, dim=-1).values\n",
        "            \n",
        "            # Process results\n",
        "            for i, (text, pred_class, confidence) in enumerate(zip(\n",
        "                batch_texts, predicted_classes.cpu().numpy(), confidences.cpu().numpy()\n",
        "            )):\n",
        "                label = \"POSITIVE\" if pred_class == 1 else \"NEGATIVE\"\n",
        "                batch_results.append({\n",
        "                    'text': text,\n",
        "                    'prediction': label,\n",
        "                    'confidence': confidence\n",
        "                })\n",
        "                print(f\"   üìù Text {i+1}: {label} ({confidence:.3f}) - {text[:40]}...\")\n",
        "        \n",
        "        else:\n",
        "            # Mock processing for demonstration\n",
        "            print(\"\\nüé≠ Using mock processing for demonstration...\")\n",
        "            for i, text in enumerate(batch_texts):\n",
        "                # Simple sentiment analysis based on keywords\n",
        "                positive_words = ['beautiful', 'incredible', 'amazing', 'good', 'great']\n",
        "                negative_words = ['frustrated', 'disappointing', 'slow', 'bad', 'terrible']\n",
        "                \n",
        "                text_lower = text.lower()\n",
        "                if any(word in text_lower for word in positive_words):\n",
        "                    label = \"POSITIVE\"\n",
        "                    confidence = 0.8 + np.random.random() * 0.2\n",
        "                elif any(word in text_lower for word in negative_words):\n",
        "                    label = \"NEGATIVE\"\n",
        "                    confidence = 0.8 + np.random.random() * 0.2\n",
        "                else:\n",
        "                    label = \"NEUTRAL\"\n",
        "                    confidence = 0.6 + np.random.random() * 0.3\n",
        "                \n",
        "                batch_results.append({\n",
        "                    'text': text,\n",
        "                    'prediction': label,\n",
        "                    'confidence': confidence\n",
        "                })\n",
        "                print(f\"   üìù Text {i+1}: {label} ({confidence:.3f}) - {text[:40]}...\")\n",
        "                \n",
        "                # Simulate processing time\n",
        "                time.sleep(0.05)\n",
        "        \n",
        "        batch_time = time.time() - batch_start\n",
        "        print(f\"\\n‚è±Ô∏è  Batch processing time: {batch_time:.2f} seconds\")\n",
        "        print(f\"‚ö° Average time per text: {batch_time/len(batch_texts):.3f} seconds\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during batch processing: {e}\")\n",
        "        batch_results = []\n",
        "\n",
        "print(f\"\\n‚úÖ Context manager automatically handled emission tracking!\")\n",
        "print(f\"üìä Processed {len(batch_results)} texts successfully\")\n",
        "\n",
        "# Display results summary\n",
        "if batch_results:\n",
        "    positive_count = sum(1 for r in batch_results if r['prediction'] == 'POSITIVE')\n",
        "    negative_count = sum(1 for r in batch_results if r['prediction'] == 'NEGATIVE')\n",
        "    neutral_count = len(batch_results) - positive_count - negative_count\n",
        "    avg_confidence = np.mean([r['confidence'] for r in batch_results])\n",
        "    \n",
        "    print(f\"\\nüìà Results Summary:\")\n",
        "    print(f\"   Positive: {positive_count}, Negative: {negative_count}, Neutral: {neutral_count}\")\n",
        "    print(f\"   Average confidence: {avg_confidence:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pattern3_section"
      },
      "source": [
        "## 7. Pattern 3: Decorator Pattern\n",
        "\n",
        "For functions that you want to consistently track, decorators provide a clean solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pattern3_demo"
      },
      "outputs": [],
      "source": [
        "# Pattern 3: Decorator Pattern\n",
        "print(\"üéØ Pattern 3: Decorator Pattern Emission Tracking\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Define a function with emission tracking\n",
        "@emissions_decorator(\n",
        "    project_name=\"hf_sentiment_inference\",\n",
        "    experiment_name=\"decorator_pattern\",\n",
        "    output_dir=\"./emissions/\",\n",
        "    output_file=\"decorator_pattern.csv\"\n",
        ")\n",
        "def analyze_sentiment_with_emissions(texts: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Analyze sentiment for a list of texts with automatic emission tracking.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings to analyze\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing analysis results\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Starting sentiment analysis for {len(texts)} texts...\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    try:\n",
        "        for i, text in enumerate(texts):\n",
        "            if MODEL_LOADED:\n",
        "                # Real model inference\n",
        "                inputs = tokenizer(\n",
        "                    text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=512\n",
        "                )\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "                    confidence = torch.max(predictions).item()\n",
        "                \n",
        "                label = \"POSITIVE\" if predicted_class == 1 else \"NEGATIVE\"\n",
        "            else:\n",
        "                # Mock inference\n",
        "                positive_indicators = ['love', 'great', 'excellent', 'brilliant', 'fantastic']\n",
        "                negative_indicators = ['hate', 'terrible', 'awful', 'disappointing', 'worst']\n",
        "                \n",
        "                text_lower = text.lower()\n",
        "                if any(word in text_lower for word in positive_indicators):\n",
        "                    label = \"POSITIVE\"\n",
        "                    confidence = 0.85 + np.random.random() * 0.14\n",
        "                elif any(word in text_lower for word in negative_indicators):\n",
        "                    label = \"NEGATIVE\"\n",
        "                    confidence = 0.85 + np.random.random() * 0.14\n",
        "                else:\n",
        "                    # Random prediction for neutral texts\n",
        "                    label = np.random.choice([\"POSITIVE\", \"NEGATIVE\"])\n",
        "                    confidence = 0.6 + np.random.random() * 0.3\n",
        "            \n",
        "            result = {\n",
        "                'text': text,\n",
        "                'prediction': label,\n",
        "                'confidence': confidence,\n",
        "                'text_length': len(text)\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"   {i+1:2d}. {label:8s} ({confidence:.3f}) - {text[:50]}...\")\n",
        "            \n",
        "            # Simulate processing delay\n",
        "            time.sleep(0.05)\n",
        "        \n",
        "        print(f\"\\n‚úÖ Analysis completed for {len(results)} texts\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during analysis: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test the decorated function\n",
        "decorator_texts = [\n",
        "    \"I absolutely love using transformers for NLP tasks!\",\n",
        "    \"This library is confusing and poorly documented.\",\n",
        "    \"CodeCarbon helps make AI development more sustainable.\",\n",
        "    \"I'm not sure if this approach will work effectively.\",\n",
        "    \"The combination of HuggingFace and emission tracking is brilliant!\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüöÄ Testing decorated function with {len(decorator_texts)} texts...\")\n",
        "decorator_results = analyze_sentiment_with_emissions(decorator_texts)\n",
        "\n",
        "print(f\"\\nüìà Decorator Pattern Results Summary:\")\n",
        "if decorator_results:\n",
        "    positive_count = sum(1 for r in decorator_results if r['prediction'] == 'POSITIVE')\n",
        "    negative_count = len(decorator_results) - positive_count\n",
        "    avg_confidence = np.mean([r['confidence'] for r in decorator_results])\n",
        "    avg_text_length = np.mean([r['text_length'] for r in decorator_results])\n",
        "    \n",
        "    print(f\"   Positive predictions: {positive_count}\")\n",
        "    print(f\"   Negative predictions: {negative_count}\")\n",
        "    print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
        "    print(f\"   Average text length: {avg_text_length:.1f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis_section"
      },
      "source": [
        "## 8. Analysis and Best Practices\n",
        "\n",
        "Let's analyze the emission tracking results and discuss best practices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analysis_demo"
      },
      "outputs": [],
      "source": [
        "# Analysis and Visualization of Emission Data\n",
        "print(\"üìä Carbon Emission Analysis and Best Practices\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Create mock comparison data for demonstration\n",
        "experiment_data = [\n",
        "    {'pattern': 'Explicit Tracking', 'texts': 5, 'time': 2.1, 'emissions': 0.001234},\n",
        "    {'pattern': 'Context Manager', 'texts': 5, 'time': 1.8, 'emissions': 0.000987},\n",
        "    {'pattern': 'Decorator', 'texts': 5, 'time': 2.3, 'emissions': 0.001156}\n",
        "]\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(experiment_data)\n",
        "df['Efficiency (texts/kg CO2)'] = df['texts'] / df['emissions']\n",
        "df['Time per Text (s)'] = df['time'] / df['texts']\n",
        "df['Emissions per Text (kg CO2)'] = df['emissions'] / df['texts']\n",
        "\n",
        "print(\"\\nüìã Experiment Comparison Summary:\")\n",
        "print(df.round(6))\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Carbon Emission Tracking Pattern Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Emissions comparison\n",
        "bars1 = axes[0, 0].bar(df['pattern'], df['emissions'], color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "axes[0, 0].set_ylabel('Total Emissions (kg CO2)')\n",
        "axes[0, 0].set_title('Total Emissions by Pattern')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.6f}',\n",
        "                   ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 2. Efficiency comparison\n",
        "bars2 = axes[0, 1].bar(df['pattern'], df['Efficiency (texts/kg CO2)'], \n",
        "                      color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "axes[0, 1].set_ylabel('Texts per kg CO2')\n",
        "axes[0, 1].set_title('Carbon Efficiency by Pattern')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.0f}',\n",
        "                   ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 3. Processing time comparison\n",
        "bars3 = axes[1, 0].bar(df['pattern'], df['Time per Text (s)'], \n",
        "                      color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "axes[1, 0].set_ylabel('Time per Text (seconds)')\n",
        "axes[1, 0].set_title('Processing Efficiency by Pattern')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}',\n",
        "                   ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 4. Emissions vs Time scatter plot\n",
        "scatter = axes[1, 1].scatter(df['Time per Text (s)'], df['Emissions per Text (kg CO2)'], \n",
        "                           s=100, c=['red', 'blue', 'green'], alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Time per Text (seconds)')\n",
        "axes[1, 1].set_ylabel('Emissions per Text (kg CO2)')\n",
        "axes[1, 1].set_title('Time vs Emissions Efficiency')\n",
        "\n",
        "# Add pattern labels\n",
        "for i, row in df.iterrows():\n",
        "    axes[1, 1].annotate(row['pattern'], \n",
        "                       (row['Time per Text (s)'], row['Emissions per Text (kg CO2)']),\n",
        "                       xytext=(5, 5), textcoords='offset points',\n",
        "                       fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analysis insights\n",
        "print(\"\\nüîç Key Insights:\")\n",
        "most_efficient = df.loc[df['Efficiency (texts/kg CO2)'].idxmax()]\n",
        "fastest = df.loc[df['Time per Text (s)'].idxmin()]\n",
        "\n",
        "print(f\"   üèÜ Most carbon-efficient: {most_efficient['pattern']}\")\n",
        "print(f\"      {most_efficient['Efficiency (texts/kg CO2)']:.0f} texts per kg CO2\")\n",
        "print(f\"   ‚ö° Fastest processing: {fastest['pattern']}\")\n",
        "print(f\"      {fastest['Time per Text (s)']:.3f} seconds per text\")\n",
        "\n",
        "efficiency_diff = df['Efficiency (texts/kg CO2)'].max() / df['Efficiency (texts/kg CO2)'].min()\n",
        "print(f\"   üìà Efficiency range: {efficiency_diff:.1f}x difference between best and worst\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "best_practices_section"
      },
      "source": [
        "## 9. Best Practices for Sustainable ML\n",
        "\n",
        "Based on our emission tracking experiments, here are key recommendations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_practices_demo"
      },
      "outputs": [],
      "source": [
        "# Best Practices for Sustainable ML\n",
        "print(\"üå± Best Practices for Sustainable Machine Learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_practices = {\n",
        "    \"üéØ Model Selection\": [\n",
        "        \"Choose the smallest model that meets your accuracy requirements\",\n",
        "        \"Consider distilled models (like DistilBERT) for production deployment\",\n",
        "        \"Evaluate trade-offs between model performance and carbon footprint\",\n",
        "        \"Use model cards to understand environmental impact information\"\n",
        "    ],\n",
        "    \n",
        "    \"‚ö° Optimization Strategies\": [\n",
        "        \"Use batch processing instead of individual predictions\",\n",
        "        \"Implement model quantization and pruning techniques\",\n",
        "        \"Leverage mixed precision training (FP16) when possible\",\n",
        "        \"Cache model outputs for repeated inputs\",\n",
        "        \"Use ONNX or TensorRT for optimized inference\"\n",
        "    ],\n",
        "    \n",
        "    \"üèóÔ∏è Infrastructure Choices\": [\n",
        "        \"Choose cloud regions with renewable energy sources\",\n",
        "        \"Prefer CPU inference for small models and low latency requirements\",\n",
        "        \"Use auto-scaling to minimize idle compute resources\",\n",
        "        \"Consider edge deployment to reduce data transfer\"\n",
        "    ],\n",
        "    \n",
        "    \"üìä Monitoring and Reporting\": [\n",
        "        \"Track carbon emissions throughout the ML lifecycle\",\n",
        "        \"Set carbon budgets for ML projects\",\n",
        "        \"Include environmental impact in model evaluation metrics\",\n",
        "        \"Report emissions alongside traditional performance metrics\"\n",
        "    ],\n",
        "    \n",
        "    \"üîÑ Development Workflow\": [\n",
        "        \"Use smaller models or datasets during development and debugging\",\n",
        "        \"Implement early stopping to avoid unnecessary training\",\n",
        "        \"Share pre-trained models to avoid redundant training\",\n",
        "        \"Consider federated learning to reduce centralized computation\"\n",
        "    ],\n",
        "    \n",
        "    \"üìã CodeCarbon Integration\": [\n",
        "        \"Use context managers for clean, automatic tracking\",\n",
        "        \"Apply decorators to functions that need consistent monitoring\",\n",
        "        \"Configure appropriate project and experiment names\",\n",
        "        \"Store emission logs for historical analysis\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, practices in best_practices.items():\n",
        "    print(f\"\\n{category}\")\n",
        "    for i, practice in enumerate(practices, 1):\n",
        "        print(f\"   {i}. {practice}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üí° Key Takeaway: Sustainable AI is about making informed trade-offs\")\n",
        "print(\"   between model performance, computational efficiency, and environmental impact.\")\n",
        "print(\"   Always measure and optimize for the right balance for your use case.\")\n",
        "\n",
        "# Environmental context\n",
        "print(f\"\\nüåç Environmental Impact Context:\")\n",
        "mock_total_emissions = sum(exp['emissions'] for exp in experiment_data)\n",
        "print(f\"   Total emissions from our experiments: {mock_total_emissions:.6f} kg CO2\")\n",
        "print(f\"   Equivalent to approximately {mock_total_emissions * 1000:.2f} grams of CO2\")\n",
        "\n",
        "# Rough equivalencies for context\n",
        "smartphone_charges = (mock_total_emissions * 1000) / 8  # ~8g CO2 per smartphone charge\n",
        "driving_meters = (mock_total_emissions * 1000) / 404 * 1000  # ~404g CO2 per km\n",
        "\n",
        "print(f\"   Equivalent to ~{smartphone_charges:.2f} smartphone charges\")\n",
        "print(f\"   Equivalent to ~{driving_meters:.1f} meters of car driving\")\n",
        "\n",
        "print(f\"\\nüéØ Action Items:\")\n",
        "print(f\"   1. Integrate CodeCarbon into your ML workflows\")\n",
        "print(f\"   2. Set carbon budgets for your projects\")\n",
        "print(f\"   3. Compare models based on efficiency, not just accuracy\")\n",
        "print(f\"   4. Share emission data with your team and community\")\n",
        "print(f\"   5. Choose sustainable infrastructure and deployment options\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_section"
      },
      "source": [
        "## 10. Summary\n",
        "\n",
        "In this comprehensive notebook, we explored carbon emission tracking for ML model inference:\n",
        "\n",
        "### üîë **Key Concepts Mastered**\n",
        "- **CodeCarbon Integration**: Multiple patterns for tracking emissions (explicit, context manager, decorator)\n",
        "- **Model Inference Monitoring**: Real-time tracking of carbon footprint during model inference\n",
        "- **Pattern Comparison**: Understanding trade-offs between different tracking approaches\n",
        "- **Emission Analysis**: Visualizing and interpreting carbon emission data\n",
        "- **Sustainable ML**: Best practices for environmentally responsible AI development\n",
        "- **Mock Implementation**: Demonstration techniques when libraries aren't available\n",
        "\n",
        "### üìà **Best Practices Learned**\n",
        "- Use context managers for clean emission tracking code\n",
        "- Apply decorators for functions requiring consistent monitoring\n",
        "- Implement batch processing to reduce emissions per prediction\n",
        "- Choose the smallest model that meets accuracy requirements\n",
        "- Monitor and report emissions alongside traditional ML metrics\n",
        "- Consider environmental impact in model selection and deployment decisions\n",
        "- Set carbon budgets for ML projects\n",
        "\n",
        "### üöÄ **Next Steps**\n",
        "- **Production Integration**: Implement emission tracking in production ML pipelines\n",
        "- **Advanced Optimization**: Explore model quantization and pruning for efficiency\n",
        "- **Green Computing**: Learn about renewable energy sources for ML workloads\n",
        "- **Community Sharing**: Contribute emission data to the ML community\n",
        "- **Policy Development**: Create organizational guidelines for sustainable AI\n",
        "\n",
        "### üí° **Key Takeaway**\n",
        "Sustainable AI development requires balancing model performance with environmental impact. By tracking carbon emissions during model inference, we can make informed decisions that contribute to more responsible and environmentally conscious machine learning practices. CodeCarbon provides the tools to measure our impact, and it's our responsibility to act on that information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "author_section"
      },
      "source": [
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}