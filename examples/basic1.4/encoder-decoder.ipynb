{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/encoder-decoder.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/encoder-decoder.ipynb)\n",
    "\n",
    "# Encoder-Decoder Architecture: Understanding Sequence-to-Sequence Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- The fundamental architecture of encoder-decoder models\n",
    "- How attention mechanisms work in sequence-to-sequence tasks\n",
    "- Practical implementation of T5 and BART encoder-decoder models\n",
    "- Auto-regressive generation and cross-attention in practice\n",
    "- Mathematical concepts behind encoder-decoder transformers\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Setup**: Environment and device detection\n",
    "2. **Architecture Overview**: Understanding the encoder-decoder mechanism\n",
    "3. **Mathematical Foundations**: Key equations and concepts\n",
    "4. **T5 Implementation**: Text-to-text transfer transformer\n",
    "5. **BART Implementation**: Bidirectional and auto-regressive transformers\n",
    "6. **Cross-Attention Visualization**: Understanding model attention patterns\n",
    "7. **Auto-regressive Generation**: Step-by-step token generation\n",
    "8. **Comparison**: Encoder vs Decoder vs Encoder-Decoder\n",
    "9. **Summary**: Key takeaways and next steps\n",
    "\n",
    "## What are Encoder-Decoder Models?\n",
    "\n",
    "**Encoder-Decoder** architecture, also known as **Sequence-to-Sequence (Seq2Seq)** models, is designed for tasks where the input and output sequences are different, such as translation, summarization, and question answering.\n",
    "\n",
    "### The Two-Phase Mechanism\n",
    "\n",
    "1. **Encoder Phase**: Processes the input sequence and creates a numerical representation\n",
    "2. **Decoder Phase**: Uses the encoder's representation to generate the output sequence\n",
    "\n",
    "### Why Encoder-Decoders Excel\n",
    "\n",
    "- **Separate Specialization**: Encoder understands input, decoder generates output\n",
    "- **Different Modalities**: Can handle different input/output types and lengths\n",
    "- **Cross-Attention**: Decoder can selectively attend to relevant parts of input\n",
    "- **Auto-regressive**: Generates output token by token, using previous outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "        # Print GPU memory info\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB total\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the optimal device\n",
    "device = get_device()\n",
    "print(f\"\\nüì± Active device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations\n",
    "\n",
    "Let's understand the key mathematical concepts behind encoder-decoder models.\n",
    "\n",
    "### Encoder Architecture\n",
    "The encoder processes input sequence $X = (x_1, x_2, ..., x_n)$ to produce representations $H = (h_1, h_2, ..., h_n)$:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Decoder with Cross-Attention\n",
    "The decoder uses three types of attention:\n",
    "\n",
    "1. **Masked Self-Attention**: $\\text{MaskedAttn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$\n",
    "\n",
    "2. **Cross-Attention**: $\\text{CrossAttn}(Q_{dec}, K_{enc}, V_{enc}) = \\text{softmax}\\left(\\frac{Q_{dec}K_{enc}^T}{\\sqrt{d_k}}\\right)V_{enc}$\n",
    "\n",
    "3. **Auto-regressive Generation**: $P(y_t | y_{<t}, X) = \\text{softmax}(\\text{Decoder}(y_{<t}, X)W_{vocab})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_attention_mechanism():\n",
    "    \"\"\"\n",
    "    Demonstrate the mathematical concepts behind attention mechanisms.\n",
    "    \"\"\"\n",
    "    print(\"üßÆ MATHEMATICAL DEMONSTRATION: Attention Mechanism\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Create sample sequences\n",
    "    seq_len = 4\n",
    "    d_model = 6  # Small dimension for demonstration\n",
    "    \n",
    "    # Generate random query, key, value matrices\n",
    "    torch.manual_seed(42)  # For reproducible results\n",
    "    Q = torch.randn(1, seq_len, d_model)\n",
    "    K = torch.randn(1, seq_len, d_model)\n",
    "    V = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    print(f\"Input dimensions:\")\n",
    "    print(f\"  Sequence length: {seq_len}\")\n",
    "    print(f\"  Model dimension: {d_model}\")\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_model)\n",
    "    print(f\"\\nStep 1: Attention scores (Q @ K^T / ‚àöd_k)\")\n",
    "    print(f\"  Score matrix shape: {scores.shape}\")\n",
    "    print(f\"  Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "    \n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    print(f\"\\nStep 2: Attention weights (softmax of scores)\")\n",
    "    print(f\"  Each row sums to 1.0: {attention_weights.sum(dim=-1)}\")\n",
    "    \n",
    "    # Step 3: Apply attention to values\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    print(f\"\\nStep 3: Attention output (weights @ V)\")\n",
    "    print(f\"  Output shape: {attention_output.shape}\")\n",
    "    \n",
    "    # Visualize attention weights\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(scores[0].detach().numpy(), annot=True, fmt='.2f', cmap='RdBu_r')\n",
    "    plt.title('Attention Scores\\n(Q @ K^T / ‚àöd_k)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(attention_weights[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues')\n",
    "    plt.title('Attention Weights\\n(Softmax of Scores)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_weights, attention_output\n",
    "\n",
    "# Run the demonstration\n",
    "weights, output = demonstrate_attention_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T5: Text-to-Text Transfer Transformer\n",
    "\n",
    "T5 treats every NLP task as a text-to-text problem. It uses explicit task prefixes to indicate the desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 model and tokenizer\n",
    "print(\"üîÑ Loading T5 model for text-to-text generation...\")\n",
    "\n",
    "try:\n",
    "    # Using T5-small for demonstration (faster and less memory)\n",
    "    t5_model_name = \"t5-small\"\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "    \n",
    "    # Move model to device\n",
    "    t5_model = t5_model.to(device)\n",
    "    t5_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"‚úÖ T5 model loaded successfully\")\n",
    "    print(f\"   Model parameters: {t5_model.num_parameters():,}\")\n",
    "    print(f\"   Encoder layers: {t5_model.config.num_layers}\")\n",
    "    print(f\"   Decoder layers: {t5_model.config.num_decoder_layers}\")\n",
    "    print(f\"   Attention heads: {t5_model.config.num_heads}\")\n",
    "    print(f\"   Model dimension: {t5_model.config.d_model}\")\n",
    "    \n",
    "    t5_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading T5 model: {e}\")\n",
    "    t5_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_t5_tasks(model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Demonstrate various T5 tasks showing encoder-decoder versatility.\n",
    "    \"\"\"\n",
    "    print(\"üìù T5 ENCODER-DECODER DEMONSTRATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define various tasks with their prefixes\n",
    "    tasks = [\n",
    "        {\n",
    "            \"name\": \"Translation\",\n",
    "            \"input\": \"translate English to German: The weather is beautiful today.\",\n",
    "            \"description\": \"Cross-lingual sequence transformation\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Summarization\", \n",
    "            \"input\": \"summarize: Machine learning is a subset of artificial intelligence that involves training algorithms to make predictions or decisions based on data. It has applications in many fields including healthcare, finance, and technology.\",\n",
    "            \"description\": \"Text compression while preserving key information\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Question Answering\",\n",
    "            \"input\": \"question: What is machine learning? context: Machine learning is a method of data analysis that automates analytical model building using algorithms that iteratively learn from data.\",\n",
    "            \"description\": \"Information extraction from context\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Sentiment Classification\",\n",
    "            \"input\": \"sentiment: I absolutely love this new smartphone! The camera quality is amazing.\",\n",
    "            \"description\": \"Sequence-to-label classification\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for task in tasks:\n",
    "        print(f\"\\nüéØ Task: {task['name']}\")\n",
    "        print(f\"   Purpose: {task['description']}\")\n",
    "        print(f\"   Input: {task['input'][:80]}...\" if len(task['input']) > 80 else f\"   Input: {task['input']}\")\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                task['input'], \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=512, \n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate output\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=50,\n",
    "                    num_beams=3,\n",
    "                    early_stopping=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"   Output: {generated_text}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "if t5_available:\n",
    "    demonstrate_t5_tasks(t5_model, t5_tokenizer, device)\n",
    "else:\n",
    "    print(\"‚ùå T5 model not available for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto-regressive Generation Explained\n",
    "\n",
    "Let's understand how encoder-decoder models generate text step by step in an auto-regressive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_autoregressive_generation(model, tokenizer, input_text, device, max_steps=8):\n",
    "    \"\"\"\n",
    "    Demonstrate step-by-step auto-regressive generation.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ AUTO-REGRESSIVE GENERATION STEP-BY-STEP\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Input: {input_text}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Encode input\n",
    "        encoder_inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get encoder outputs (these remain constant during generation)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.get_encoder()(**encoder_inputs)\n",
    "        \n",
    "        print(f\"üì• Encoder Phase Complete:\")\n",
    "        print(f\"   Input tokens: {len(encoder_inputs.input_ids[0])}\")\n",
    "        print(f\"   Encoder output shape: {encoder_outputs.last_hidden_state.shape}\")\n",
    "        print(f\"   Each position now has a {encoder_outputs.last_hidden_state.shape[-1]}-dim representation\\n\")\n",
    "        \n",
    "        # Initialize decoder with start token\n",
    "        decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        \n",
    "        print(\"üéØ Decoder Phase (Auto-regressive Generation):\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                # Run decoder with current sequence and encoder outputs\n",
    "                decoder_outputs = model.get_decoder()(\n",
    "                    input_ids=decoder_input_ids,\n",
    "                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "                    encoder_attention_mask=encoder_inputs.attention_mask\n",
    "                )\n",
    "                \n",
    "                # Get logits for next token prediction\n",
    "                logits = model.lm_head(decoder_outputs.last_hidden_state)\n",
    "                next_token_logits = logits[0, -1, :]  # Last position logits\n",
    "                \n",
    "                # Get top-k predictions\n",
    "                top_k = 5\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "                top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "                \n",
    "                # Choose next token (greedy for demonstration)\n",
    "                next_token_id = top_k_indices[0]\n",
    "                next_token = tokenizer.decode(next_token_id, skip_special_tokens=True)\n",
    "                \n",
    "                print(f\"   Step {step + 1}:\")\n",
    "                print(f\"     Current sequence: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
    "                print(f\"     Next token: '{next_token}' (prob: {top_k_probs[0]:.3f})\")\n",
    "                print(f\"     Top alternatives:\")\n",
    "                for i in range(1, min(3, top_k)):\n",
    "                    alt_token = tokenizer.decode(top_k_indices[i], skip_special_tokens=True)\n",
    "                    print(f\"       '{alt_token}' (prob: {top_k_probs[i]:.3f})\")\n",
    "                \n",
    "                # Stop if we hit end token or pad token\n",
    "                if next_token_id == tokenizer.eos_token_id or next_token_id == tokenizer.pad_token_id:\n",
    "                    print(f\"     üõë Generation stopped (end token reached)\")\n",
    "                    break\n",
    "                \n",
    "                # Add token to sequence\n",
    "                decoder_input_ids = torch.cat(\n",
    "                    [decoder_input_ids, next_token_id.unsqueeze(0).unsqueeze(0)], \n",
    "                    dim=-1\n",
    "                )\n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                print()  # Empty line for readability\n",
    "        \n",
    "        # Final generated text\n",
    "        final_text = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
    "        print(f\"\\n‚úÖ Final Generated Text: {final_text}\")\n",
    "        print(f\"üìä Generation Statistics:\")\n",
    "        print(f\"   Total steps: {len(generated_tokens)}\")\n",
    "        print(f\"   Generated tokens: {generated_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in auto-regressive demonstration: {e}\")\n",
    "\n",
    "# Demonstrate auto-regressive generation with T5\n",
    "if t5_available:\n",
    "    demo_text = \"summarize: Machine learning is a powerful technology.\"\n",
    "    demonstrate_autoregressive_generation(t5_model, t5_tokenizer, demo_text, device, max_steps=6)\n",
    "else:\n",
    "    print(\"‚ùå T5 model not available for auto-regressive demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Architecture Comparison: Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "Let's understand the differences between the three main transformer architectures and when to use each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transformer_architectures():\n",
    "    \"\"\"\n",
    "    Compare different transformer architectures with practical examples.\n",
    "    \"\"\"\n",
    "    print(\"üèóÔ∏è TRANSFORMER ARCHITECTURE COMPARISON\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    architectures = {\n",
    "        \"Encoder-only (e.g., BERT)\": {\n",
    "            \"attention\": \"Bi-directional self-attention\",\n",
    "            \"training\": \"Masked Language Modeling (MLM)\",\n",
    "            \"strengths\": [\n",
    "                \"Excellent context understanding\",\n",
    "                \"Great for classification tasks\", \n",
    "                \"Efficient inference\",\n",
    "                \"Perfect for hate speech detection\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Sentiment analysis\",\n",
    "                \"Hate speech detection\", \n",
    "                \"Named entity recognition\",\n",
    "                \"Text classification\"\n",
    "            ],\n",
    "            \"examples\": \"BERT, RoBERTa, DeBERTa, DistilBERT\"\n",
    "        },\n",
    "        \"Decoder-only (e.g., GPT)\": {\n",
    "            \"attention\": \"Uni-directional (causal) self-attention\",\n",
    "            \"training\": \"Causal Language Modeling (CLM)\",\n",
    "            \"strengths\": [\n",
    "                \"Excellent for text generation\",\n",
    "                \"Auto-regressive by design\",\n",
    "                \"Good few-shot learning\",\n",
    "                \"Scales well with data/parameters\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Text generation\",\n",
    "                \"Creative writing\",\n",
    "                \"Code generation\", \n",
    "                \"Conversational AI\"\n",
    "            ],\n",
    "            \"examples\": \"GPT-2, GPT-3/4, LLaMA, Mistral\"\n",
    "        },\n",
    "        \"Encoder-Decoder (e.g., T5)\": {\n",
    "            \"attention\": \"Bi-directional + Cross-attention + Causal\",\n",
    "            \"training\": \"Denoising/Sequence-to-sequence\",\n",
    "            \"strengths\": [\n",
    "                \"Best for sequence transformation\",\n",
    "                \"Flexible input/output lengths\",\n",
    "                \"Excellent for conditional generation\",\n",
    "                \"Cross-attention provides interpretability\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Machine translation\",\n",
    "                \"Text summarization\",\n",
    "                \"Question answering\",\n",
    "                \"Paraphrasing\"\n",
    "            ],\n",
    "            \"examples\": \"T5, BART, mT5, PEGASUS\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for arch_name, details in architectures.items():\n",
    "        print(f\"\\nüîß {arch_name}\")\n",
    "        print(f\"   Attention Pattern: {details['attention']}\")\n",
    "        print(f\"   Training Objective: {details['training']}\")\n",
    "        print(f\"   Key Strengths:\")\n",
    "        for strength in details['strengths']:\n",
    "            print(f\"     ‚Ä¢ {strength}\")\n",
    "        print(f\"   Best Use Cases:\")\n",
    "        for use_case in details['use_cases']:\n",
    "            print(f\"     ‚Ä¢ {use_case}\")\n",
    "        print(f\"   Popular Models: {details['examples']}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\nüìä PERFORMANCE CHARACTERISTICS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    metrics = [\n",
    "        [\"Architecture\", \"Memory Usage\", \"Inference Speed\", \"Training Complexity\"],\n",
    "        [\"Encoder-only\", \"Moderate ‚ö°\", \"Fast ‚ö°‚ö°\", \"Simple ‚úÖ\"],\n",
    "        [\"Decoder-only\", \"Efficient ‚ö°‚ö°\", \"Fast ‚ö°‚ö°\", \"Simple ‚úÖ\"],\n",
    "        [\"Encoder-Decoder\", \"High ‚ö†Ô∏è\", \"Slower ‚ö†Ô∏è\", \"Complex ‚ö†Ô∏è\"]\n",
    "    ]\n",
    "    \n",
    "    for row in metrics:\n",
    "        print(f\"   {row[0]:15} | {row[1]:15} | {row[2]:15} | {row[3]}\")\n",
    "        if row[0] == \"Architecture\":\n",
    "            print(\"   \" + \"-\" * 70)\n",
    "\n",
    "# Run the comparison\n",
    "compare_transformer_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Model Selection Guide\n",
    "\n",
    "Let's create a practical guide for choosing the right model architecture for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_guide():\n",
    "    \"\"\"\n",
    "    Provide practical guidance for model selection.\n",
    "    \"\"\"\n",
    "    print(\"üéØ MODEL SELECTION DECISION TREE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    decision_tree = {\n",
    "        \"Do you need to generate new text?\": {\n",
    "            \"No (Classification/Understanding)\": {\n",
    "                \"Architecture\": \"Encoder-only\",\n",
    "                \"Examples\": \"BERT, RoBERTa, DeBERTa\",\n",
    "                \"Perfect for\": [\n",
    "                    \"Hate speech detection\",\n",
    "                    \"Sentiment analysis\", \n",
    "                    \"Text classification\",\n",
    "                    \"Named entity recognition\"\n",
    "                ]\n",
    "            },\n",
    "            \"Yes (Text Generation)\": {\n",
    "                \"Is your input different from output format?\": {\n",
    "                    \"Yes (Different formats)\": {\n",
    "                        \"Architecture\": \"Encoder-Decoder\",\n",
    "                        \"Examples\": \"T5, BART, mT5\",\n",
    "                        \"Perfect for\": [\n",
    "                            \"Machine translation\",\n",
    "                            \"Text summarization\",\n",
    "                            \"Question answering\",\n",
    "                            \"Code generation from text\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"No (Same format continuation)\": {\n",
    "                        \"Architecture\": \"Decoder-only\",\n",
    "                        \"Examples\": \"GPT-2/3/4, LLaMA\",\n",
    "                        \"Perfect for\": [\n",
    "                            \"Text completion\",\n",
    "                            \"Creative writing\",\n",
    "                            \"Conversational AI\",\n",
    "                            \"Code completion\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print decision tree\n",
    "    def print_decision_node(node, level=0):\n",
    "        indent = \"  \" * level\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key in [\"Architecture\", \"Examples\", \"Perfect for\"]:\n",
    "                    if key == \"Perfect for\":\n",
    "                        print(f\"{indent}   {key}:\")\n",
    "                        for item in value:\n",
    "                            print(f\"{indent}     ‚Ä¢ {item}\")\n",
    "                    else:\n",
    "                        print(f\"{indent}   {key}: {value}\")\n",
    "                else:\n",
    "                    print(f\"{indent}‚ùì {key}\")\n",
    "                    print_decision_node(value, level + 1)\n",
    "        elif isinstance(node, str):\n",
    "            print(f\"{indent}‚û°Ô∏è {node}\")\n",
    "    \n",
    "    print_decision_node(decision_tree)\n",
    "    \n",
    "    # Practical recommendations\n",
    "    print(\"\\nüí° PRACTICAL RECOMMENDATIONS\")\n",
    "    print(\"=\" * 32)\n",
    "    \n",
    "    recommendations = [\n",
    "        {\n",
    "            \"scenario\": \"Building a hate speech detection system\",\n",
    "            \"choice\": \"Encoder-only (BERT/RoBERTa)\",\n",
    "            \"reason\": \"Needs bi-directional context understanding, no generation required\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Creating a translation service\", \n",
    "            \"choice\": \"Encoder-Decoder (mT5/NLLB)\",\n",
    "            \"reason\": \"Different input/output languages, needs cross-attention\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Building a chatbot\",\n",
    "            \"choice\": \"Decoder-only (GPT family)\",\n",
    "            \"reason\": \"Conversational continuation, same format input/output\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Automatic document summarization\",\n",
    "            \"choice\": \"Encoder-Decoder (BART/PEGASUS)\",\n",
    "            \"reason\": \"Long input ‚Üí short output, requires content transformation\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Sentiment analysis for reviews\",\n",
    "            \"choice\": \"Encoder-only (DistilBERT)\",\n",
    "            \"reason\": \"Classification task, needs full context understanding\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(f\"\\nüéØ Scenario: {rec['scenario']}\")\n",
    "        print(f\"   Best Choice: {rec['choice']}\")\n",
    "        print(f\"   Why: {rec['reason']}\")\n",
    "\n",
    "# Run the selection guide\n",
    "model_selection_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of encoder-decoder architectures and how they work in practice.\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Encoder-Decoder Architecture**: Two-phase processing with specialized components\n",
    "- **Cross-Attention**: How decoders attend to encoder representations\n",
    "- **Auto-regressive Generation**: Step-by-step token generation process\n",
    "- **Mathematical Foundations**: Attention mechanisms and sequence probability\n",
    "- **Model Selection**: When to use encoder-only vs decoder-only vs encoder-decoder\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use encoder-decoder models for sequence transformation tasks\n",
    "- T5's text-to-text format provides flexibility across many tasks\n",
    "- BART excels at generation tasks with its denoising pre-training\n",
    "- Cross-attention provides interpretability for model decisions\n",
    "- Choose architecture based on task requirements, not popularity\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Fine-tuning**: Learn to adapt pre-trained models to your specific domains\n",
    "- **Evaluation Metrics**: Study BLEU, ROUGE, and BERTScore for generation quality\n",
    "- **Advanced Techniques**: Explore beam search, nucleus sampling, and length penalties\n",
    "- **Deployment**: Build scalable inference systems for production use\n",
    "- **Documentation**: Read the comprehensive [Encoder-Decoder Guide](../../docs/encoder-decoder.md)\n",
    "\n",
    "### üìö Additional Resources\n",
    "- [Hugging Face Sequence-to-Sequence Guide](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "- [T5 Paper: \"Exploring the Limits of Transfer Learning\"](https://arxiv.org/abs/1910.10683)\n",
    "- [BART Paper: \"Denoising Sequence-to-Sequence Pre-training\"](https://arxiv.org/abs/1910.13461)\n",
    "- [Attention Is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "### üß† Understanding the Impact\n",
    "\n",
    "Encoder-decoder models represent a significant advancement in NLP because they:\n",
    "- **Separate concerns**: Encoding (understanding) and decoding (generation)\n",
    "- **Enable flexibility**: Handle different input/output sequence lengths\n",
    "- **Provide interpretability**: Cross-attention shows what the model focuses on\n",
    "- **Scale effectively**: Can be adapted to many different tasks with task-specific prefixes\n",
    "\n",
    "The combination of bi-directional encoding and auto-regressive decoding with cross-attention makes encoder-decoder models exceptionally powerful for sequence-to-sequence tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}