{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/translation.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/translation.ipynb)\n",
    "\n",
    "# Translation with BART and T5\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How translation works as a sequence-to-sequence task\n",
    "- How BART and T5 models handle translation\n",
    "- Using encoder-decoder architectures for translation\n",
    "- Basic translation implementation with Hugging Face\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Setup**: Environment and device detection\n",
    "2. **Translation Basics**: Understanding sequence-to-sequence tasks\n",
    "3. **BART for Translation**: How BART adapts to translation\n",
    "4. **T5 for Translation**: Using T5's text-to-text framework\n",
    "5. **Practical Examples**: Simple translation demonstrations\n",
    "6. **Summary**: Key takeaways and next steps\n",
    "\n",
    "## What is Translation?\n",
    "\n",
    "Translation involves converting text from one language to another while preserving its meaning. Translation is a sequence-to-sequence task, which means you can use encoder-decoder models like **BART** or **T5** to do it.\n",
    "\n",
    "### How BART Works for Translation\n",
    "BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder's embeddings are passed to the pretrained encoder instead of the original word embeddings.\n",
    "\n",
    "### How T5 Works for Translation\n",
    "T5 treats translation as a text-to-text task, where the input is prefixed with \"translate [source] to [target]:\" followed by the source text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection for optimal performance\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the optimal device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Translation with Pipeline\n",
    "\n",
    "The simplest way to get started with translation is using Hugging Face's pipeline interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple translation pipeline\n",
    "print(\"üîÑ Loading translation model...\")\n",
    "print(\"This may take a few minutes on first run (downloading model)\")\n",
    "\n",
    "try:\n",
    "    # Use a simple English to German translation model\n",
    "    translator = pipeline(\n",
    "        \"translation\",\n",
    "        model=\"Helsinki-NLP/opus-mt-en-de\",\n",
    "        device=0 if device.type == 'cuda' else -1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Translation pipeline loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading translation model: {e}\")\n",
    "    print(\"üí° This model might not be available. Translation features may be limited.\")\n",
    "    translator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic translation\n",
    "if translator:\n",
    "    sample_texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I love learning about artificial intelligence.\",\n",
    "        \"The weather is beautiful today.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üåê English to German Translation Examples\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"\\n{i}. üá∫üá∏ English: {text}\")\n",
    "        \n",
    "        try:\n",
    "            result = translator(text)\n",
    "            german_text = result[0]['translation_text']\n",
    "            print(f\"   üá©üá™ German: {german_text}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Translation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Translation pipeline not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using BART for Translation\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers) is an encoder-decoder model that can be adapted for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a BART-based translation model manually\n",
    "print(\"üîÑ Loading BART-based translation model...\")\n",
    "\n",
    "try:\n",
    "    # Use a multilingual BART model for translation\n",
    "    model_name = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Move model to optimal device\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"‚úÖ BART model loaded successfully\")\n",
    "    \n",
    "    # Set source and target languages\n",
    "    # mBART uses language codes like \"en_XX\" for English, \"de_DE\" for German\n",
    "    tokenizer.src_lang = \"en_XX\"  # English source\n",
    "    \n",
    "    def translate_with_bart(text, target_lang=\"de_DE\"):\n",
    "        \"\"\"\n",
    "        Translate text using BART model\n",
    "        \n",
    "        Args:\n",
    "            text: Input text in source language\n",
    "            target_lang: Target language code\n",
    "        \n",
    "        Returns:\n",
    "            Translated text\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "                max_length=50,\n",
    "                num_beams=2,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated_text\n",
    "    \n",
    "    bart_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading BART model: {e}\")\n",
    "    print(\"üí° Using pipeline fallback instead\")\n",
    "    bart_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BART translation\n",
    "if bart_available:\n",
    "    test_text = \"Machine learning is revolutionizing technology.\"\n",
    "    \n",
    "    print(\"ü§ñ BART Translation Example\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"üá∫üá∏ English: {test_text}\")\n",
    "    \n",
    "    try:\n",
    "        translated = translate_with_bart(test_text)\n",
    "        print(f\"üá©üá™ German (BART): {translated}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BART translation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå BART model not available for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using T5 for Translation\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer) treats every NLP task as a text-to-text problem, including translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T5 translation pipeline\n",
    "print(\"üîÑ Loading T5 model for translation...\")\n",
    "\n",
    "try:\n",
    "    # Use T5 small model for demonstration\n",
    "    t5_translator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"t5-small\",\n",
    "        device=0 if device.type == 'cuda' else -1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ T5 model loaded successfully\")\n",
    "    \n",
    "    def translate_with_t5(text, source_lang=\"English\", target_lang=\"German\"):\n",
    "        \"\"\"\n",
    "        Translate text using T5's text-to-text format\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to translate\n",
    "            source_lang: Source language name\n",
    "            target_lang: Target language name\n",
    "        \n",
    "        Returns:\n",
    "            Translated text\n",
    "        \"\"\"\n",
    "        # T5 uses task prefixes - format for translation\n",
    "        prompt = f\"translate {source_lang} to {target_lang}: {text}\"\n",
    "        \n",
    "        result = t5_translator(prompt, max_length=50, num_return_sequences=1)\n",
    "        return result[0]['generated_text']\n",
    "    \n",
    "    t5_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading T5 model: {e}\")\n",
    "    print(\"üí° T5 model might not be available\")\n",
    "    t5_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test T5 translation\n",
    "if t5_available:\n",
    "    test_sentences = [\n",
    "        \"Hello world!\",\n",
    "        \"How are you doing today?\",\n",
    "        \"I enjoy learning new languages.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìù T5 Translation Examples\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        print(f\"\\n{i}. üá∫üá∏ English: {sentence}\")\n",
    "        \n",
    "        try:\n",
    "            translated = translate_with_t5(sentence)\n",
    "            print(f\"   üá©üá™ German (T5): {translated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Translation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå T5 model not available for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Model Differences\n",
    "\n",
    "Let's compare how different models approach the same translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different approaches\n",
    "test_phrase = \"Artificial intelligence is changing the world.\"\n",
    "\n",
    "print(\"üîç Model Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìù Original: {test_phrase}\")\n",
    "print()\n",
    "\n",
    "# Pipeline translation (OPUS-MT)\n",
    "if translator:\n",
    "    try:\n",
    "        pipeline_result = translator(test_phrase)\n",
    "        print(f\"üîß Pipeline (OPUS-MT): {pipeline_result[0]['translation_text']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üîß Pipeline (OPUS-MT): ‚ùå {e}\")\n",
    "\n",
    "# BART translation\n",
    "if bart_available:\n",
    "    try:\n",
    "        bart_result = translate_with_bart(test_phrase)\n",
    "        print(f\"ü§ñ BART (mBART): {bart_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ü§ñ BART (mBART): ‚ùå {e}\")\n",
    "\n",
    "# T5 translation\n",
    "if t5_available:\n",
    "    try:\n",
    "        t5_result = translate_with_t5(test_phrase)\n",
    "        print(f\"üìù T5: {t5_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üìù T5: ‚ùå {e}\")\n",
    "\n",
    "print(\"\\nüí° Each model has different strengths:\")\n",
    "print(\"   ‚Ä¢ OPUS-MT: Specialized for specific language pairs\")\n",
    "print(\"   ‚Ä¢ BART/mBART: Good for multilingual translation\")\n",
    "print(\"   ‚Ä¢ T5: Flexible text-to-text approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of machine translation with encoder-decoder models.\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Sequence-to-Sequence Tasks**: Understanding how translation works as an encoder-decoder problem\n",
    "- **BART for Translation**: How BART adapts with additional encoders for cross-language tasks\n",
    "- **T5 for Translation**: Using text-to-text format for translation tasks\n",
    "- **Model Comparison**: Different approaches and their trade-offs\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use specialized models (OPUS-MT) for specific language pairs\n",
    "- Consider multilingual models (mBART) for broader language coverage\n",
    "- Leverage T5's flexibility for various text-to-text tasks\n",
    "- Always handle errors gracefully in production systems\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced Models**: Explore more sophisticated models like NLLB (No Language Left Behind)\n",
    "- **Fine-tuning**: Learn to fine-tune models on domain-specific data\n",
    "- **Evaluation**: Study translation quality metrics (BLEU, METEOR, BERTScore)\n",
    "- **Production**: Build scalable translation APIs and services\n",
    "\n",
    "### üìö Additional Resources\n",
    "- [Hugging Face Translation Guide](https://huggingface.co/docs/transformers/tasks/translation)\n",
    "- [BART Documentation](https://huggingface.co/docs/transformers/model_doc/bart)\n",
    "- [T5 Documentation](https://huggingface.co/docs/transformers/model_doc/t5)\n",
    "- [mBART Documentation](https://huggingface.co/docs/transformers/model_doc/mbart)\n",
    "\n",
    "As you've seen throughout this guide, many models follow similar patterns despite addressing different tasks. Understanding these common patterns can help you quickly grasp how new models work and how to adapt existing models to your specific needs.\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}