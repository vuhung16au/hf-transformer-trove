{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "text-classification-header"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/text-classification.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/text-classification.ipynb)\n",
    "\n",
    "# Text Classification with BERT\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What text classification is and its applications\n",
    "- How BERT works for text classification tasks\n",
    "- How to use pre-trained BERT models for text classification\n",
    "- The structure of BERT's classification outputs\n",
    "- Device awareness for optimal performance\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Introduction**: Understanding text classification and BERT\n",
    "2. **Setup**: Installing and importing required libraries\n",
    "3. **BERT Model**: Loading and configuring BERT for classification\n",
    "4. **Simple Classification**: Basic text classification examples\n",
    "5. **Understanding Outputs**: Interpreting BERT's classification results\n",
    "6. **Summary**: Key takeaways and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Text Classification with BERT\n",
    "\n",
    "**Text Classification** involves assigning predefined categories to text documents, such as sentiment analysis, topic classification, or spam detection.\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "[**BERT (Bidirectional Encoder Representations from Transformers)**](https://huggingface.co/docs/transformers/model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\n",
    "\n",
    "### Key BERT Features for Classification:\n",
    "- **Bidirectional Context**: Unlike traditional models, BERT reads text in both directions\n",
    "- **WordPiece Tokenization**: Handles unknown words by breaking them into subword tokens\n",
    "- **Special Tokens**: Uses [CLS] token for classification tasks\n",
    "- **Pre-trained**: Already trained on massive text corpora\n",
    "\n",
    "### How BERT Performs Classification:\n",
    "1. **[CLS] Token**: Added at the beginning of every sequence\n",
    "2. **[SEP] Token**: Separates sentences when needed\n",
    "3. **Classification Head**: Linear layer that takes [CLS] output for classification\n",
    "4. **Softmax**: Converts logits to probability scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install transformers torch datasets tokenizers matplotlib seaborn\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Device Detection and Setup\n",
    "\n",
    "Let's detect the best available device for optimal performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get optimal device\n",
    "device = get_device()\n",
    "print(f\"üì± Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading BERT for Text Classification\n",
    "\n",
    "We'll use a pre-trained BERT model fine-tuned for sentiment analysis as our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pre-trained BERT model for sentiment analysis\n",
    "model_name = \"bert-base-uncased\"\n",
    "# Alternative: \"distilbert-base-uncased-finetuned-sst-2-english\" for sentiment analysis\n",
    "\n",
    "print(f\"üì• Loading BERT model: {model_name}\")\n",
    "print(\"‚è±Ô∏è This may take a moment for first-time download...\")\n",
    "\n",
    "# Load tokenizer and model with error handling\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "    \n",
    "    # For basic demonstration, we'll use a pipeline\n",
    "    # This automatically loads the appropriate model architecture\n",
    "    classifier = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        device=0 if device.type == \"cuda\" else -1  # Use GPU if available\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ BERT classification pipeline loaded successfully\")\n",
    "    print(f\"üìä Model ready for inference on {device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üí° Try checking your internet connection or model name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Text Classification Examples\n",
    "\n",
    "Let's test our BERT model with various text examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test texts for classification\n",
    "test_texts = [\n",
    "    \"I love using BERT for text classification!\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"The weather today is okay, nothing special.\",\n",
    "    \"BERT is a powerful transformer model for NLP.\",\n",
    "    \"I hate waiting in long queues.\"\n",
    "]\n",
    "\n",
    "print(\"üîç Performing Text Classification with BERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each text with timing\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\nüìù Example {i}:\")\n",
    "    print(f\"Text: '{text}'\")\n",
    "    \n",
    "    # Perform classification with timing\n",
    "    start_time = time.time()\n",
    "    result = classifier(text)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Extract results\n",
    "    label = result[0]['label']\n",
    "    confidence = result[0]['score']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"üéØ Prediction: {label}\")\n",
    "    print(f\"üìä Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"‚è±Ô∏è Inference time: {inference_time:.3f} seconds\")\n",
    "    \n",
    "    # Store for analysis\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'confidence': confidence,\n",
    "        'inference_time': inference_time\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Classified {len(test_texts)} texts successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding BERT's Classification Process\n",
    "\n",
    "Let's examine what happens under the hood when BERT classifies text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the tokenization process\n",
    "example_text = \"BERT is amazing for text classification!\"\n",
    "\n",
    "print(\"üîç BERT Tokenization Process\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original text: '{example_text}'\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(f\"\\nüî§ Tokens: {tokens}\")\n",
    "\n",
    "# Convert to input IDs\n",
    "input_ids = tokenizer.encode(example_text)\n",
    "print(f\"\\nüî¢ Input IDs: {input_ids}\")\n",
    "\n",
    "# Show special tokens\n",
    "print(f\"\\nüè∑Ô∏è Special Tokens:\")\n",
    "print(f\"   [CLS] token ID: {tokenizer.cls_token_id} ('{tokenizer.cls_token}')\")\n",
    "print(f\"   [SEP] token ID: {tokenizer.sep_token_id} ('{tokenizer.sep_token}')\")\n",
    "print(f\"   [PAD] token ID: {tokenizer.pad_token_id} ('{tokenizer.pad_token}')\")\n",
    "\n",
    "# Decode back to verify\n",
    "decoded = tokenizer.decode(input_ids)\n",
    "print(f\"\\nüîÑ Decoded: '{decoded}'\")\n",
    "\n",
    "print(f\"\\nüí° Notice how BERT adds [CLS] at the start and [SEP] at the end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization of Results\n",
    "\n",
    "Let's visualize our classification results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of our results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Confidence scores\n",
    "texts_short = [r['text'][:30] + '...' if len(r['text']) > 30 else r['text'] for r in results]\n",
    "confidences = [r['confidence'] for r in results]\n",
    "colors = ['green' if r['label'] == 'POSITIVE' else 'red' for r in results]\n",
    "\n",
    "ax1.barh(range(len(texts_short)), confidences, color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(texts_short)))\n",
    "ax1.set_yticklabels(texts_short)\n",
    "ax1.set_xlabel('Confidence Score')\n",
    "ax1.set_title('BERT Classification Confidence Scores')\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Add confidence values as text\n",
    "for i, conf in enumerate(confidences):\n",
    "    ax1.text(conf + 0.01, i, f'{conf:.3f}', va='center')\n",
    "\n",
    "# Plot 2: Inference times\n",
    "inference_times = [r['inference_time'] for r in results]\n",
    "ax2.bar(range(len(texts_short)), inference_times, color='blue', alpha=0.7)\n",
    "ax2.set_xticks(range(len(texts_short)))\n",
    "ax2.set_xticklabels([f'Text {i+1}' for i in range(len(texts_short))], rotation=45)\n",
    "ax2.set_ylabel('Inference Time (seconds)')\n",
    "ax2.set_title('BERT Inference Performance')\n",
    "\n",
    "# Add time values as text\n",
    "for i, time_val in enumerate(inference_times):\n",
    "    ax2.text(i, time_val + max(inference_times)*0.01, f'{time_val:.3f}s', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "avg_confidence = np.mean(confidences)\n",
    "avg_time = np.mean(inference_times)\n",
    "\n",
    "print(f\"üìä Summary Statistics:\")\n",
    "print(f\"   Average Confidence: {avg_confidence:.4f}\")\n",
    "print(f\"   Average Inference Time: {avg_time:.4f} seconds\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Concepts Summary\n",
    "\n",
    "### üß† What We Learned About BERT:\n",
    "\n",
    "1. **Bidirectional Context**: BERT reads text in both directions, giving it a complete understanding of context\n",
    "2. **WordPiece Tokenization**: Breaks down unknown words into manageable subword tokens\n",
    "3. **Special Tokens**: Uses [CLS] token for classification and [SEP] to separate sentences\n",
    "4. **Pre-trained Power**: Leverages massive pre-training on text corpora\n",
    "\n",
    "### üîß Technical Implementation:\n",
    "- **Pipeline API**: Simplifies model usage for common tasks\n",
    "- **Device Awareness**: Automatically utilizes available hardware (GPU/CPU)\n",
    "- **Confidence Scores**: Provides probability estimates for predictions\n",
    "\n",
    "### üìà Performance Insights:\n",
    "- BERT provides highly confident predictions for clear sentiment\n",
    "- Inference time varies but is generally fast for individual texts\n",
    "- GPU acceleration significantly improves performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Text Classification**: Understanding how to assign categories to text using BERT\n",
    "- **BERT Architecture**: How bidirectional encoding works for classification tasks\n",
    "- **Tokenization**: WordPiece tokenization and special tokens ([CLS], [SEP])\n",
    "- **Pipeline Usage**: Using Hugging Face pipelines for simple classification tasks\n",
    "- **Device Optimization**: Leveraging GPU/MPS for faster inference\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use device detection for optimal performance across different hardware\n",
    "- Pre-trained models provide excellent out-of-the-box performance\n",
    "- Pipeline API simplifies complex model operations\n",
    "- Always examine confidence scores to assess prediction reliability\n",
    "- Visualization helps understand model behavior and performance\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Fine-tuning**: Learn to adapt BERT for custom classification tasks\n",
    "- **Advanced Models**: Explore RoBERTa, DeBERTa, and other BERT variants\n",
    "- **Multi-class Classification**: Handle more than two categories\n",
    "- **Documentation**: [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/)\n",
    "- **BERT Paper**: [Original BERT Research Paper](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}