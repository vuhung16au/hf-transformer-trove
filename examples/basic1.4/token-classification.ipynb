{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/token-classification.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.4/token-classification.ipynb)\n",
        "\n",
        "# Token Classification with BERT\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- What token classification is and its applications (NER, POS tagging)\n",
        "- How to use BERT for token classification tasks\n",
        "- The structure of token classification outputs and label formats\n",
        "- How transformer models assign labels to individual tokens\n",
        "- Common use cases and practical applications\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with Python programming\n",
        "- Basic knowledge of Natural Language Processing (NLP) concepts\n",
        "- Understanding of tokenization (refer to [Tokenizers Notebook](../02_tokenizers.ipynb))\n",
        "\n",
        "## üìö What We'll Cover\n",
        "1. **Introduction**: What is token classification?\n",
        "2. **Setup**: Installing and importing required libraries\n",
        "3. **Core Implementation**: Using BERT for Named Entity Recognition\n",
        "4. **Understanding Output**: Interpreting token labels and confidence scores\n",
        "5. **Advanced Examples**: Testing different text inputs and entity types\n",
        "6. **Visualization**: Displaying results in a readable format\n",
        "7. **Summary**: Key takeaways and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "## 1. Introduction to Token Classification\n",
        "\n",
        "**Token Classification** is a fundamental NLP task that assigns a label to each token (word or sub-word) in a sequence. Unlike sequence classification which assigns one label to the entire text, token classification provides granular, token-level predictions.\n",
        "\n",
        "### Common Applications:\n",
        "\n",
        "- **Named Entity Recognition (NER)**: Identifying entities like persons, locations, organizations\n",
        "- **Part-of-Speech (POS) Tagging**: Identifying grammatical roles (noun, verb, adjective)\n",
        "- **Chunk Parsing**: Identifying phrases and syntactic structures\n",
        "- **Information Extraction**: Extracting specific data fields from documents\n",
        "\n",
        "### How Token Classification Works:\n",
        "\n",
        "1. **Tokenize** the input text into individual tokens\n",
        "2. **Encode** each token into numerical representations\n",
        "3. **Process** the sequence using transformer attention mechanisms\n",
        "4. **Classify** each token with a specific label and confidence score\n",
        "5. **Aggregate** sub-word tokens back into complete entities\n",
        "\n",
        "### Label Formats:\n",
        "- **BIO Format**: B (Beginning), I (Inside), O (Outside)\n",
        "  - Example: `John Smith lives in New York` ‚Üí `B-PER I-PER O O B-LOC I-LOC`\n",
        "- **Entity Types**: PERSON, LOCATION, ORGANIZATION, MISCELLANEOUS, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 2. Setup and Installation\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run only once in Colab)\n",
        "# !pip install transformers torch numpy pandas\n",
        "\n",
        "# Import essential libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device detection for optimal performance\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device for PyTorch operations.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "print(\"üìö Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "core_implementation"
      },
      "source": [
        "## 3. Core Implementation with BERT\n",
        "\n",
        "We'll use a pre-trained BERT model fine-tuned for Named Entity Recognition. This model can identify various types of entities in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained BERT model for token classification (NER)\n",
        "# Using a reliable model with proper entity label mappings\n",
        "model_name = \"dslim/bert-base-NER\"\n",
        "\n",
        "print(f\"üì• Loading model: {model_name}\")\n",
        "print(\"   This model is optimized for Named Entity Recognition...\")\n",
        "\n",
        "# Create token classification pipeline\n",
        "# Pipeline automatically handles tokenization, prediction, and post-processing\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",  # Named Entity Recognition task\n",
        "    model=model_name,\n",
        "    tokenizer=model_name,\n",
        "    aggregation_strategy=\"simple\",  # Aggregate sub-word tokens into complete entities\n",
        "    device=0 if device.type == \"cuda\" else -1  # Use GPU if available\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üîß Model details:\")\n",
        "print(f\"   Task: Named Entity Recognition (NER)\")\n",
        "print(f\"   Base Model: BERT Base\")\n",
        "print(f\"   Training Data: CoNLL-2003 NER dataset\")\n",
        "print(f\"   Supported Entities: PERSON, LOCATION, ORGANIZATION, MISCELLANEOUS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basic_example"
      },
      "source": [
        "## 4. Basic Token Classification Example\n",
        "\n",
        "Let's start with a simple example to understand how token classification works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_example_code"
      },
      "outputs": [],
      "source": [
        "# Example text with various named entities\n",
        "sample_text = \"My name is John Smith and I work at Microsoft in Seattle, Washington.\"\n",
        "\n",
        "print(f\"üìù Input Text: '{sample_text}'\")\n",
        "print(\"\\nüîç Performing token classification...\")\n",
        "\n",
        "# Perform named entity recognition\n",
        "entities = ner_pipeline(sample_text)\n",
        "\n",
        "print(f\"\\n‚ú® Found {len(entities)} entities:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display results in a readable format\n",
        "for i, entity in enumerate(entities, 1):\n",
        "    print(f\"{i}. Entity: '{entity['word']}'\")\n",
        "    print(f\"   Type: {entity['entity_group']}\")\n",
        "    print(f\"   Confidence: {entity['score']:.4f} ({entity['score']*100:.1f}%)\")\n",
        "    print(f\"   Position: characters {entity['start']}-{entity['end']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "understand_output"
      },
      "source": [
        "## 5. Understanding the Output\n",
        "\n",
        "Let's break down what each part of the output means and explore the model's confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "output_analysis"
      },
      "outputs": [],
      "source": [
        "# Create a more detailed analysis function\n",
        "def analyze_entities(text, show_tokens=True):\n",
        "    \"\"\"Analyze entities with detailed breakdown.\"\"\"\n",
        "    \n",
        "    print(f\"üìä Detailed Analysis of: '{text}'\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Get entities\n",
        "    entities = ner_pipeline(text)\n",
        "    \n",
        "    if not entities:\n",
        "        print(\"‚ùå No entities found in the text.\")\n",
        "        return\n",
        "    \n",
        "    # Entity type summary\n",
        "    entity_types = {}\n",
        "    for entity in entities:\n",
        "        entity_type = entity['entity_group']\n",
        "        if entity_type not in entity_types:\n",
        "            entity_types[entity_type] = []\n",
        "        entity_types[entity_type].append(entity['word'])\n",
        "    \n",
        "    print(f\"üìà Entity Summary:\")\n",
        "    for entity_type, words in entity_types.items():\n",
        "        print(f\"   {entity_type}: {len(words)} entities ‚Üí {', '.join(words)}\")\n",
        "    \n",
        "    print(f\"\\nüîç Detailed Results:\")\n",
        "    \n",
        "    # Detailed entity information\n",
        "    for i, entity in enumerate(entities, 1):\n",
        "        confidence_level = \"High\" if entity['score'] > 0.9 else \"Medium\" if entity['score'] > 0.7 else \"Low\"\n",
        "        \n",
        "        print(f\"\\n{i}. '{entity['word']}' ‚Üí {entity['entity_group']}\")\n",
        "        print(f\"   üìç Position: {entity['start']}-{entity['end']} | Confidence: {entity['score']:.4f} ({confidence_level})\")\n",
        "        \n",
        "        # Extract context (words around the entity)\n",
        "        start_context = max(0, entity['start'] - 20)\n",
        "        end_context = min(len(text), entity['end'] + 20)\n",
        "        context = text[start_context:end_context]\n",
        "        print(f\"   üî§ Context: '...{context}...'\")\n",
        "\n",
        "# Test with our sample\n",
        "analyze_entities(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_examples"
      },
      "source": [
        "## 6. Advanced Examples with Different Entity Types\n",
        "\n",
        "Let's test the model with various types of texts to see how it handles different scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_examples_code"
      },
      "outputs": [],
      "source": [
        "# Test cases with different types of entities\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"Business Context\",\n",
        "        \"text\": \"Apple Inc. CEO Tim Cook announced new products at the Apple Park campus in Cupertino, California.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"News Article\",\n",
        "        \"text\": \"President Biden met with European leaders in Brussels to discuss NATO policies.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Academic Context\",\n",
        "        \"text\": \"The research was conducted by Dr. Sarah Johnson at Stanford University in collaboration with MIT.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Mixed Entities\",\n",
        "        \"text\": \"Google's headquarters in Mountain View, California, hosts thousands of engineers working on AI projects.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Multiple Scenarios:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, case in enumerate(test_cases, 1):\n",
        "    print(f\"\\nüìã Test Case {i}: {case['name']}\")\n",
        "    print(f\"üìù Text: '{case['text']}'\")\n",
        "    \n",
        "    entities = ner_pipeline(case['text'])\n",
        "    \n",
        "    if entities:\n",
        "        print(f\"‚úÖ Found {len(entities)} entities:\")\n",
        "        for entity in entities:\n",
        "            print(f\"   ‚Ä¢ '{entity['word']}' ‚Üí {entity['entity_group']} (confidence: {entity['score']:.3f})\")\n",
        "    else:\n",
        "        print(\"‚ùå No entities detected\")\n",
        "    \n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 7. Visualization of Results\n",
        "\n",
        "Let's create a visual representation of the token classification results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_code"
      },
      "outputs": [],
      "source": [
        "def visualize_entities(text, title=\"Entity Visualization\"):\n",
        "    \"\"\"Create a visual representation of entities in text.\"\"\"\n",
        "    \n",
        "    entities = ner_pipeline(text)\n",
        "    \n",
        "    if not entities:\n",
        "        print(f\"üìä {title}\")\n",
        "        print(\"‚ùå No entities found to visualize.\")\n",
        "        return\n",
        "    \n",
        "    # Color coding for different entity types\n",
        "    colors = {\n",
        "        'PER': 'üü¶',  # Person - Blue\n",
        "        'ORG': 'üü©',  # Organization - Green\n",
        "        'LOC': 'üü®',  # Location - Yellow\n",
        "        'MISC': 'üü™'  # Miscellaneous - Purple\n",
        "    }\n",
        "    \n",
        "    print(f\"üìä {title}\")\n",
        "    print(\"=\" * len(title))\n",
        "    print(f\"üìù Original: {text}\")\n",
        "    print()\n",
        "    \n",
        "    # Create annotated version\n",
        "    annotated_text = text\n",
        "    offset = 0\n",
        "    \n",
        "    # Sort entities by start position (reverse order for proper insertion)\n",
        "    entities_sorted = sorted(entities, key=lambda x: x['start'], reverse=True)\n",
        "    \n",
        "    for entity in entities_sorted:\n",
        "        entity_type = entity['entity_group']\n",
        "        color = colors.get(entity_type, 'üü´')\n",
        "        \n",
        "        # Insert annotation\n",
        "        start_pos = entity['start']\n",
        "        end_pos = entity['end']\n",
        "        \n",
        "        # Create the annotation\n",
        "        annotation = f\"{color}[{entity['word']}({entity_type})]\"\n",
        "        \n",
        "        # Replace in the text\n",
        "        annotated_text = annotated_text[:start_pos] + annotation + annotated_text[end_pos:]\n",
        "    \n",
        "    print(f\"üéØ Annotated: {annotated_text}\")\n",
        "    print()\n",
        "    \n",
        "    # Legend\n",
        "    print(\"üìã Legend:\")\n",
        "    legend_items = []\n",
        "    for entity in entities:\n",
        "        entity_type = entity['entity_group']\n",
        "        if entity_type not in [item['type'] for item in legend_items]:\n",
        "            legend_items.append({\n",
        "                'type': entity_type,\n",
        "                'color': colors.get(entity_type, 'üü´'),\n",
        "                'full_name': {\n",
        "                    'PER': 'Person',\n",
        "                    'ORG': 'Organization', \n",
        "                    'LOC': 'Location',\n",
        "                    'MISC': 'Miscellaneous'\n",
        "                }.get(entity_type, entity_type)\n",
        "            })\n",
        "    \n",
        "    for item in legend_items:\n",
        "        print(f\"   {item['color']} {item['type']} = {item['full_name']}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Visualize our examples\n",
        "visualize_entities(sample_text, \"Sample Text Visualization\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Visualize a complex example\n",
        "complex_text = \"Barack Obama visited Google headquarters in Mountain View and met with Sundar Pichai to discuss AI innovations.\"\n",
        "visualize_entities(complex_text, \"Complex Example Visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_details"
      },
      "source": [
        "## 8. Understanding the Model Architecture\n",
        "\n",
        "Let's explore the technical details of our BERT-based token classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_details_code"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model separately for detailed analysis\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"üîß Model Technical Details:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Model configuration\n",
        "config = model.config\n",
        "print(f\"üìã Model Configuration:\")\n",
        "print(f\"   Model Name: {config.name_or_path}\")\n",
        "print(f\"   Model Type: {config.model_type.upper()}\")\n",
        "print(f\"   Number of Labels: {config.num_labels}\")\n",
        "print(f\"   Vocabulary Size: {config.vocab_size:,}\")\n",
        "print(f\"   Max Sequence Length: {config.max_position_embeddings}\")\n",
        "print(f\"   Hidden Size: {config.hidden_size}\")\n",
        "print(f\"   Number of Attention Heads: {config.num_attention_heads}\")\n",
        "print(f\"   Number of Hidden Layers: {config.num_hidden_layers}\")\n",
        "\n",
        "# Label mapping\n",
        "print(f\"\\nüè∑Ô∏è  Label Mapping:\")\n",
        "for label_id, label_name in config.id2label.items():\n",
        "    print(f\"   {label_id}: {label_name}\")\n",
        "\n",
        "# Model size information\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Model Size Information:\")\n",
        "print(f\"   Total Parameters: {total_params:,}\")\n",
        "print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"   Model Size: ~{total_params/1_000_000:.1f}M parameters\")\n",
        "print(f\"   Memory Footprint: ~{total_params * 4 / (1024**3):.2f} GB (FP32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance"
      },
      "source": [
        "## 9. Performance Analysis\n",
        "\n",
        "Let's analyze the performance characteristics of token classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_performance(texts, num_runs=3):\n",
        "    \"\"\"Benchmark the performance of token classification.\"\"\"\n",
        "    \n",
        "    times = []\n",
        "    total_tokens = 0\n",
        "    total_entities = 0\n",
        "    \n",
        "    print(f\"‚è±Ô∏è Performance Benchmark ({num_runs} runs):\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for run in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for text in texts:\n",
        "            entities = ner_pipeline(text)\n",
        "            \n",
        "            # Count tokens and entities\n",
        "            tokens = tokenizer.tokenize(text)\n",
        "            total_tokens += len(tokens)\n",
        "            total_entities += len(entities)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "        \n",
        "        print(f\"   Run {run + 1}: {times[-1]:.3f}s\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    avg_time = sum(times) / len(times)\n",
        "    avg_tokens = total_tokens / (len(texts) * num_runs)\n",
        "    avg_entities = total_entities / (len(texts) * num_runs)\n",
        "    \n",
        "    print(f\"\\nüìä Performance Statistics:\")\n",
        "    print(f\"   Average Time per Batch: {avg_time:.3f}s\")\n",
        "    print(f\"   Average Time per Text: {avg_time/len(texts):.3f}s\")\n",
        "    print(f\"   Tokens per Second: {(total_tokens/num_runs)/avg_time:.1f}\")\n",
        "    print(f\"   Average Tokens per Text: {avg_tokens:.1f}\")\n",
        "    print(f\"   Average Entities per Text: {avg_entities:.1f}\")\n",
        "    print(f\"   Processing Speed: {len(texts)/avg_time:.1f} texts/second\")\n",
        "\n",
        "# Performance test with various text lengths\n",
        "perf_texts = [\n",
        "    \"John works at Google.\",  # Short\n",
        "    \"Microsoft CEO Satya Nadella visited the Seattle office to discuss Azure innovations with the engineering team.\",  # Medium\n",
        "    \"The meeting between Apple's Tim Cook and Google's Sundar Pichai in Mountain View, California, focused on artificial intelligence collaboration between the two tech giants, with discussions covering machine learning, privacy policies, and future technological partnerships that could benefit consumers worldwide.\"  # Long\n",
        "]\n",
        "\n",
        "benchmark_performance(perf_texts)\n",
        "\n",
        "# Memory usage tip\n",
        "print(f\"\\nüí° Performance Tips:\")\n",
        "print(f\"   ‚Ä¢ Batch processing multiple texts together is more efficient\")\n",
        "print(f\"   ‚Ä¢ GPU acceleration significantly improves speed for large batches\")\n",
        "print(f\"   ‚Ä¢ Consider using smaller models (distilbert) for faster inference\")\n",
        "print(f\"   ‚Ä¢ Enable mixed precision (FP16) to reduce memory usage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_section"
      },
      "source": [
        "---\n",
        "\n",
        "## üìã Summary\n",
        "\n",
        "### üîë Key Concepts Mastered\n",
        "- **Token Classification**: Understanding how to assign labels to individual tokens in text\n",
        "- **BERT for NER**: Using pre-trained BERT models for Named Entity Recognition tasks\n",
        "- **Entity Types**: Recognizing different types of entities (Person, Organization, Location, Miscellaneous)\n",
        "- **BIO Labeling**: Understanding the Begin-Inside-Outside labeling scheme\n",
        "- **Confidence Scores**: Interpreting model confidence and prediction reliability\n",
        "- **Aggregation Strategy**: How sub-word tokens are combined into complete entities\n",
        "\n",
        "### üìà Best Practices Learned\n",
        "- Use aggregation strategies to combine sub-word tokens into meaningful entities\n",
        "- Monitor confidence scores to assess prediction reliability\n",
        "- Consider context when interpreting entity predictions\n",
        "- Batch processing improves efficiency for multiple texts\n",
        "- Validate results especially for domain-specific or uncommon entity types\n",
        "\n",
        "### üöÄ Next Steps\n",
        "- **Fine-tuning**: Learn to fine-tune BERT for custom entity types in [Fine-tuning Notebook](../05_fine_tuning_trainer.ipynb)\n",
        "- **Custom Datasets**: Explore training on domain-specific NER datasets\n",
        "- **Other Models**: Try different model architectures (RoBERTa, DistilBERT, SpaCy)\n",
        "- **Documentation**: Review [Hugging Face Token Classification Guide](https://huggingface.co/docs/transformers/tasks/token_classification)\n",
        "- **Model Hub**: Explore specialized [NER models](https://huggingface.co/models?pipeline_tag=token-classification) for different domains\n",
        "\n",
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}