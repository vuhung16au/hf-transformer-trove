{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - The Datasets Library: Efficient Data Handling for NLP\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What the Hugging Face datasets library provides\n",
    "- How to load datasets from the Hub and local files\n",
    "- Dataset processing and transformation techniques\n",
    "- Working with large datasets efficiently (streaming)\n",
    "- Creating custom datasets\n",
    "- Integrating datasets with tokenizers and models\n",
    "\n",
    "## Why Use the Datasets Library?\n",
    "\n",
    "The Hugging Face datasets library provides:\n",
    "- **Unified API** for accessing thousands of datasets\n",
    "- **Efficient storage** with Apache Arrow backend\n",
    "- **Memory mapping** for large datasets\n",
    "- **Streaming** for datasets too large for memory\n",
    "- **Built-in preprocessing** and caching\n",
    "- **Easy integration** with transformers and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import (\n",
    "    load_dataset, Dataset, DatasetDict,\n",
    "    load_from_disk, concatenate_datasets\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Datasets from the Hub\n",
    "\n",
    "The Hugging Face Hub hosts thousands of datasets for various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a popular sentiment analysis dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"Dataset type: {type(imdb_dataset)}\")\n",
    "print(f\"Dataset splits: {list(imdb_dataset.keys())}\")\n",
    "print(f\"Train set size: {len(imdb_dataset['train']):,}\")\n",
    "print(f\"Test set size: {len(imdb_dataset['test']):,}\")\n",
    "\n",
    "# Examine the structure\n",
    "print(f\"\\nDataset features: {imdb_dataset['train'].features}\")\n",
    "print(f\"Column names: {imdb_dataset['train'].column_names}\")\n",
    "\n",
    "# Look at a few examples\n",
    "print(\"\\nFirst example:\")\n",
    "first_example = imdb_dataset['train'][0]\n",
    "print(f\"Text (first 200 chars): {first_example['text'][:200]}...\")\n",
    "print(f\"Label: {first_example['label']} ({'positive' if first_example['label'] == 1 else 'negative'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other popular datasets\n",
    "datasets_info = {\n",
    "    \"Squad (Question Answering)\": {\"name\": \"squad\", \"config\": None},\n",
    "    \"CNN/DailyMail (Summarization)\": {\"name\": \"cnn_dailymail\", \"config\": \"3.0.0\"},\n",
    "    \"CoLA (Grammar)\": {\"name\": \"glue\", \"config\": \"cola\"},\n",
    "    \"AG News (Classification)\": {\"name\": \"ag_news\", \"config\": None}\n",
    "}\n",
    "\n",
    "loaded_datasets = {}\n",
    "\n",
    "for dataset_desc, info in datasets_info.items():\n",
    "    try:\n",
    "        print(f\"\\nLoading {dataset_desc}...\")\n",
    "        \n",
    "        if info[\"config\"]:\n",
    "            dataset = load_dataset(info[\"name\"], info[\"config\"])\n",
    "        else:\n",
    "            dataset = load_dataset(info[\"name\"])\n",
    "        \n",
    "        loaded_datasets[dataset_desc] = dataset\n",
    "        \n",
    "        print(f\"  Splits: {list(dataset.keys())}\")\n",
    "        print(f\"  Features: {dataset[list(dataset.keys())[0]].features}\")\n",
    "        print(f\"  Size: {len(dataset[list(dataset.keys())[0]]):,} examples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {dataset_desc}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the IMDB dataset in detail\n",
    "train_dataset = imdb_dataset['train']\n",
    "\n",
    "print(\"IMDB Dataset Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Label distribution\n",
    "labels = train_dataset['label']\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Label distribution: {dict(label_counts)}\")\n",
    "print(f\"Class balance: {label_counts[0] / len(labels):.2%} negative, {label_counts[1] / len(labels):.2%} positive\")\n",
    "\n",
    "# Text length analysis\n",
    "text_lengths = [len(text.split()) for text in train_dataset['text'][:1000]]  # Sample first 1000\n",
    "\n",
    "print(f\"\\nText length statistics (first 1000 examples):\")\n",
    "print(f\"  Mean: {np.mean(text_lengths):.1f} words\")\n",
    "print(f\"  Median: {np.median(text_lengths):.1f} words\")\n",
    "print(f\"  Min: {min(text_lengths)} words\")\n",
    "print(f\"  Max: {max(text_lengths)} words\")\n",
    "print(f\"  Standard deviation: {np.std(text_lengths):.1f} words\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Label distribution\n",
    "ax1.bar(['Negative', 'Positive'], [label_counts[0], label_counts[1]])\n",
    "ax1.set_title('Label Distribution (IMDB)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Text length distribution\n",
    "ax2.hist(text_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Text Length Distribution (Words)')\n",
    "ax2.set_xlabel('Number of Words')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(np.mean(text_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(text_lengths):.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dataset Processing and Transformation\n",
    "\n",
    "The datasets library provides powerful tools for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset operations\n",
    "print(\"Basic Dataset Operations:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Select a subset\n",
    "small_dataset = train_dataset.select(range(1000))\n",
    "print(f\"Selected subset size: {len(small_dataset)}\")\n",
    "\n",
    "# Filter examples\n",
    "positive_reviews = train_dataset.filter(lambda example: example['label'] == 1)\n",
    "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
    "\n",
    "# Sort examples\n",
    "def text_length(example):\n",
    "    return len(example['text'].split())\n",
    "\n",
    "sorted_by_length = small_dataset.sort('text', key=text_length)\n",
    "shortest = sorted_by_length[0]\n",
    "longest = sorted_by_length[-1]\n",
    "\n",
    "print(f\"\\nShortest review ({len(shortest['text'].split())} words): {shortest['text'][:100]}...\")\n",
    "print(f\"Longest review ({len(longest['text'].split())} words): {longest['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced transformations with map()\n",
    "def add_text_stats(examples):\n",
    "    \"\"\"Add text statistics to examples\"\"\"\n",
    "    results = {\n",
    "        'word_count': [],\n",
    "        'char_count': [],\n",
    "        'avg_word_length': []\n",
    "    }\n",
    "    \n",
    "    for text in examples['text']:\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        char_count = len(text)\n",
    "        avg_word_len = np.mean([len(word) for word in words]) if words else 0\n",
    "        \n",
    "        results['word_count'].append(word_count)\n",
    "        results['char_count'].append(char_count)\n",
    "        results['avg_word_length'].append(avg_word_len)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply transformation\n",
    "print(\"Adding text statistics...\")\n",
    "enhanced_dataset = small_dataset.map(\n",
    "    add_text_stats,\n",
    "    batched=True,\n",
    "    desc=\"Adding text statistics\"\n",
    ")\n",
    "\n",
    "print(f\"New features: {enhanced_dataset.features}\")\n",
    "\n",
    "# Show enhanced examples\n",
    "example = enhanced_dataset[0]\n",
    "print(f\"\\nExample with statistics:\")\n",
    "print(f\"  Text: {example['text'][:100]}...\")\n",
    "print(f\"  Label: {example['label']}\")\n",
    "print(f\"  Word count: {example['word_count']}\")\n",
    "print(f\"  Character count: {example['char_count']}\")\n",
    "print(f\"  Average word length: {example['avg_word_length']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tokenization Integration\n",
    "\n",
    "The datasets library integrates seamlessly with tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,  # We'll pad later in batches\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = small_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing\",\n",
    "    remove_columns=['text']  # Remove original text to save memory\n",
    ")\n",
    "\n",
    "print(f\"Tokenized features: {tokenized_dataset.features}\")\n",
    "\n",
    "# Examine tokenized example\n",
    "tokenized_example = tokenized_dataset[0]\n",
    "print(f\"\\nTokenized example:\")\n",
    "print(f\"  Input IDs length: {len(tokenized_example['input_ids'])}\")\n",
    "print(f\"  Input IDs: {tokenized_example['input_ids'][:20]}...\")\n",
    "print(f\"  Attention mask: {tokenized_example['attention_mask'][:20]}...\")\n",
    "print(f\"  Label: {tokenized_example['label']}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded_text = tokenizer.decode(tokenized_example['input_ids'], skip_special_tokens=True)\n",
    "print(f\"  Decoded text: {decoded_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token length distribution\n",
    "token_lengths = [len(example['input_ids']) for example in tokenized_dataset]\n",
    "\n",
    "print(\"Token Length Analysis:\")\n",
    "print(f\"  Mean: {np.mean(token_lengths):.1f} tokens\")\n",
    "print(f\"  Median: {np.median(token_lengths):.1f} tokens\")\n",
    "print(f\"  Min: {min(token_lengths)} tokens\")\n",
    "print(f\"  Max: {max(token_lengths)} tokens\")\n",
    "print(f\"  95th percentile: {np.percentile(token_lengths, 95):.1f} tokens\")\n",
    "\n",
    "# Visualize token length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(token_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(token_lengths):.1f}')\n",
    "plt.axvline(512, color='orange', linestyle='--', label='Max length: 512')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Cumulative distribution\n",
    "sorted_lengths = np.sort(token_lengths)\n",
    "cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
    "plt.plot(sorted_lengths, cumulative)\n",
    "plt.axvline(np.percentile(token_lengths, 95), color='red', linestyle='--', \n",
    "           label=f'95th percentile: {np.percentile(token_lengths, 95):.0f}')\n",
    "plt.title('Cumulative Distribution of Token Lengths')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Cumulative Proportion')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show examples of different lengths\n",
    "short_examples = [i for i, length in enumerate(token_lengths) if length < 50]\n",
    "long_examples = [i for i, length in enumerate(token_lengths) if length > 400]\n",
    "\n",
    "print(f\"\\nExamples:\")\n",
    "print(f\"Short examples (< 50 tokens): {len(short_examples)}\")\n",
    "print(f\"Long examples (> 400 tokens): {len(long_examples)}\")\n",
    "\n",
    "if short_examples:\n",
    "    idx = short_examples[0]\n",
    "    print(f\"\\nShort example ({token_lengths[idx]} tokens):\")\n",
    "    print(tokenizer.decode(tokenized_dataset[idx]['input_ids'], skip_special_tokens=True))\n",
    "\n",
    "if long_examples:\n",
    "    idx = long_examples[0]\n",
    "    print(f\"\\nLong example ({token_lengths[idx]} tokens):\")\n",
    "    print(tokenizer.decode(tokenized_dataset[idx]['input_ids'], skip_special_tokens=True)[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Working with Large Datasets - Streaming\n",
    "\n",
    "For datasets too large to fit in memory, use streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in streaming mode\n",
    "print(\"Loading large dataset in streaming mode...\")\n",
    "\n",
    "try:\n",
    "    # Load a large dataset that would be impractical to load entirely\n",
    "    streaming_dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", streaming=True, split=\"train\")\n",
    "    \n",
    "    print(f\"Streaming dataset type: {type(streaming_dataset)}\")\n",
    "    \n",
    "    # Process first few examples\n",
    "    print(\"\\nProcessing first few examples from stream:\")\n",
    "    for i, example in enumerate(streaming_dataset):\n",
    "        if i >= 3:  # Only show first 3 examples\n",
    "            break\n",
    "        print(f\"Example {i+1}: {example['text'][:100]}...\")\n",
    "    \n",
    "    # Demonstrate streaming with transformations\n",
    "    def clean_and_tokenize(example):\n",
    "        # Clean the text\n",
    "        text = example['text'].strip()\n",
    "        \n",
    "        # Skip very short or very long texts\n",
    "        if len(text) < 50 or len(text) > 1000:\n",
    "            return None\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(text, truncation=True, max_length=256)\n",
    "        return tokenized\n",
    "    \n",
    "    # Apply transformation to streaming dataset\n",
    "    processed_stream = streaming_dataset.map(clean_and_tokenize)\n",
    "    \n",
    "    # Filter out None values\n",
    "    processed_stream = processed_stream.filter(lambda x: x is not None)\n",
    "    \n",
    "    print(\"\\nProcessed streaming examples:\")\n",
    "    count = 0\n",
    "    for example in processed_stream:\n",
    "        if count >= 2:  # Show 2 processed examples\n",
    "            break\n",
    "        if example is not None:\n",
    "            print(f\"Processed example {count+1}: {len(example.get('input_ids', []))} tokens\")\n",
    "            count += 1\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Streaming example failed (might need internet/permissions): {e}\")\n",
    "    print(\"This is normal in some environments - streaming works in practice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Best Practices Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate streaming best practices with a smaller dataset\n",
    "def streaming_processing_demo():\n",
    "    \"\"\"Demo of efficient streaming processing\"\"\"\n",
    "    \n",
    "    # Load IMDB in streaming mode for demo\n",
    "    streaming_imdb = load_dataset(\"imdb\", streaming=True, split=\"train\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    def process_batch(examples, batch_size=32):\n",
    "        \"\"\"Process examples in batches for efficiency\"\"\"\n",
    "        batch = []\n",
    "        \n",
    "        for example in examples:\n",
    "            batch.append(example)\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                # Process batch\n",
    "                texts = [ex['text'] for ex in batch]\n",
    "                tokenized = tokenizer(texts, truncation=True, padding=True, max_length=256)\n",
    "                \n",
    "                # Yield processed batch\n",
    "                for i, ex in enumerate(batch):\n",
    "                    yield {\n",
    "                        'input_ids': tokenized['input_ids'][i],\n",
    "                        'attention_mask': tokenized['attention_mask'][i],\n",
    "                        'label': ex['label']\n",
    "                    }\n",
    "                \n",
    "                batch = []\n",
    "        \n",
    "        # Process remaining examples\n",
    "        if batch:\n",
    "            texts = [ex['text'] for ex in batch]\n",
    "            tokenized = tokenizer(texts, truncation=True, padding=True, max_length=256)\n",
    "            \n",
    "            for i, ex in enumerate(batch):\n",
    "                yield {\n",
    "                    'input_ids': tokenized['input_ids'][i],\n",
    "                    'attention_mask': tokenized['attention_mask'][i],\n",
    "                    'label': ex['label']\n",
    "                }\n",
    "    \n",
    "    print(\"Streaming processing demo:\")\n",
    "    print(\"Processing first 100 examples in batches...\")\n",
    "    \n",
    "    # Take first 100 examples and process them\n",
    "    limited_stream = streaming_imdb.take(100)\n",
    "    processed_count = 0\n",
    "    \n",
    "    for processed_example in process_batch(limited_stream, batch_size=16):\n",
    "        processed_count += 1\n",
    "        if processed_count <= 3:  # Show first 3\n",
    "            print(f\"  Example {processed_count}: {len(processed_example['input_ids'])} tokens, label: {processed_example['label']}\")\n",
    "    \n",
    "    print(f\"Total processed: {processed_count} examples\")\n",
    "\n",
    "streaming_processing_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating Custom Datasets\n",
    "\n",
    "You can create datasets from your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from Python data\n",
    "print(\"Creating custom dataset from Python data:\")\n",
    "\n",
    "# Sample data\n",
    "custom_data = {\n",
    "    'text': [\n",
    "        \"I love this product! It's amazing.\",\n",
    "        \"This is terrible, worst purchase ever.\",\n",
    "        \"Pretty good, would recommend to others.\",\n",
    "        \"Not bad, but could be better.\",\n",
    "        \"Absolutely fantastic! Five stars!\",\n",
    "        \"Waste of money, very disappointed.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 0],  # 1: positive, 0: negative\n",
    "    'rating': [5, 1, 4, 2, 5, 1]\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "custom_dataset = Dataset.from_dict(custom_data)\n",
    "\n",
    "print(f\"Custom dataset size: {len(custom_dataset)}\")\n",
    "print(f\"Features: {custom_dataset.features}\")\n",
    "\n",
    "# Show examples\n",
    "for i, example in enumerate(custom_dataset):\n",
    "    print(f\"Example {i+1}: '{example['text']}' -> Label: {example['label']}, Rating: {example['rating']}\")\n",
    "\n",
    "# Split into train/test\n",
    "split_dataset = custom_dataset.train_test_split(test_size=0.33, seed=42)\n",
    "print(f\"\\nAfter split:\")\n",
    "print(f\"  Train size: {len(split_dataset['train'])}\")\n",
    "print(f\"  Test size: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from pandas DataFrame\n",
    "print(\"\\nCreating dataset from pandas DataFrame:\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_data = pd.DataFrame({\n",
    "    'review': [\n",
    "        \"Great product, highly recommended!\",\n",
    "        \"Poor quality, broke after one day.\",\n",
    "        \"Excellent value for money.\",\n",
    "        \"Not worth the price.\",\n",
    "        \"Amazing quality and fast shipping!\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive'],\n",
    "    'score': [0.9, 0.1, 0.8, 0.2, 0.95]\n",
    "})\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df_data)\n",
    "\n",
    "# Convert to dataset\n",
    "df_dataset = Dataset.from_pandas(df_data)\n",
    "\n",
    "print(f\"\\nDataset from DataFrame:\")\n",
    "print(f\"  Size: {len(df_dataset)}\")\n",
    "print(f\"  Features: {df_dataset.features}\")\n",
    "print(f\"  First example: {df_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from CSV/JSON files (demo with temporary files)\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Creating datasets from files:\")\n",
    "\n",
    "# Create temporary CSV file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "    csv_content = \"\"\"text,label\n",
    "\"This is a positive example\",1\n",
    "\"This is a negative example\",0\n",
    "\"Another positive case\",1\n",
    "\"Another negative case\",0\"\"\"\n",
    "    f.write(csv_content)\n",
    "    csv_file = f.name\n",
    "\n",
    "# Load from CSV\n",
    "csv_dataset = load_dataset('csv', data_files=csv_file)\n",
    "print(f\"CSV dataset: {len(csv_dataset['train'])} examples\")\n",
    "print(f\"Features: {csv_dataset['train'].features}\")\n",
    "\n",
    "# Create temporary JSON file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "    json_data = [\n",
    "        {\"text\": \"JSON example 1\", \"label\": \"positive\"},\n",
    "        {\"text\": \"JSON example 2\", \"label\": \"negative\"},\n",
    "        {\"text\": \"JSON example 3\", \"label\": \"positive\"}\n",
    "    ]\n",
    "    for item in json_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "    json_file = f.name\n",
    "\n",
    "# Load from JSON\n",
    "json_dataset = load_dataset('json', data_files=json_file)\n",
    "print(f\"JSON dataset: {len(json_dataset['train'])} examples\")\n",
    "print(f\"Features: {json_dataset['train'].features}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.unlink(csv_file)\n",
    "os.unlink(json_file)\n",
    "\n",
    "print(\"\\nExample from JSON dataset:\")\n",
    "print(json_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Dataset Caching and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate caching and saving\n",
    "import tempfile\n",
    "\n",
    "print(\"Dataset caching and saving:\")\n",
    "\n",
    "# Save processed dataset to disk\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    save_path = os.path.join(temp_dir, \"processed_imdb\")\n",
    "    \n",
    "    # Save tokenized dataset\n",
    "    print(f\"Saving dataset to {save_path}...\")\n",
    "    tokenized_dataset.save_to_disk(save_path)\n",
    "    \n",
    "    # Check saved files\n",
    "    saved_files = os.listdir(save_path)\n",
    "    print(f\"Saved files: {saved_files}\")\n",
    "    \n",
    "    # Load from disk\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    loaded_dataset = load_from_disk(save_path)\n",
    "    \n",
    "    print(f\"Loaded dataset size: {len(loaded_dataset)}\")\n",
    "    print(f\"Features match: {loaded_dataset.features == tokenized_dataset.features}\")\n",
    "    print(f\"First example matches: {loaded_dataset[0] == tokenized_dataset[0]}\")\n",
    "\n",
    "# Demonstrate caching behavior\n",
    "print(\"\\nCaching behavior:\")\n",
    "\n",
    "def expensive_operation(example):\n",
    "    \"\"\"Simulate an expensive operation\"\"\"\n",
    "    import time\n",
    "    time.sleep(0.01)  # Simulate processing time\n",
    "    return {'processed_text': example['text'].upper()}\n",
    "\n",
    "# First run - will be slow\n",
    "start_time = time.time()\n",
    "processed_once = small_dataset.map(expensive_operation, cache_file_name=\"expensive_cache.arrow\")\n",
    "first_run_time = time.time() - start_time\n",
    "\n",
    "# Second run - should use cache and be fast\n",
    "start_time = time.time()\n",
    "processed_twice = small_dataset.map(expensive_operation, cache_file_name=\"expensive_cache.arrow\")\n",
    "second_run_time = time.time() - start_time\n",
    "\n",
    "print(f\"First run time: {first_run_time:.3f} seconds\")\n",
    "print(f\"Second run time: {second_run_time:.3f} seconds\")\n",
    "print(f\"Speedup from caching: {first_run_time/second_run_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating datasets\n",
    "print(\"Advanced dataset operations:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create two small datasets\n",
    "dataset1 = Dataset.from_dict({\n",
    "    'text': ['Example 1', 'Example 2'],\n",
    "    'label': [1, 0]\n",
    "})\n",
    "\n",
    "dataset2 = Dataset.from_dict({\n",
    "    'text': ['Example 3', 'Example 4'],\n",
    "    'label': [0, 1]\n",
    "})\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2])\n",
    "print(f\"Combined dataset size: {len(combined_dataset)}\")\n",
    "print(f\"Combined examples: {[ex['text'] for ex in combined_dataset]}\")\n",
    "\n",
    "# Interleaving datasets\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "interleaved = interleave_datasets([dataset1, dataset2])\n",
    "print(f\"Interleaved examples: {[ex['text'] for ex in interleaved]}\")\n",
    "\n",
    "# Dataset dictionary operations\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset1,\n",
    "    'test': dataset2\n",
    "})\n",
    "\n",
    "print(f\"\\nDatasetDict splits: {list(dataset_dict.keys())}\")\n",
    "print(f\"Train size: {len(dataset_dict['train'])}\")\n",
    "print(f\"Test size: {len(dataset_dict['test'])}\")\n",
    "\n",
    "# Apply operations to all splits\n",
    "def add_length(example):\n",
    "    return {'text_length': len(example['text'])}\n",
    "\n",
    "dataset_dict_with_length = dataset_dict.map(add_length)\n",
    "print(f\"After adding length feature:\")\n",
    "print(f\"Train features: {dataset_dict_with_length['train'].features}\")\n",
    "print(f\"Example: {dataset_dict_with_length['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Dataset Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "import time\n",
    "\n",
    "def performance_tips_demo():\n",
    "    \"\"\"Demonstrate performance optimization techniques\"\"\"\n",
    "    \n",
    "    print(\"Performance optimization tips:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a larger sample for meaningful timing\n",
    "    large_sample = train_dataset.select(range(5000))\n",
    "    \n",
    "    def simple_transform(example):\n",
    "        return {'word_count': len(example['text'].split())}\n",
    "    \n",
    "    def batch_transform(examples):\n",
    "        return {'word_count': [len(text.split()) for text in examples['text']]}\n",
    "    \n",
    "    # Single example processing\n",
    "    print(\"1. Single vs Batch processing:\")\n",
    "    start_time = time.time()\n",
    "    single_result = large_sample.map(simple_transform)\n",
    "    single_time = time.time() - start_time\n",
    "    \n",
    "    # Batch processing\n",
    "    start_time = time.time()\n",
    "    batch_result = large_sample.map(batch_transform, batched=True, batch_size=1000)\n",
    "    batch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Single processing: {single_time:.3f} seconds\")\n",
    "    print(f\"  Batch processing: {batch_time:.3f} seconds\")\n",
    "    print(f\"  Speedup: {single_time/batch_time:.1f}x\")\n",
    "    \n",
    "    # Multiprocessing\n",
    "    print(\"\\n2. Multiprocessing:\")\n",
    "    start_time = time.time()\n",
    "    mp_result = large_sample.map(\n",
    "        batch_transform, \n",
    "        batched=True, \n",
    "        batch_size=1000,\n",
    "        num_proc=2  # Use 2 processes\n",
    "    )\n",
    "    mp_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Multiprocessing time: {mp_time:.3f} seconds\")\n",
    "    print(f\"  Speedup vs single: {single_time/mp_time:.1f}x\")\n",
    "    \n",
    "    # Memory usage tips\n",
    "    print(\"\\n3. Memory optimization:\")\n",
    "    \n",
    "    # Remove unused columns\n",
    "    memory_optimized = large_sample.map(\n",
    "        lambda x: tokenizer(x['text'], truncation=True, max_length=128),\n",
    "        batched=True,\n",
    "        remove_columns=['text'],  # Remove original text to save memory\n",
    "        desc=\"Tokenizing and removing text\"\n",
    "    )\n",
    "    \n",
    "    original_features = large_sample.features\n",
    "    optimized_features = memory_optimized.features\n",
    "    \n",
    "    print(f\"  Original features: {list(original_features.keys())}\")\n",
    "    print(f\"  Optimized features: {list(optimized_features.keys())}\")\n",
    "    print(f\"  Memory saved by removing text column\")\n",
    "\n",
    "performance_tips_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Real-world Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline for preparing data for training\n",
    "def create_training_pipeline(dataset_name, model_name, max_length=128, test_size=0.2):\n",
    "    \"\"\"Complete pipeline for dataset preparation\"\"\"\n",
    "    \n",
    "    print(f\"Creating training pipeline for {dataset_name} with {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load dataset\n",
    "    print(\"Step 1: Loading dataset...\")\n",
    "    if dataset_name == \"imdb\":\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        text_column = \"text\"\n",
    "        label_column = \"label\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "    \n",
    "    print(f\"  Loaded {len(dataset['train'])} training examples\")\n",
    "    \n",
    "    # Step 2: Load tokenizer\n",
    "    print(\"Step 2: Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Step 3: Analyze data\n",
    "    print(\"Step 3: Analyzing data...\")\n",
    "    sample_texts = [ex[text_column] for ex in dataset['train'].select(range(1000))]\n",
    "    token_lengths = [len(tokenizer.encode(text)) for text in sample_texts]\n",
    "    \n",
    "    print(f\"  Token length statistics:\")\n",
    "    print(f\"    Mean: {np.mean(token_lengths):.1f}\")\n",
    "    print(f\"    95th percentile: {np.percentile(token_lengths, 95):.1f}\")\n",
    "    print(f\"    Max length setting: {max_length}\")\n",
    "    \n",
    "    # Step 4: Create preprocessing function\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[text_column],\n",
    "            truncation=True,\n",
    "            padding=False,  # Dynamic padding during training\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    # Step 5: Apply preprocessing\n",
    "    print(\"Step 4: Preprocessing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=2,\n",
    "        remove_columns=[text_column],  # Remove text to save memory\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    # Step 6: Prepare for training\n",
    "    print(\"Step 5: Preparing for training...\")\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(label_column, \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Step 7: Split if needed\n",
    "    if \"validation\" not in tokenized_dataset:\n",
    "        print(f\"Step 6: Creating validation split ({test_size:.1%})...\")\n",
    "        train_val = tokenized_dataset['train'].train_test_split(\n",
    "            test_size=test_size, \n",
    "            seed=42\n",
    "        )\n",
    "        tokenized_dataset['train'] = train_val['train']\n",
    "        tokenized_dataset['validation'] = train_val['test']\n",
    "    \n",
    "    print(f\"\\nFinal dataset:\")\n",
    "    for split, data in tokenized_dataset.items():\n",
    "        print(f\"  {split}: {len(data)} examples\")\n",
    "    \n",
    "    print(f\"  Features: {tokenized_dataset['train'].features}\")\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Create a complete pipeline\n",
    "prepared_dataset = create_training_pipeline(\n",
    "    dataset_name=\"imdb\",\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    max_length=256,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample from prepared dataset:\")\n",
    "example = prepared_dataset['train'][0]\n",
    "for key, value in example.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"  {key}: tensor of shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered the comprehensive usage of the Hugging Face datasets library:\n",
    "\n",
    "1. **Loading Datasets**: From the Hub, local files, and custom data\n",
    "2. **Dataset Exploration**: Understanding structure, features, and distributions\n",
    "3. **Data Processing**: Transformations, filtering, and mapping operations\n",
    "4. **Tokenization Integration**: Seamless integration with transformers tokenizers\n",
    "5. **Streaming**: Handling large datasets that don't fit in memory\n",
    "6. **Custom Datasets**: Creating datasets from various data sources\n",
    "7. **Caching and Saving**: Efficient storage and retrieval of processed data\n",
    "8. **Advanced Operations**: Concatenation, interleaving, and dataset dictionaries\n",
    "9. **Performance Optimization**: Batching, multiprocessing, and memory management\n",
    "10. **Production Pipeline**: Complete workflow from raw data to training-ready datasets\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **The datasets library is memory-efficient** thanks to Apache Arrow backend\n",
    "- **Streaming is essential** for large datasets that don't fit in memory\n",
    "- **Batch processing** is much faster than individual example processing\n",
    "- **Caching prevents recomputation** of expensive operations\n",
    "- **Integration with tokenizers** makes preprocessing seamless\n",
    "- **Performance optimization** through multiprocessing and smart batching\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 04**: Mini-project combining concepts from notebooks 01-03\n",
    "- **Notebook 05**: Fine-tuning models with the Trainer API\n",
    "\n",
    "Understanding the datasets library is crucial for efficient NLP workflows. The concepts learned here will be essential for handling real-world data processing challenges!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}