{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.5/multiple-sequences.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.5/multiple-sequences.ipynb)\n",
    "\n",
    "# Handling Multiple Sequences: Padding, Attention Masks, and Long Context\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to handle sequences of different lengths in batch processing\n",
    "- The importance and mechanics of padding and attention masks\n",
    "- Strategies for dealing with longer sequences and truncation\n",
    "- Advanced techniques with Longformer for extended context understanding\n",
    "- Visual analysis of attention patterns across different sequence lengths\n",
    "\n",
    "## 📋 Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of tokenization concepts (refer to `02_tokenizers.ipynb`)\n",
    "\n",
    "## 📚 What We'll Cover\n",
    "1. Section 1: Understanding the Multiple Sequences Problem\n",
    "2. Section 2: Padding and Truncation Strategies\n",
    "3. Section 3: Attention Masks Deep Dive\n",
    "4. Section 4: Handling Longer Sequences\n",
    "5. Section 5: Longformer for Extended Context\n",
    "6. Section 6: Visualizing Attention Patterns\n",
    "7. Section 7: Best Practices and Performance Optimization\n",
    "8. Section 8: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for this comprehensive tutorial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    LongformerTokenizer, LongformerModel, LongformerForSequenceClassification,\n",
    "    AutoConfig, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "import os\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env.local loading\")\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    COLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "\n",
    "def get_api_key(key_name: str, required: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load API key from environment or Google Colab secrets.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Environment variable name\n",
    "        required: Whether to raise error if not found\n",
    "        \n",
    "    Returns:\n",
    "        API key string or None\n",
    "    \"\"\"\n",
    "    # Try Colab secrets first\n",
    "    if COLAB_AVAILABLE:\n",
    "        try:\n",
    "            return userdata.get(key_name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    api_key = os.getenv(key_name)\n",
    "    \n",
    "    if required and not api_key:\n",
    "        raise ValueError(\n",
    "            f\"{key_name} not found. Set it in:\\n\"\n",
    "            f\"- Local: .env.local file\\n\"\n",
    "            f\"- Colab: Secrets manager\"\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"🚀 Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"🍎 Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"💻 Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup authentication and device\n",
    "hf_token = get_api_key('HF_TOKEN', required=False)\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"✅ Hugging Face token configured\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Set up plotting style for educational visualizations\n",
    "plt.style.use('default')  # Use default style for better compatibility\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"\\n=== Setup Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Ready for multiple sequence processing! 🎯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding the Multiple Sequences Problem\n",
    "\n",
    "When working with real-world NLP tasks, we rarely process single sequences. Instead, we need to handle batches of text sequences that vary significantly in length. This creates several challenges:\n",
    "\n",
    "### Key Challenges:\n",
    "- **Variable Length**: Text sequences have different numbers of tokens\n",
    "- **Batch Processing**: Neural networks require fixed-size inputs for efficient computation\n",
    "- **Memory Efficiency**: Longer sequences consume more computational resources\n",
    "- **Attention Mechanics**: Models need to know which parts of input to focus on\n",
    "\n",
    "Let's start by demonstrating this problem with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sequence_length_problem():\n",
    "    \"\"\"\n",
    "    Demonstrate the variable sequence length problem with real text examples.\n",
    "    \"\"\"\n",
    "    print(\"🔍 DEMONSTRATING THE MULTIPLE SEQUENCES PROBLEM\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Example texts with varying lengths - using hate speech detection focus\n",
    "    example_texts = [\n",
    "        \"Great post!\",  # Short: 3 words\n",
    "        \"I really enjoyed reading this article about machine learning.\",  # Medium: 10 words\n",
    "        \"This comprehensive tutorial on natural language processing and transformer models provides detailed insights into modern NLP techniques and their practical applications in industry.\",  # Long: 25 words\n",
    "        \"AI\",  # Very short: 1 word\n",
    "        \"The development of large language models has revolutionized how we approach various NLP tasks, from text classification to question answering, enabling more sophisticated and nuanced understanding of human language patterns.\"  # Very long: 32 words\n",
    "    ]\n",
    "    \n",
    "    # Load a BERT tokenizer for demonstration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    print(\"📊 Text Length Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    tokenized_lengths = []\n",
    "    for i, text in enumerate(example_texts):\n",
    "        # Tokenize without any padding or truncation\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        word_count = len(text.split())\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        tokenized_lengths.append(token_count)\n",
    "        \n",
    "        print(f\"{i+1}. Words: {word_count:2d} | Tokens: {token_count:2d} | Text: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "    \n",
    "    print(f\"\\n📈 Token Length Statistics:\")\n",
    "    print(f\"   Min length: {min(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Max length: {max(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Range: {max(tokenized_lengths) - min(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Average: {np.mean(tokenized_lengths):.1f} tokens\")\n",
    "    \n",
    "    # Visualize the length distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(range(1, len(tokenized_lengths) + 1), tokenized_lengths, color='skyblue', alpha=0.8)\n",
    "    plt.title('Token Count per Sequence')\n",
    "    plt.xlabel('Sequence Number')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, length in zip(bars, tokenized_lengths):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                str(length), ha='center', va='bottom')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Create a visual representation of sequence lengths\n",
    "    sequences_visual = np.zeros((len(tokenized_lengths), max(tokenized_lengths)))\n",
    "    for i, length in enumerate(tokenized_lengths):\n",
    "        sequences_visual[i, :length] = 1\n",
    "    \n",
    "    plt.imshow(sequences_visual, cmap='RdYlBu_r', aspect='auto')\n",
    "    plt.title('Sequence Length Visualization\\n(Blue = tokens, White = padding needed)')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Sequence Number')\n",
    "    plt.yticks(range(len(tokenized_lengths)), [f'Seq {i+1}' for i in range(len(tokenized_lengths))])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🚨 THE PROBLEM:\")\n",
    "    print(\"   • Neural networks need fixed-size inputs for batch processing\")\n",
    "    print(\"   • Variable sequence lengths prevent efficient batching\")\n",
    "    print(\"   • Need padding to make all sequences the same length\")\n",
    "    print(\"   • Need attention masks to ignore padded positions\")\n",
    "    \n",
    "    return example_texts, tokenized_lengths\n",
    "\n",
    "# Run the demonstration\n",
    "example_texts, tokenized_lengths = demonstrate_sequence_length_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Padding and Truncation Strategies\n",
    "\n",
    "To solve the variable sequence length problem, we use **padding** and **truncation**:\n",
    "\n",
    "### Padding Strategies:\n",
    "- **Right Padding**: Add padding tokens to the end of sequences (most common)\n",
    "- **Left Padding**: Add padding tokens to the beginning (used for some generative models)\n",
    "- **Dynamic Padding**: Pad only to the length of the longest sequence in the batch\n",
    "- **Fixed Padding**: Pad all sequences to a predetermined maximum length\n",
    "\n",
    "### Truncation Strategies:\n",
    "- **Right Truncation**: Remove tokens from the end\n",
    "- **Left Truncation**: Remove tokens from the beginning  \n",
    "- **Middle Truncation**: Remove tokens from the middle (less common)\n",
    "\n",
    "Let's implement and compare these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_padding_strategies():\n",
    "    \"\"\"\n",
    "    Demonstrate different padding and truncation strategies.\n",
    "    \"\"\"\n",
    "    print(\"🛠️ PADDING AND TRUNCATION STRATEGIES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Load preferred hate speech detection model tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "    \n",
    "    # Use example texts from hate speech detection domain\n",
    "    texts = [\n",
    "        \"This message promotes understanding and respect.\",\n",
    "        \"Great work!\",\n",
    "        \"I completely disagree with this perspective, but I respect your right to express it and welcome civil discourse on the topic.\",\n",
    "        \"Thanks\"\n",
    "    ]\n",
    "    \n",
    "    print(\"📝 Original texts:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"{i+1}. '{text}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Strategy 1: No padding/truncation (shows the problem)\n",
    "    print(\"\\n1️⃣ NO PADDING/TRUNCATION (The Problem):\")\n",
    "    try:\n",
    "        unpadded = tokenizer(texts, return_tensors=\"pt\")\n",
    "        print(\"   ✅ Successful batching\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {str(e)[:100]}...\")\n",
    "        print(\"   This is why we need padding!\")\n",
    "    \n",
    "    # Strategy 2: Right padding (most common)\n",
    "    print(\"\\n2️⃣ RIGHT PADDING:\")\n",
    "    right_padded = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {right_padded['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {right_padded['attention_mask'].shape}\")\n",
    "    \n",
    "    # Strategy 3: Fixed max length padding  \n",
    "    print(\"\\n3️⃣ FIXED LENGTH PADDING (max_length=20):\")\n",
    "    fixed_padded = tokenizer(texts, padding='max_length', max_length=20, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {fixed_padded['input_ids'].shape}\")\n",
    "    \n",
    "    # Strategy 4: Truncation with padding\n",
    "    print(\"\\n4️⃣ TRUNCATION + PADDING (max_length=15):\")\n",
    "    truncated_padded = tokenizer(texts, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {truncated_padded['input_ids'].shape}\")\n",
    "    \n",
    "    # Visualize the different strategies\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Padding and Truncation Strategies Visualization', fontsize=16)\n",
    "    \n",
    "    strategies = [\n",
    "        (\"Right Padding\", right_padded),\n",
    "        (\"Fixed Length (20)\", fixed_padded),\n",
    "        (\"Truncated + Padded (15)\", truncated_padded),\n",
    "        (\"Attention Masks\", right_padded)  # Show attention masks\n",
    "    ]\n",
    "    \n",
    "    for idx, (title, encoded) in enumerate(strategies):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        if title == \"Attention Masks\":\n",
    "            # Show attention masks\n",
    "            data = encoded['attention_mask'].numpy()\n",
    "            cmap = 'RdBu_r'\n",
    "        else:\n",
    "            # Show input IDs (replace pad tokens with -1 for visualization)\n",
    "            data = encoded['input_ids'].numpy()\n",
    "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            data = np.where(data == pad_token_id, -1, 1)  # 1 for real tokens, -1 for padding\n",
    "            cmap = 'RdYlBu_r'\n",
    "        \n",
    "        im = ax.imshow(data, cmap=cmap, aspect='auto')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Token Position')\n",
    "        ax.set_ylabel('Sequence Number')\n",
    "        ax.set_yticks(range(len(texts)))\n",
    "        ax.set_yticklabels([f'Text {i+1}' for i in range(len(texts))])\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show detailed token analysis for first text\n",
    "    print(\"\\n🔍 DETAILED ANALYSIS - First Text:\")\n",
    "    print(f\"   Original: '{texts[0]}'\")\n",
    "    \n",
    "    # Show tokens for different strategies\n",
    "    strategies_detail = [\n",
    "        (\"Right Padded\", right_padded['input_ids'][0]),\n",
    "        (\"Fixed Length\", fixed_padded['input_ids'][0]),  \n",
    "        (\"Truncated\", truncated_padded['input_ids'][0])\n",
    "    ]\n",
    "    \n",
    "    for name, token_ids in strategies_detail:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        print(f\"   {name:15}: {tokens}\")\n",
    "        print(f\"   {'Length':15}: {len([t for t in tokens if t != tokenizer.pad_token])} real tokens, {len(tokens)} total\")\n",
    "    \n",
    "    return {\n",
    "        'right_padded': right_padded,\n",
    "        'fixed_padded': fixed_padded,\n",
    "        'truncated_padded': truncated_padded\n",
    "    }\n",
    "\n",
    "# Run the demonstration\n",
    "padding_results = demonstrate_padding_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Attention Masks Deep Dive\n",
    "\n",
    "Attention masks are crucial for telling the model which tokens to pay attention to and which to ignore. This is especially important when we have padded sequences.\n",
    "\n",
    "### Understanding Attention Masks:\n",
    "- **1**: Pay attention to this token (real content)\n",
    "- **0**: Ignore this token (padding)\n",
    "- **Purpose**: Prevents the model from learning meaningless patterns from padding tokens\n",
    "- **Effect**: Improves model performance and training stability\n",
    "\n",
    "Let's explore how attention masks work in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_attention_masks():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of attention masks and their importance.\n",
    "    \"\"\"\n",
    "    print(\"🎭 ATTENTION MASKS DEEP DIVE\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Use preferred hate speech detection model\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Example texts with very different lengths\n",
    "    texts = [\n",
    "        \"Love this!\",\n",
    "        \"This is a wonderful example of positive communication.\",\n",
    "        \"AI\"\n",
    "    ]\n",
    "    \n",
    "    print(\"📝 Processing these texts:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"{i+1}. '{text}'\")\n",
    "    \n",
    "    # Tokenize with padding\n",
    "    encoded = tokenizer(texts, padding=True, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    \n",
    "    print(f\"\\n📊 Encoded Results:\")\n",
    "    print(f\"   Batch shape: {encoded['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {encoded['attention_mask'].shape}\")\n",
    "    \n",
    "    # Create detailed visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Attention Masks: Understanding Token Processing', fontsize=16)\n",
    "    \n",
    "    # 1. Input IDs heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    input_ids_viz = encoded['input_ids'].numpy()\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    \n",
    "    # Create visualization data: 1 for real tokens, 0 for padding\n",
    "    viz_data = np.where(input_ids_viz == pad_token_id, 0, 1)\n",
    "    \n",
    "    im1 = ax1.imshow(viz_data, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax1.set_title('Input Tokens\\n(Blue = Real Token, Red = Padding)')\n",
    "    ax1.set_xlabel('Token Position')\n",
    "    ax1.set_ylabel('Sequence')\n",
    "    ax1.set_yticks(range(len(texts)))\n",
    "    ax1.set_yticklabels([f'Text {i+1}' for i in range(len(texts))])\n",
    "    \n",
    "    # 2. Attention masks heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    attention_masks = encoded['attention_mask'].numpy()\n",
    "    im2 = ax2.imshow(attention_masks, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax2.set_title('Attention Masks\\n(Blue = Attend, Red = Ignore)')\n",
    "    ax2.set_xlabel('Token Position')\n",
    "    ax2.set_ylabel('Sequence')\n",
    "    ax2.set_yticks(range(len(texts)))\n",
    "    ax2.set_yticklabels([f'Text {i+1}' for i in range(len(texts))])\n",
    "    \n",
    "    # 3. Token-by-token breakdown for first sequence\n",
    "    ax3 = axes[1, 0]\n",
    "    first_seq_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    first_seq_mask = encoded['attention_mask'][0].numpy()\n",
    "    \n",
    "    colors = ['lightcoral' if mask == 0 else 'lightblue' for mask in first_seq_mask]\n",
    "    bars = ax3.bar(range(len(first_seq_tokens)), first_seq_mask, color=colors)\n",
    "    ax3.set_title(f'First Sequence Token Analysis\\n\\\"{texts[0]}\\\"')\n",
    "    ax3.set_xlabel('Token Position')\n",
    "    ax3.set_ylabel('Attention Value')\n",
    "    ax3.set_xticks(range(len(first_seq_tokens)))\n",
    "    ax3.set_xticklabels(first_seq_tokens, rotation=45, ha='right')\n",
    "    ax3.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # 4. Sequence length comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    real_lengths = [torch.sum(mask).item() for mask in encoded['attention_mask']]\n",
    "    total_length = encoded['input_ids'].shape[1]\n",
    "    \n",
    "    x_pos = range(len(texts))\n",
    "    bars1 = ax4.bar(x_pos, real_lengths, label='Real Tokens', color='lightblue', alpha=0.8)\n",
    "    bars2 = ax4.bar(x_pos, [total_length - length for length in real_lengths], \n",
    "                   bottom=real_lengths, label='Padding Tokens', color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax4.set_title('Token Composition per Sequence')\n",
    "    ax4.set_xlabel('Sequence Number')\n",
    "    ax4.set_ylabel('Number of Tokens')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels([f'Text {i+1}' for i in range(len(texts))])\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (real, total) in enumerate(zip(real_lengths, [total_length] * len(texts))):\n",
    "        ax4.text(i, real/2, str(real), ha='center', va='center', fontweight='bold')\n",
    "        if total - real > 0:\n",
    "            ax4.text(i, real + (total-real)/2, str(total-real), ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed token-by-token analysis\n",
    "    print(\"\\n🔍 TOKEN-BY-TOKEN ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\n📄 Sequence {i+1}: '{text}'\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])\n",
    "        input_ids = encoded['input_ids'][i].tolist()\n",
    "        attention_mask = encoded['attention_mask'][i].tolist()\n",
    "        \n",
    "        print(f\"   {'Position':<8} {'Token':<15} {'ID':<8} {'Attention':<10} {'Status'}\")\n",
    "        print(\"   \" + \"-\" * 55)\n",
    "        \n",
    "        for pos, (token, token_id, attn) in enumerate(zip(tokens, input_ids, attention_mask)):\n",
    "            status = \"ATTEND\" if attn == 1 else \"IGNORE\"\n",
    "            print(f\"   {pos:<8} {token:<15} {token_id:<8} {attn:<10} {status}\")\n",
    "    \n",
    "    print(\"\\n💡 WHY ATTENTION MASKS MATTER:\")\n",
    "    print(\"   ✅ Prevent model from learning patterns in padding tokens\")\n",
    "    print(\"   ✅ Ensure attention weights sum correctly over real tokens\")\n",
    "    print(\"   ✅ Improve training stability and convergence\")\n",
    "    print(\"   ✅ Enable variable-length sequence processing in batches\")\n",
    "    print(\"   ✅ Essential for tasks like hate speech detection where text length varies greatly\")\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Run the demonstration\n",
    "attention_results = demonstrate_attention_masks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Handling Longer Sequences\n",
    "\n",
    "Real-world texts can be very long - social media posts, articles, documents, and conversations. Standard transformer models like BERT have limitations:\n",
    "\n",
    "### Sequence Length Limitations:\n",
    "- **BERT**: Maximum 512 tokens\n",
    "- **RoBERTa**: Maximum 512 tokens  \n",
    "- **Memory**: Quadratic scaling with sequence length\n",
    "- **Computational Cost**: Attention complexity is O(n²)\n",
    "\n",
    "### Strategies for Long Sequences:\n",
    "1. **Truncation**: Cut sequences to fit (may lose important information)\n",
    "2. **Sliding Windows**: Process text in overlapping chunks\n",
    "3. **Hierarchical Processing**: Summarize chunks, then process summaries\n",
    "4. **Specialized Models**: Longformer, BigBird, etc.\n",
    "\n",
    "Let's explore these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_long_sequence_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate strategies for handling sequences longer than model limits.\n",
    "    \"\"\"\n",
    "    print(\"📏 HANDLING LONGER SEQUENCES\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a long text example (simulating a long social media thread or article)\n",
    "    long_text = \"\"\"\n",
    "    The field of artificial intelligence has undergone tremendous growth and transformation over the past decade. \n",
    "    Machine learning algorithms have become increasingly sophisticated, enabling applications that were once \n",
    "    considered science fiction. Natural language processing, in particular, has seen remarkable advances with \n",
    "    the introduction of transformer architectures and large language models. These developments have \n",
    "    revolutionized how we approach text classification, sentiment analysis, and content moderation tasks.\n",
    "    \n",
    "    When dealing with content moderation and hate speech detection, it's crucial to consider context and nuance. \n",
    "    Simple keyword-based approaches often fail to capture the complexity of human communication. Modern NLP \n",
    "    models can better understand context, sarcasm, and subtle forms of harmful content. However, they also \n",
    "    face challenges with longer texts that exceed typical model limitations.\n",
    "    \n",
    "    The challenge of processing long sequences is particularly relevant for analyzing extended conversations, \n",
    "    comment threads, or lengthy posts. Traditional transformer models have fixed input size limitations, \n",
    "    typically 512 tokens for models like BERT and RoBERTa. This constraint requires careful consideration \n",
    "    of how to handle longer content while preserving important contextual information.\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "    \n",
    "    # Analyze the long text\n",
    "    tokens = tokenizer.encode(long_text, add_special_tokens=True)\n",
    "    word_count = len(long_text.split())\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    print(f\"📊 Long Text Analysis:\")\n",
    "    print(f\"   Words: {word_count}\")\n",
    "    print(f\"   Tokens: {token_count}\")\n",
    "    print(f\"   Characters: {len(long_text)}\")\n",
    "    print(f\"   Model limit (typical): 512 tokens\")\n",
    "    print(f\"   Overflow: {max(0, token_count - 512)} tokens\")\n",
    "    \n",
    "    # Strategy 1: Simple truncation\n",
    "    print(\"\\n1️⃣ SIMPLE TRUNCATION STRATEGY:\")\n",
    "    truncated = tokenizer(long_text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    truncated_tokens = tokenizer.convert_ids_to_tokens(truncated['input_ids'][0])\n",
    "    \n",
    "    print(f\"   Kept tokens: {truncated['input_ids'].shape[1]}\")\n",
    "    print(f\"   Lost tokens: {token_count - truncated['input_ids'].shape[1]}\")\n",
    "    print(f\"   Information loss: {((token_count - truncated['input_ids'].shape[1]) / token_count * 100):.1f}%\")\n",
    "    \n",
    "    # Strategy 2: Sliding window approach\n",
    "    print(\"\\n2️⃣ SLIDING WINDOW STRATEGY:\")\n",
    "    window_size = 400  # Leave room for special tokens\n",
    "    stride = 200  # 50% overlap\n",
    "    \n",
    "    windows = []\n",
    "    for start in range(0, len(tokens), stride):\n",
    "        end = min(start + window_size, len(tokens))\n",
    "        window_tokens = tokens[start:end]\n",
    "        windows.append(window_tokens)\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    print(f\"   Number of windows: {len(windows)}\")\n",
    "    print(f\"   Window size: {window_size} tokens\")\n",
    "    print(f\"   Stride: {stride} tokens\")\n",
    "    print(f\"   Overlap: {window_size - stride} tokens\")\n",
    "    \n",
    "    # Visualization of strategies\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('Long Sequence Handling Strategies', fontsize=16)\n",
    "    \n",
    "    # 1. Original sequence length\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.barh(['Original', 'Model Limit'], [token_count, 512], color=['lightcoral', 'lightblue'])\n",
    "    ax1.set_title('Sequence Length vs Model Limit')\n",
    "    ax1.set_xlabel('Number of Tokens')\n",
    "    for i, v in enumerate([token_count, 512]):\n",
    "        ax1.text(v + 10, i, str(v), va='center')\n",
    "    \n",
    "    # 2. Truncation loss\n",
    "    ax2 = axes[0, 1]\n",
    "    kept = 512\n",
    "    lost = token_count - 512\n",
    "    ax2.pie([kept, lost], labels=['Kept', 'Lost'], autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])\n",
    "    ax2.set_title('Information Loss with Truncation')\n",
    "    \n",
    "    # 3. Sliding window coverage\n",
    "    ax3 = axes[1, 0]\n",
    "    window_starts = list(range(0, len(tokens), stride))[:len(windows)]\n",
    "    window_coverage = np.zeros(len(tokens))\n",
    "    \n",
    "    for i, start in enumerate(window_starts):\n",
    "        end = min(start + window_size, len(tokens))\n",
    "        window_coverage[start:end] += 1\n",
    "    \n",
    "    ax3.plot(window_coverage, linewidth=2, color='green')\n",
    "    ax3.fill_between(range(len(window_coverage)), window_coverage, alpha=0.3, color='green')\n",
    "    ax3.set_title('Sliding Window Coverage\\n(Height = Number of Times Token is Processed)')\n",
    "    ax3.set_xlabel('Token Position')\n",
    "    ax3.set_ylabel('Coverage Count')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Strategy comparison table\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Create comparison table\n",
    "    strategies_data = [\n",
    "        [\"Strategy\", \"Pros\", \"Cons\"],\n",
    "        [\"Truncation\", \"Simple, fast\", \"Loses information\"],\n",
    "        [\"Sliding Window\", \"Preserves all info\", \"Redundant processing\"],\n",
    "        [\"Specialized Models\", \"Built for long sequences\", \"Higher memory usage\"]\n",
    "    ]\n",
    "    \n",
    "    table = ax4.table(cellText=strategies_data, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "    ax4.set_title('Strategy Comparison')\n",
    "    \n",
    "    # Color header row\n",
    "    for i in range(3):\n",
    "        table[(0, i)].set_facecolor('#E6E6FA')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📋 KEY TAKEAWAYS:\")\n",
    "    print(\"   📏 Most transformer models have 512-token limits\")\n",
    "    print(\"   ✂️  Truncation is simple but loses information\")\n",
    "    print(\"   🪟 Sliding windows preserve information but increase computation\")\n",
    "    print(\"   🤖 Specialized models like Longformer handle longer sequences efficiently\")\n",
    "    print(\"   ⚖️  Choose strategy based on your specific use case and resources\")\n",
    "    \n",
    "    return {\n",
    "        'original_tokens': token_count,\n",
    "        'windows': windows,\n",
    "        'truncated': truncated\n",
    "    }\n",
    "\n",
    "# Run the demonstration\n",
    "long_sequence_results = demonstrate_long_sequence_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Longformer for Extended Context\n",
    "\n",
    "**Longformer** is a specialized transformer model designed to handle much longer sequences efficiently. It addresses the quadratic memory and compute complexity of standard transformers.\n",
    "\n",
    "### Key Innovations:\n",
    "- **Sparse Attention**: Combines local, global, and sliding window attention\n",
    "- **Extended Length**: Can handle up to 4,096 tokens (8x more than BERT)\n",
    "- **Efficient Memory**: Linear scaling with sequence length\n",
    "- **Task Performance**: Maintains performance on long document tasks\n",
    "\n",
    "Let's explore Longformer concepts and compare with standard models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_longformer_concepts():\n",
    "    \"\"\"\n",
    "    Demonstrate Longformer concepts and compare with standard attention.\n",
    "    \"\"\"\n",
    "    print(\"🔬 LONGFORMER: EXTENDED CONTEXT PROCESSING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create conceptual visualization of attention patterns\n",
    "    seq_length = 64  # Smaller for visualization\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Longformer Attention Patterns vs Standard Attention', fontsize=16)\n",
    "    \n",
    "    # 1. Standard full attention (BERT-style)\n",
    "    ax1 = axes[0, 0]\n",
    "    full_attention = np.ones((seq_length, seq_length))\n",
    "    im1 = ax1.imshow(full_attention, cmap='Blues')\n",
    "    ax1.set_title('Standard Full Attention (BERT)\\nO(n²) complexity')\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # 2. Local sliding window attention\n",
    "    ax2 = axes[0, 1]\n",
    "    window_size = 8\n",
    "    local_attention = np.zeros((seq_length, seq_length))\n",
    "    \n",
    "    for i in range(seq_length):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(seq_length, i + window_size // 2 + 1)\n",
    "        local_attention[i, start:end] = 1\n",
    "    \n",
    "    im2 = ax2.imshow(local_attention, cmap='Greens')\n",
    "    ax2.set_title(f'Local Attention (window={window_size})\\nO(n×w) complexity')\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    # 3. Global + Local attention (Longformer style)\n",
    "    ax3 = axes[1, 0]\n",
    "    global_local_attention = local_attention.copy()\n",
    "    \n",
    "    # Add global attention for first few tokens (like CLS token)\n",
    "    global_tokens = [0, 1, seq_length//4, seq_length//2, seq_length-1]\n",
    "    for token in global_tokens:\n",
    "        if token < seq_length:\n",
    "            global_local_attention[token, :] = 1  # Global tokens attend to all\n",
    "            global_local_attention[:, token] = 1  # All tokens attend to global\n",
    "    \n",
    "    im3 = ax3.imshow(global_local_attention, cmap='Oranges')\n",
    "    ax3.set_title('Longformer: Global + Local Attention\\nEfficient sparse patterns')\n",
    "    ax3.set_xlabel('Key Position')\n",
    "    ax3.set_ylabel('Query Position')\n",
    "    plt.colorbar(im3, ax=ax3)\n",
    "    \n",
    "    # 4. Computational complexity comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate complexity for different sequence lengths\n",
    "    seq_lengths = np.array([128, 256, 512, 1024, 2048, 4096])\n",
    "    full_complexity = seq_lengths ** 2\n",
    "    local_complexity = seq_lengths * window_size\n",
    "    longformer_complexity = seq_lengths * (window_size + len(global_tokens))\n",
    "    \n",
    "    ax4.plot(seq_lengths, full_complexity, 'b-', label='Full Attention O(n²)', linewidth=2)\n",
    "    ax4.plot(seq_lengths, local_complexity, 'g-', label='Local Only O(n×w)', linewidth=2)\n",
    "    ax4.plot(seq_lengths, longformer_complexity, 'orange', label='Longformer O(n×(w+g))', linewidth=2)\n",
    "    \n",
    "    ax4.axvline(x=512, color='blue', linestyle='--', alpha=0.5, label='BERT limit')\n",
    "    ax4.axvline(x=4096, color='orange', linestyle='--', alpha=0.5, label='Longformer limit')\n",
    "    \n",
    "    ax4.set_xlabel('Sequence Length')\n",
    "    ax4.set_ylabel('Computational Operations')\n",
    "    ax4.set_title('Computational Complexity Comparison')\n",
    "    ax4.legend()\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show efficiency gains\n",
    "    print(\"\\n📊 EFFICIENCY COMPARISON:\")\n",
    "    print(f\"   {'Sequence Length':<15} {'Full Attention':<15} {'Longformer':<15} {'Speedup'}\")\n",
    "    print(\"   \" + \"-\" * 65)\n",
    "    \n",
    "    for seq_len in [512, 1024, 2048, 4096]:\n",
    "        full_ops = seq_len ** 2\n",
    "        longformer_ops = seq_len * (window_size + len(global_tokens))\n",
    "        speedup = full_ops / longformer_ops if longformer_ops > 0 else 0\n",
    "        \n",
    "        print(f\"   {seq_len:<15} {full_ops:<15,} {longformer_ops:<15,} {speedup:.1f}x\")\n",
    "    \n",
    "    print(\"\\n💡 LONGFORMER KEY ADVANTAGES:\")\n",
    "    print(\"   ✅ 8x longer sequences than BERT (4096 vs 512 tokens)\")\n",
    "    print(\"   ✅ Sparse attention reduces memory complexity from O(n²) to O(n)\")\n",
    "    print(\"   ✅ Maintains performance on long document tasks\")\n",
    "    print(\"   ✅ Ideal for analyzing long social media threads or articles\")\n",
    "    print(\"   ✅ Better context understanding for hate speech detection in long texts\")\n",
    "    \n",
    "    print(\"\\n🎯 USE CASES FOR LONGFORMER:\")\n",
    "    print(\"   📄 Long document classification and analysis\")\n",
    "    print(\"   💬 Extended conversation thread moderation\")\n",
    "    print(\"   📰 Article sentiment analysis and summarization\")\n",
    "    print(\"   🧵 Social media thread context understanding\")\n",
    "    print(\"   📚 Research paper and academic document processing\")\n",
    "    \n",
    "    # Try to demonstrate with actual model if available\n",
    "    print(\"\\n🔬 TRYING TO LOAD LONGFORMER MODEL:\")\n",
    "    try:\n",
    "        from transformers import LongformerTokenizer, LongformerModel\n",
    "        \n",
    "        print(\"📥 Loading Longformer model...\")\n",
    "        longformer_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "        print(\"✅ Longformer tokenizer loaded successfully!\")\n",
    "        print(f\"   Max position embeddings: 4096\")\n",
    "        print(f\"   Vocab size: {longformer_tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Compare tokenization\n",
    "        sample_text = \"This is a sample text for comparing tokenization approaches.\"\n",
    "        bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        bert_tokens = bert_tokenizer.encode(sample_text)\n",
    "        longformer_tokens = longformer_tokenizer.encode(sample_text)\n",
    "        \n",
    "        print(f\"\\n🔍 Tokenization Comparison:\")\n",
    "        print(f\"   Sample: '{sample_text}'\")\n",
    "        print(f\"   BERT tokens ({len(bert_tokens)}): {bert_tokens}\")\n",
    "        print(f\"   Longformer tokens ({len(longformer_tokens)}): {longformer_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not load Longformer model: {e}\")\n",
    "        print(\"💡 This is expected in some environments. The conceptual demonstration above shows the key ideas.\")\n",
    "    \n",
    "    return {'demonstration': 'completed'}\n",
    "\n",
    "# Run the Longformer demonstration\n",
    "longformer_results = demonstrate_longformer_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Best Practices and Performance Optimization\n",
    "\n",
    "Based on our exploration of multiple sequence handling, let's consolidate the best practices for real-world applications.\n",
    "\n",
    "### Performance Optimization Strategies:\n",
    "- **Dynamic Padding**: Only pad to the maximum length in each batch\n",
    "- **Gradient Accumulation**: Handle larger effective batch sizes with limited memory\n",
    "- **Mixed Precision**: Use FP16 for faster training and inference\n",
    "- **Sequence Length Distribution**: Understand your data to optimize processing\n",
    "\n",
    "Let's implement these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for handling multiple sequences efficiently.\n",
    "    \"\"\"\n",
    "    print(\"🎯 BEST PRACTICES FOR MULTIPLE SEQUENCES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load preferred model for hate speech detection\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Create a realistic dataset with varying lengths\n",
    "    sample_texts = [\n",
    "        \"Thanks!\",\n",
    "        \"This is a great example of positive communication.\",\n",
    "        \"I appreciate the constructive discussion in this thread.\",\n",
    "        \"AI ethics is an important topic that deserves careful consideration.\",\n",
    "        \"Excellent work on this project! Very helpful.\",\n",
    "        \"Good job.\",\n",
    "        \"The development of responsible AI systems requires collaboration.\",\n",
    "        \"Thanks for sharing this resource.\",\n",
    "        \"This analysis provides valuable insights.\",\n",
    "        \"Perfect!\"\n",
    "    ] * 5  # Repeat to create a larger dataset\n",
    "    \n",
    "    print(f\"📊 Dataset Analysis ({len(sample_texts)} texts):\")\n",
    "    \n",
    "    # Analyze length distribution\n",
    "    lengths = [len(tokenizer.encode(text)) for text in sample_texts]\n",
    "    \n",
    "    print(f\"   Token lengths - Min: {min(lengths)}, Max: {max(lengths)}, Mean: {np.mean(lengths):.1f}\")\n",
    "    print(f\"   Std deviation: {np.std(lengths):.1f}\")\n",
    "    \n",
    "    # Strategy comparison\n",
    "    import time\n",
    "    \n",
    "    print(\"\\n🏁 STRATEGY PERFORMANCE COMPARISON:\")\n",
    "    \n",
    "    # Fixed padding (worst case)\n",
    "    start_time = time.time()\n",
    "    fixed_padded = tokenizer(sample_texts, padding='max_length', max_length=128, \n",
    "                            truncation=True, return_tensors=\"pt\")\n",
    "    fixed_time = time.time() - start_time\n",
    "    \n",
    "    # Dynamic padding (optimal)\n",
    "    start_time = time.time()\n",
    "    dynamic_padded = tokenizer(sample_texts, padding=True, truncation=True, \n",
    "                              return_tensors=\"pt\")\n",
    "    dynamic_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Fixed padding (128):   Shape {fixed_padded['input_ids'].shape}, Time: {fixed_time:.3f}s\")\n",
    "    print(f\"   Dynamic padding:       Shape {dynamic_padded['input_ids'].shape}, Time: {dynamic_time:.3f}s\")\n",
    "    \n",
    "    # Calculate memory savings\n",
    "    fixed_memory = fixed_padded['input_ids'].numel() * 4  # 4 bytes per int32\n",
    "    dynamic_memory = dynamic_padded['input_ids'].numel() * 4\n",
    "    memory_savings = (fixed_memory - dynamic_memory) / fixed_memory * 100\n",
    "    \n",
    "    print(f\"   Memory usage - Fixed: {fixed_memory:,} bytes, Dynamic: {dynamic_memory:,} bytes\")\n",
    "    print(f\"   Memory savings: {memory_savings:.1f}%\")\n",
    "    \n",
    "    # Visualize optimization benefits\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Multiple Sequence Processing: Optimization Strategies', fontsize=16)\n",
    "    \n",
    "    # 1. Length distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(lengths, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.1f}')\n",
    "    ax1.axvline(np.median(lengths), color='orange', linestyle='--', label=f'Median: {np.median(lengths):.1f}')\n",
    "    ax1.set_title('Token Length Distribution')\n",
    "    ax1.set_xlabel('Number of Tokens')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Memory usage comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    strategies = ['Fixed\\nPadding', 'Dynamic\\nPadding']\n",
    "    memory_usage = [fixed_memory/1024, dynamic_memory/1024]  # Convert to KB\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    bars = ax2.bar(strategies, memory_usage, color=colors, alpha=0.8)\n",
    "    ax2.set_title('Memory Usage Comparison')\n",
    "    ax2.set_ylabel('Memory Usage (KB)')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, memory_usage):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{value:.1f} KB', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Padding efficiency visualization\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Create visual representation of padding efficiency\n",
    "    sample_batch = sample_texts[:8]\n",
    "    sample_lengths = [len(tokenizer.encode(text)) for text in sample_batch]\n",
    "    max_length = max(sample_lengths)\n",
    "    \n",
    "    # Dynamic padding visualization\n",
    "    dynamic_padding_matrix = np.zeros((len(sample_batch), max_length))\n",
    "    for i, length in enumerate(sample_lengths):\n",
    "        dynamic_padding_matrix[i, :length] = 1\n",
    "    \n",
    "    im3 = ax3.imshow(dynamic_padding_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax3.set_title('Dynamic Padding Efficiency\\n(Blue = Content, Red = Padding)')\n",
    "    ax3.set_xlabel('Token Position')\n",
    "    ax3.set_ylabel('Sequence Number')\n",
    "    ax3.set_yticks(range(len(sample_batch)))\n",
    "    ax3.set_yticklabels([f'Seq {i+1} ({l} tokens)' for i, l in enumerate(sample_lengths)])\n",
    "    \n",
    "    # 4. Performance summary\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create performance metrics comparison\n",
    "    metrics = ['Memory\\nEfficiency', 'Processing\\nSpeed', 'Batch\\nUtilization']\n",
    "    fixed_scores = [60, 70, 40]  # Example scores out of 100\n",
    "    dynamic_scores = [90, 85, 95]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax4.bar(x - width/2, fixed_scores, width, label='Fixed Padding', \n",
    "                   color='lightcoral', alpha=0.8)\n",
    "    bars2 = ax4.bar(x + width/2, dynamic_scores, width, label='Dynamic Padding', \n",
    "                   color='lightgreen', alpha=0.8)\n",
    "    \n",
    "    ax4.set_title('Performance Metrics Comparison')\n",
    "    ax4.set_ylabel('Score (0-100)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(metrics)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                    f'{height}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary of best practices\n",
    "    print(\"\\n🏆 BEST PRACTICES SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    recommendations = [\n",
    "        (\"✅ Dynamic Padding\", \"Always use dynamic padding instead of fixed max_length\"),\n",
    "        (\"✅ Length Grouping\", \"Group similar-length sequences for efficient batching\"),\n",
    "        (\"✅ Attention Masks\", \"Always use attention masks to handle padding properly\"),\n",
    "        (\"✅ Model Choice\", \"Choose Longformer for consistently long sequences\"),\n",
    "        (\"✅ Memory Monitoring\", \"Monitor GPU memory usage and adjust accordingly\"),\n",
    "        (\"✅ Data Profiling\", \"Understand your data's length distribution\")\n",
    "    ]\n",
    "    \n",
    "    for practice, description in recommendations:\n",
    "        print(f\"   {practice:<20}: {description}\")\n",
    "    \n",
    "    print(\"\\n🚨 COMMON PITFALLS TO AVOID:\")\n",
    "    pitfalls = [\n",
    "        (\"❌ Fixed Max Length\", \"Wastes memory and computation on short sequences\"),\n",
    "        (\"❌ No Attention Masks\", \"Model learns from padding tokens (bad!)\"),\n",
    "        (\"❌ Ignoring Length Distribution\", \"Inefficient batching strategy\"),\n",
    "        (\"❌ Too Large Batches\", \"Can cause out-of-memory errors\"),\n",
    "        (\"❌ No Truncation Strategy\", \"Crashes on unexpectedly long sequences\")\n",
    "    ]\n",
    "    \n",
    "    for pitfall, description in pitfalls:\n",
    "        print(f\"   {pitfall:<25}: {description}\")\n",
    "    \n",
    "    return {\n",
    "        'memory_savings': memory_savings,\n",
    "        'fixed_memory': fixed_memory,\n",
    "        'dynamic_memory': dynamic_memory\n",
    "    }\n",
    "\n",
    "# Run the best practices demonstration\n",
    "optimization_results = demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📋 Summary\n",
    "\n",
    "### 🔑 Key Concepts Mastered\n",
    "- **Multiple Sequence Challenge**: Understanding why variable-length sequences create processing difficulties\n",
    "- **Padding and Truncation**: Strategic approaches to normalize sequence lengths for batch processing\n",
    "- **Attention Masks**: Critical mechanism for distinguishing real content from padding tokens\n",
    "- **Long Sequence Handling**: Techniques for processing sequences beyond standard model limits\n",
    "- **Longformer Architecture**: Specialized model design for extended context understanding\n",
    "- **Attention Visualization**: Methods to understand model behavior across different sequence types\n",
    "- **Performance Optimization**: Best practices for efficient multiple sequence processing\n",
    "\n",
    "### 📈 Best Practices Learned\n",
    "- Use dynamic padding instead of fixed max_length for memory efficiency\n",
    "- Always implement proper attention masking to prevent padding interference\n",
    "- Group sequences by similar lengths for optimal batching efficiency\n",
    "- Choose appropriate models (Longformer) for consistently long sequences\n",
    "- Profile your data's length distribution to inform processing strategies\n",
    "- Monitor memory usage and adjust batch sizes accordingly\n",
    "- Visualize attention patterns to understand model behavior and debug issues\n",
    "\n",
    "### 🚀 Next Steps\n",
    "- **Advanced Training**: Explore gradient accumulation and mixed precision training\n",
    "- **Custom Models**: Implement custom attention mechanisms for specific use cases\n",
    "- **Production Deployment**: Learn about model serving and inference optimization\n",
    "- **Evaluation Metrics**: Understanding how sequence length affects model performance\n",
    "- **Data Preprocessing**: Advanced techniques for handling diverse text sources\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- 🌐 **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- 💼 **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- 💻 **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}