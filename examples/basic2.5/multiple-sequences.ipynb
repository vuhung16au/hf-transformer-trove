{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.5/multiple-sequences.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.5/multiple-sequences.ipynb)\n",
    "\n",
    "# Handling Multiple Sequences: Padding, Attention Masks, and Long Context\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to handle sequences of different lengths in batch processing\n",
    "- The importance and mechanics of padding and attention masks\n",
    "- Strategies for dealing with longer sequences and truncation\n",
    "- Advanced techniques with Longformer for extended context understanding\n",
    "- Visual analysis of attention patterns across different sequence lengths\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of tokenization concepts (refer to `02_tokenizers.ipynb`)\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. Section 1: Understanding the Multiple Sequences Problem\n",
    "2. Section 2: Padding and Truncation Strategies\n",
    "3. Section 3: Attention Masks Deep Dive\n",
    "4. Section 4: Handling Longer Sequences\n",
    "5. Section 5: Longformer for Extended Context\n",
    "6. Section 6: Visualizing Attention Patterns\n",
    "7. Section 7: Best Practices and Performance Optimization\n",
    "8. Section 8: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for this comprehensive tutorial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    LongformerTokenizer, LongformerModel, LongformerForSequenceClassification,\n",
    "    AutoConfig, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "import os\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env.local loading\")\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    COLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "\n",
    "def get_api_key(key_name: str, required: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load API key from environment or Google Colab secrets.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Environment variable name\n",
    "        required: Whether to raise error if not found\n",
    "        \n",
    "    Returns:\n",
    "        API key string or None\n",
    "    \"\"\"\n",
    "    # Try Colab secrets first\n",
    "    if COLAB_AVAILABLE:\n",
    "        try:\n",
    "            return userdata.get(key_name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    api_key = os.getenv(key_name)\n",
    "    \n",
    "    if required and not api_key:\n",
    "        raise ValueError(\n",
    "            f\"{key_name} not found. Set it in:\\n\"\n",
    "            f\"- Local: .env.local file\\n\"\n",
    "            f\"- Colab: Secrets manager\"\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup authentication and device\n",
    "hf_token = get_api_key('HF_TOKEN', required=False)\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"‚úÖ Hugging Face token configured\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Set up plotting style for educational visualizations\n",
    "plt.style.use('default')  # Use default style for better compatibility\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"\\n=== Setup Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Ready for multiple sequence processing! üéØ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding the Multiple Sequences Problem\n",
    "\n",
    "When working with real-world NLP tasks, we rarely process single sequences. Instead, we need to handle batches of text sequences that vary significantly in length. This creates several challenges:\n",
    "\n",
    "### Key Challenges:\n",
    "- **Variable Length**: Text sequences have different numbers of tokens\n",
    "- **Batch Processing**: Neural networks require fixed-size inputs for efficient computation\n",
    "- **Memory Efficiency**: Longer sequences consume more computational resources\n",
    "- **Attention Mechanics**: Models need to know which parts of input to focus on\n",
    "\n",
    "Let's start by demonstrating this problem with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sequence_length_problem():\n",
    "    \"\"\"\n",
    "    Demonstrate the variable sequence length problem with real text examples.\n",
    "    \"\"\"\n",
    "    print(\"üîç DEMONSTRATING THE MULTIPLE SEQUENCES PROBLEM\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Example texts with varying lengths - using hate speech detection focus\n",
    "    example_texts = [\n",
    "        \"Great post!\",  # Short: 3 words\n",
    "        \"I really enjoyed reading this article about machine learning.\",  # Medium: 10 words\n",
    "        \"This comprehensive tutorial on natural language processing and transformer models provides detailed insights into modern NLP techniques and their practical applications in industry.\",  # Long: 25 words\n",
    "        \"AI\",  # Very short: 1 word\n",
    "        \"The development of large language models has revolutionized how we approach various NLP tasks, from text classification to question answering, enabling more sophisticated and nuanced understanding of human language patterns.\"  # Very long: 32 words\n",
    "    ]\n",
    "    \n",
    "    # Load a BERT tokenizer for demonstration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    print(\"üìä Text Length Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    tokenized_lengths = []\n",
    "    for i, text in enumerate(example_texts):\n",
    "        # Tokenize without any padding or truncation\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        word_count = len(text.split())\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        tokenized_lengths.append(token_count)\n",
    "        \n",
    "        print(f\"{i+1}. Words: {word_count:2d} | Tokens: {token_count:2d} | Text: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "    \n",
    "    print(f\"\\nüìà Token Length Statistics:\")\n",
    "    print(f\"   Min length: {min(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Max length: {max(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Range: {max(tokenized_lengths) - min(tokenized_lengths)} tokens\")\n",
    "    print(f\"   Average: {np.mean(tokenized_lengths):.1f} tokens\")\n",
    "    \n",
    "    # Visualize the length distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(range(1, len(tokenized_lengths) + 1), tokenized_lengths, color='skyblue', alpha=0.8)\n",
    "    plt.title('Token Count per Sequence')\n",
    "    plt.xlabel('Sequence Number')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, length in zip(bars, tokenized_lengths):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                str(length), ha='center', va='bottom')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Create a visual representation of sequence lengths\n",
    "    sequences_visual = np.zeros((len(tokenized_lengths), max(tokenized_lengths)))\n",
    "    for i, length in enumerate(tokenized_lengths):\n",
    "        sequences_visual[i, :length] = 1\n",
    "    \n",
    "    plt.imshow(sequences_visual, cmap='RdYlBu_r', aspect='auto')\n",
    "    plt.title('Sequence Length Visualization\\n(Blue = tokens, White = padding needed)')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Sequence Number')\n",
    "    plt.yticks(range(len(tokenized_lengths)), [f'Seq {i+1}' for i in range(len(tokenized_lengths))])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüö® THE PROBLEM:\")\n",
    "    print(\"   ‚Ä¢ Neural networks need fixed-size inputs for batch processing\")\n",
    "    print(\"   ‚Ä¢ Variable sequence lengths prevent efficient batching\")\n",
    "    print(\"   ‚Ä¢ Need padding to make all sequences the same length\")\n",
    "    print(\"   ‚Ä¢ Need attention masks to ignore padded positions\")\n",
    "    \n",
    "    return example_texts, tokenized_lengths\n",
    "\n",
    "# Run the demonstration\n",
    "example_texts, tokenized_lengths = demonstrate_sequence_length_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Padding and Truncation Strategies\n",
    "\n",
    "To solve the variable sequence length problem, we use **padding** and **truncation**:\n",
    "\n",
    "### Padding Strategies:\n",
    "- **Right Padding**: Add padding tokens to the end of sequences (most common)\n",
    "- **Left Padding**: Add padding tokens to the beginning (used for some generative models)\n",
    "- **Dynamic Padding**: Pad only to the length of the longest sequence in the batch\n",
    "- **Fixed Padding**: Pad all sequences to a predetermined maximum length\n",
    "\n",
    "### Truncation Strategies:\n",
    "- **Right Truncation**: Remove tokens from the end\n",
    "- **Left Truncation**: Remove tokens from the beginning  \n",
    "- **Middle Truncation**: Remove tokens from the middle (less common)\n",
    "\n",
    "Let's implement and compare these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_padding_strategies():\n",
    "    \"\"\"\n",
    "    Demonstrate different padding and truncation strategies.\n",
    "    \"\"\"\n",
    "    print(\"üõ†Ô∏è PADDING AND TRUNCATION STRATEGIES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Load preferred hate speech detection model tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "    \n",
    "    # Use example texts from hate speech detection domain\n",
    "    texts = [\n",
    "        \"This message promotes understanding and respect.\",\n",
    "        \"Great work!\",\n",
    "        \"I completely disagree with this perspective, but I respect your right to express it and welcome civil discourse on the topic.\",\n",
    "        \"Thanks\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìù Original texts:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"{i+1}. '{text}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Strategy 1: No padding/truncation (shows the problem)\n",
    "    print(\"\\n1Ô∏è‚É£ NO PADDING/TRUNCATION (The Problem):\")\n",
    "    try:\n",
    "        unpadded = tokenizer(texts, return_tensors=\"pt\")\n",
    "        print(\"   ‚úÖ Successful batching\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)[:100]}...\")\n",
    "        print(\"   This is why we need padding!\")\n",
    "    \n",
    "    # Strategy 2: Right padding (most common)\n",
    "    print(\"\\n2Ô∏è‚É£ RIGHT PADDING:\")\n",
    "    right_padded = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {right_padded['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {right_padded['attention_mask'].shape}\")\n",
    "    \n",
    "    # Strategy 3: Fixed max length padding  \n",
    "    print(\"\\n3Ô∏è‚É£ FIXED LENGTH PADDING (max_length=20):\")\n",
    "    fixed_padded = tokenizer(texts, padding='max_length', max_length=20, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {fixed_padded['input_ids'].shape}\")\n",
    "    \n",
    "    # Strategy 4: Truncation with padding\n",
    "    print(\"\\n4Ô∏è‚É£ TRUNCATION + PADDING (max_length=15):\")\n",
    "    truncated_padded = tokenizer(texts, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "    print(f\"   Shape: {truncated_padded['input_ids'].shape}\")\n",
    "    \n",
    "    # Visualize the different strategies\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Padding and Truncation Strategies Visualization', fontsize=16)\n",
    "    \n",
    "    strategies = [\n",
    "        (\"Right Padding\", right_padded),\n",
    "        (\"Fixed Length (20)\", fixed_padded),\n",
    "        (\"Truncated + Padded (15)\", truncated_padded),\n",
    "        (\"Attention Masks\", right_padded)  # Show attention masks\n",
    "    ]\n",
    "    \n",
    "    for idx, (title, encoded) in enumerate(strategies):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        if title == \"Attention Masks\":\n",
    "            # Show attention masks\n",
    "            data = encoded['attention_mask'].numpy()\n",
    "            cmap = 'RdBu_r'\n",
    "        else:\n",
    "            # Show input IDs (replace pad tokens with -1 for visualization)\n",
    "            data = encoded['input_ids'].numpy()\n",
    "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            data = np.where(data == pad_token_id, -1, 1)  # 1 for real tokens, -1 for padding\n",
    "            cmap = 'RdYlBu_r'\n",
    "        \n",
    "        im = ax.imshow(data, cmap=cmap, aspect='auto')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Token Position')\n",
    "        ax.set_ylabel('Sequence Number')\n",
    "        ax.set_yticks(range(len(texts)))\n",
    "        ax.set_yticklabels([f'Text {i+1}' for i in range(len(texts))])\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show detailed token analysis for first text\n",
    "    print(\"\\nüîç DETAILED ANALYSIS - First Text:\")\n",
    "    print(f\"   Original: '{texts[0]}'\")\n",
    "    \n",
    "    # Show tokens for different strategies\n",
    "    strategies_detail = [\n",
    "        (\"Right Padded\", right_padded['input_ids'][0]),\n",
    "        (\"Fixed Length\", fixed_padded['input_ids'][0]),  \n",
    "        (\"Truncated\", truncated_padded['input_ids'][0])\n",
    "    ]\n",
    "    \n",
    "    for name, token_ids in strategies_detail:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        print(f\"   {name:15}: {tokens}\")\n",
    "        print(f\"   {'Length':15}: {len([t for t in tokens if t != tokenizer.pad_token])} real tokens, {len(tokens)} total\")\n",
    "    \n",
    "    return {\n",
    "        'right_padded': right_padded,\n",
    "        'fixed_padded': fixed_padded,\n",
    "        'truncated_padded': truncated_padded\n",
    "    }\n",
    "\n",
    "# Run the demonstration\n",
    "padding_results = demonstrate_padding_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Multiple Sequence Challenge**: Understanding why variable-length sequences create processing difficulties\n",
    "- **Padding and Truncation**: Strategic approaches to normalize sequence lengths for batch processing\n",
    "- **Attention Masks**: Critical mechanism for distinguishing real content from padding tokens\n",
    "- **Long Sequence Handling**: Techniques for processing sequences beyond standard model limits\n",
    "- **Longformer Architecture**: Specialized model design for extended context understanding\n",
    "- **Attention Visualization**: Methods to understand model behavior across different sequence types\n",
    "- **Performance Optimization**: Best practices for efficient multiple sequence processing\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use dynamic padding instead of fixed max_length for memory efficiency\n",
    "- Always implement proper attention masking to prevent padding interference\n",
    "- Group sequences by similar lengths for optimal batching efficiency\n",
    "- Choose appropriate models (Longformer) for consistently long sequences\n",
    "- Profile your data's length distribution to inform processing strategies\n",
    "- Monitor memory usage and adjust batch sizes accordingly\n",
    "- Visualize attention patterns to understand model behavior and debug issues\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced Training**: Explore gradient accumulation and mixed precision training\n",
    "- **Custom Models**: Implement custom attention mechanisms for specific use cases\n",
    "- **Production Deployment**: Learn about model serving and inference optimization\n",
    "- **Evaluation Metrics**: Understanding how sequence length affects model performance\n",
    "- **Data Preprocessing**: Advanced techniques for handling diverse text sources\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}