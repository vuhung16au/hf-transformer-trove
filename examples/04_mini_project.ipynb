{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/04_mini_project.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/04_mini_project.ipynb)\n",
    "\n",
    "# 04 - Mini-Project: Building a Complete Sentiment Analysis System\n",
    "\n",
    "## Project Overview\n",
    "In this mini-project, we'll combine everything we've learned from the first three notebooks:\n",
    "- **Hugging Face transformers** for model loading and inference\n",
    "- **Tokenizers** for text preprocessing\n",
    "- **Datasets** for efficient data handling\n",
    "\n",
    "We'll build a complete sentiment analysis system that can:\n",
    "1. Load and preprocess data efficiently\n",
    "2. Compare different models and tokenizers\n",
    "3. Analyze model performance\n",
    "4. Create a user-friendly interface for predictions\n",
    "5. Handle edge cases and errors gracefully\n",
    "\n",
    "## Learning Objectives\n",
    "- Integrate transformers, tokenizers, and datasets libraries\n",
    "- Build a complete ML pipeline from data to deployment\n",
    "- Compare different models systematically\n",
    "- Create robust, production-ready code\n",
    "- Visualize and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline, Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env.local loading\")\n",
    "\n",
    "# Credential management function\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Get API key from environment or Colab secrets.\"\"\"\n",
    "    try:\n",
    "        # Try to import Colab userdata (only available in Colab)\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(key_name)\n",
    "    except (ImportError, Exception):\n",
    "        # Fall back to local environment variable\n",
    "        api_key = os.getenv(key_name)\n",
    "        if not api_key:\n",
    "            print(f\"Info: {key_name} not found. Public models will work without authentication.\")\n",
    "            return None\n",
    "        return api_key\n",
    "\n",
    "# Device detection function\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for training/inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup authentication and device\n",
    "hf_token = get_api_key('HF_TOKEN')\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"Hugging Face token configured\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"\\n=== Setup Information ===\")\n",
    "print(f\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif device.type == 'mps':\n",
    "    print(\"Apple Silicon GPU (MPS) detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation Pipeline\n",
    "\n",
    "Let's start by creating a robust data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataProcessor:\n",
    "    \"\"\"A complete data processing pipeline for sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name=\"imdb\", sample_size=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.sample_size = sample_size\n",
    "        self.dataset = None\n",
    "        self.processed_dataset = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load dataset with error handling\"\"\"\n",
    "        print(f\"Loading {self.dataset_name} dataset...\")\n",
    "        \n",
    "        try:\n",
    "            self.dataset = load_dataset(self.dataset_name)\n",
    "            \n",
    "            if self.sample_size:\n",
    "                print(f\"Sampling {self.sample_size} examples for faster processing...\")\n",
    "                self.dataset['train'] = self.dataset['train'].select(range(min(self.sample_size, len(self.dataset['train']))))\n",
    "                if 'test' in self.dataset:\n",
    "                    test_sample = min(self.sample_size // 2, len(self.dataset['test']))\n",
    "                    self.dataset['test'] = self.dataset['test'].select(range(test_sample))\n",
    "            \n",
    "            print(f\"‚úì Dataset loaded successfully!\")\n",
    "            for split, data in self.dataset.items():\n",
    "                print(f\"  {split}: {len(data):,} examples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading dataset: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def analyze_data(self):\n",
    "        \"\"\"Analyze dataset characteristics\"\"\"\n",
    "        if not self.dataset:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_data() first.\")\n",
    "        \n",
    "        print(\"\\nDataset Analysis:\")\n",
    "        print(\"=\" * 20)\n",
    "        \n",
    "        train_data = self.dataset['train']\n",
    "        \n",
    "        # Label distribution\n",
    "        labels = train_data['label']\n",
    "        label_counts = Counter(labels)\n",
    "        \n",
    "        print(f\"Features: {train_data.features}\")\n",
    "        print(f\"Label distribution: {dict(label_counts)}\")\n",
    "        \n",
    "        # Text statistics\n",
    "        sample_texts = [ex['text'] for ex in train_data.select(range(min(1000, len(train_data))))]\n",
    "        word_counts = [len(text.split()) for text in sample_texts]\n",
    "        char_counts = [len(text) for text in sample_texts]\n",
    "        \n",
    "        print(f\"\\nText Statistics (sample of {len(sample_texts)}):\")\n",
    "        print(f\"  Word count - Mean: {np.mean(word_counts):.1f}, Median: {np.median(word_counts):.1f}\")\n",
    "        print(f\"  Char count - Mean: {np.mean(char_counts):.1f}, Median: {np.median(char_counts):.1f}\")\n",
    "        print(f\"  Min words: {min(word_counts)}, Max words: {max(word_counts)}\")\n",
    "        \n",
    "        return {\n",
    "            'label_distribution': label_counts,\n",
    "            'word_stats': {'mean': np.mean(word_counts), 'median': np.median(word_counts)},\n",
    "            'sample_size': len(sample_texts)\n",
    "        }\n",
    "    \n",
    "    def create_small_test_set(self, size=100):\n",
    "        \"\"\"Create a small balanced test set for quick evaluation\"\"\"\n",
    "        if not self.dataset:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_data() first.\")\n",
    "        \n",
    "        train_data = self.dataset['train']\n",
    "        \n",
    "        # Get equal numbers of positive and negative examples\n",
    "        positive_examples = train_data.filter(lambda x: x['label'] == 1).select(range(size // 2))\n",
    "        negative_examples = train_data.filter(lambda x: x['label'] == 0).select(range(size // 2))\n",
    "        \n",
    "        # Combine and create new dataset\n",
    "        from datasets import concatenate_datasets\n",
    "        small_test = concatenate_datasets([positive_examples, negative_examples])\n",
    "        \n",
    "        print(f\"Created balanced test set with {len(small_test)} examples\")\n",
    "        return small_test\n",
    "\n",
    "# Initialize data processor\n",
    "data_processor = SentimentDataProcessor(sample_size=5000)  # Use smaller sample for speed\n",
    "data_processor.load_data()\n",
    "analysis_results = data_processor.analyze_data()\n",
    "\n",
    "# Create test set\n",
    "test_set = data_processor.create_small_test_set(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Model Comparison Framework\n",
    "\n",
    "Now let's create a framework to systematically compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparison:\n",
    "    \"\"\"Framework for comparing different sentiment analysis models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.pipelines = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Define models to compare\n",
    "        self.model_configs = {\n",
    "            \"DistilBERT\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            \"RoBERTa\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            \"BERT\": \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        }\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all models and tokenizers\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        for name, model_name in self.model_configs.items():\n",
    "            try:\n",
    "                print(f\"  Loading {name} ({model_name})...\")\n",
    "                \n",
    "                # Create pipeline (easiest for comparison)\n",
    "                self.pipelines[name] = pipeline(\n",
    "                    \"sentiment-analysis\",\n",
    "                    model=model_name,\n",
    "                    tokenizer=model_name,\n",
    "                    device=0 if device.type == 'cuda' else -1\n",
    "                )\n",
    "                \n",
    "                print(f\"    ‚úì {name} loaded successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚úó Failed to load {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nLoaded {len(self.pipelines)} models successfully\")\n",
    "    \n",
    "    def evaluate_models(self, test_dataset, max_examples=100):\n",
    "        \"\"\"Evaluate all models on test dataset\"\"\"\n",
    "        print(f\"\\nEvaluating models on {min(max_examples, len(test_dataset))} examples...\")\n",
    "        \n",
    "        # Prepare test data\n",
    "        test_examples = test_dataset.select(range(min(max_examples, len(test_dataset))))\n",
    "        texts = [ex['text'] for ex in test_examples]\n",
    "        true_labels = [ex['label'] for ex in test_examples]\n",
    "        \n",
    "        for model_name, pipeline_model in self.pipelines.items():\n",
    "            print(f\"\\n  Evaluating {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = pipeline_model(texts)\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Convert predictions to binary format\n",
    "                pred_labels = self._convert_predictions(predictions, model_name)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(true_labels, pred_labels)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[model_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'inference_time': inference_time,\n",
    "                    'predictions': pred_labels,\n",
    "                    'true_labels': true_labels,\n",
    "                    'raw_predictions': predictions\n",
    "                }\n",
    "                \n",
    "                print(f\"    ‚úì Accuracy: {accuracy:.3f}\")\n",
    "                print(f\"    ‚úì Inference time: {inference_time:.2f}s\")\n",
    "                print(f\"    ‚úì Speed: {len(texts)/inference_time:.1f} examples/second\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚úó Error evaluating {model_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def _convert_predictions(self, predictions, model_name):\n",
    "        \"\"\"Convert model predictions to binary labels (0/1)\"\"\"\n",
    "        pred_labels = []\n",
    "        \n",
    "        for pred in predictions:\n",
    "            label = pred['label'].upper()\n",
    "            \n",
    "            # Handle different label formats\n",
    "            if label in ['POSITIVE', 'POS', '1', 'LABEL_1']:\n",
    "                pred_labels.append(1)\n",
    "            elif label in ['NEGATIVE', 'NEG', '0', 'LABEL_0']:\n",
    "                pred_labels.append(0)\n",
    "            else:\n",
    "                # For models with different labels, use confidence score\n",
    "                if pred['score'] > 0.5:\n",
    "                    pred_labels.append(1 if 'POS' in label else 0)\n",
    "                else:\n",
    "                    pred_labels.append(0 if 'POS' in label else 1)\n",
    "        \n",
    "        return pred_labels\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualizations of model comparison results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Run evaluate_models() first.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Model Comparison Results', fontsize=16)\n",
    "        \n",
    "        # 1. Accuracy comparison\n",
    "        models = list(self.results.keys())\n",
    "        accuracies = [self.results[model]['accuracy'] for model in models]\n",
    "        \n",
    "        axes[0, 0].bar(models, accuracies, alpha=0.7)\n",
    "        axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(accuracies):\n",
    "            axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Speed comparison\n",
    "        speeds = [len(self.results[model]['predictions'])/self.results[model]['inference_time'] \n",
    "                 for model in models]\n",
    "        \n",
    "        axes[0, 1].bar(models, speeds, alpha=0.7, color='orange')\n",
    "        axes[0, 1].set_title('Inference Speed Comparison')\n",
    "        axes[0, 1].set_ylabel('Examples/Second')\n",
    "        \n",
    "        # 3. Confusion matrix for best model\n",
    "        best_model = max(models, key=lambda x: self.results[x]['accuracy'])\n",
    "        cm = confusion_matrix(self.results[best_model]['true_labels'], \n",
    "                             self.results[best_model]['predictions'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title(f'Confusion Matrix - {best_model}')\n",
    "        axes[1, 0].set_xlabel('Predicted')\n",
    "        axes[1, 0].set_ylabel('Actual')\n",
    "        \n",
    "        # 4. Accuracy vs Speed trade-off\n",
    "        axes[1, 1].scatter(speeds, accuracies, s=100, alpha=0.7)\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            axes[1, 1].annotate(model, (speeds[i], accuracies[i]), \n",
    "                               xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Speed (examples/second)')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_title('Accuracy vs Speed Trade-off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\nDetailed Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        for model in models:\n",
    "            results = self.results[model]\n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  Accuracy: {results['accuracy']:.3f}\")\n",
    "            print(f\"  Speed: {len(results['predictions'])/results['inference_time']:.1f} examples/sec\")\n",
    "            print(f\"  Inference time: {results['inference_time']:.2f}s\")\n",
    "\n",
    "# Initialize and run model comparison\n",
    "model_comparison = ModelComparison()\n",
    "model_comparison.load_models()\n",
    "model_comparison.evaluate_models(test_set, max_examples=100)\n",
    "model_comparison.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Advanced Analysis and Error Cases\n",
    "\n",
    "Let's analyze model behavior on different types of inputs and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisAnalyzer:\n",
    "    \"\"\"Advanced analysis of sentiment models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_comparison):\n",
    "        self.model_comparison = model_comparison\n",
    "        self.edge_cases = self._create_edge_cases()\n",
    "    \n",
    "    def _create_edge_cases(self):\n",
    "        \"\"\"Create challenging test cases\"\"\"\n",
    "        return {\n",
    "            \"Short texts\": [\n",
    "                \"Good\", \"Bad\", \"OK\", \"Meh\", \"Great!\", \"Awful\"\n",
    "            ],\n",
    "            \"Sarcastic/Ironic\": [\n",
    "                \"Oh great, another delay...\",\n",
    "                \"Just what I needed today\",\n",
    "                \"Perfect timing as always\",\n",
    "                \"Wow, such amazing service\"\n",
    "            ],\n",
    "            \"Neutral/Mixed\": [\n",
    "                \"It was okay, nothing special\",\n",
    "                \"Some good points, some bad ones\",\n",
    "                \"Average product, average price\",\n",
    "                \"Could be better, could be worse\"\n",
    "            ],\n",
    "            \"Emotional/Strong\": [\n",
    "                \"I absolutely LOVE this product!!!\",\n",
    "                \"This is the WORST thing I've ever bought\",\n",
    "                \"Amazing amazing amazing!!!\",\n",
    "                \"Terrible terrible terrible\"\n",
    "            ],\n",
    "            \"Long/Complex\": [\n",
    "                \"While the initial experience was somewhat disappointing due to shipping delays, the actual product quality exceeded my expectations and the customer service team was very helpful in resolving my concerns.\",\n",
    "                \"I have mixed feelings about this purchase because although the price was reasonable and the features are adequate for basic use, the build quality feels somewhat cheap and I'm not sure it will last very long.\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def test_edge_cases(self):\n",
    "        \"\"\"Test models on edge cases\"\"\"\n",
    "        print(\"Testing Edge Cases:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for category, texts in self.edge_cases.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(\"-\" * len(category))\n",
    "            \n",
    "            category_results = {}\n",
    "            \n",
    "            for model_name, pipeline_model in self.model_comparison.pipelines.items():\n",
    "                try:\n",
    "                    predictions = pipeline_model(texts)\n",
    "                    category_results[model_name] = predictions\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with {model_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Display results for each text\n",
    "            for i, text in enumerate(texts):\n",
    "                print(f\"\\n  Text: '{text}'\")\n",
    "                \n",
    "                for model_name in category_results:\n",
    "                    pred = category_results[model_name][i]\n",
    "                    label = pred['label']\n",
    "                    score = pred['score']\n",
    "                    print(f\"    {model_name:12}: {label:12} ({score:.3f})\")\n",
    "            \n",
    "            results[category] = category_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_agreement(self, edge_case_results):\n",
    "        \"\"\"Analyze where models agree/disagree\"\"\"\n",
    "        print(\"\\n\\nModel Agreement Analysis:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        model_names = list(self.model_comparison.pipelines.keys())\n",
    "        \n",
    "        for category, results in edge_case_results.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            \n",
    "            texts = self.edge_cases[category]\n",
    "            agreements = []\n",
    "            \n",
    "            for i in range(len(texts)):\n",
    "                # Get predictions for this text from all models\n",
    "                text_predictions = []\n",
    "                for model in model_names:\n",
    "                    if model in results:\n",
    "                        label = results[model][i]['label'].upper()\n",
    "                        # Normalize labels\n",
    "                        if 'POS' in label or label in ['POSITIVE', '1', 'LABEL_1']:\n",
    "                            text_predictions.append('POSITIVE')\n",
    "                        else:\n",
    "                            text_predictions.append('NEGATIVE')\n",
    "                \n",
    "                # Check if all models agree\n",
    "                if len(set(text_predictions)) == 1:\n",
    "                    agreements.append(\"AGREE\")\n",
    "                    consensus = text_predictions[0]\n",
    "                else:\n",
    "                    agreements.append(\"DISAGREE\")\n",
    "                    consensus = \"MIXED\"\n",
    "                \n",
    "                print(f\"  '{texts[i][:50]}...': {agreements[-1]} ({consensus})\")\n",
    "            \n",
    "            agreement_rate = agreements.count(\"AGREE\") / len(agreements)\n",
    "            print(f\"  ‚Üí Agreement rate: {agreement_rate:.1%}\")\n",
    "    \n",
    "    def create_interactive_demo(self):\n",
    "        \"\"\"Create a simple interactive demo\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"INTERACTIVE SENTIMENT ANALYSIS DEMO\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Enter text to analyze (or 'quit' to exit)\")\n",
    "        \n",
    "        # For notebook environment, we'll just demo with predefined examples\n",
    "        demo_texts = [\n",
    "            \"I love this product!\",\n",
    "            \"This is terrible\",\n",
    "            \"It's okay, nothing special\",\n",
    "            \"Best purchase ever!!!\",\n",
    "            \"Waste of money\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nDemo with sample texts:\")\n",
    "        for text in demo_texts:\n",
    "            print(f\"\\nInput: '{text}'\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for model_name, pipeline_model in self.model_comparison.pipelines.items():\n",
    "                try:\n",
    "                    result = pipeline_model(text)[0]\n",
    "                    label = result['label']\n",
    "                    confidence = result['score']\n",
    "                    \n",
    "                    # Add emoji for fun\n",
    "                    emoji = \"üòä\" if \"POS\" in label.upper() else \"üòû\"\n",
    "                    \n",
    "                    print(f\"  {model_name:12}: {label:12} {emoji} (confidence: {confidence:.3f})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {model_name:12}: Error - {e}\")\n",
    "\n",
    "# Run advanced analysis\n",
    "analyzer = SentimentAnalysisAnalyzer(model_comparison)\n",
    "edge_case_results = analyzer.test_edge_cases()\n",
    "analyzer.analyze_agreement(edge_case_results)\n",
    "analyzer.create_interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Tokenization Deep Dive\n",
    "\n",
    "Let's analyze how different tokenizers handle the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_comparison_analysis():\n",
    "    \"\"\"Compare how different tokenizers handle the same texts\"\"\"\n",
    "    \n",
    "    print(\"Tokenization Comparison Analysis:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Load different tokenizers\n",
    "    tokenizer_configs = {\n",
    "        \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "        \"RoBERTa\": \"roberta-base\",\n",
    "        \"BERT\": \"bert-base-uncased\"\n",
    "    }\n",
    "    \n",
    "    tokenizers = {}\n",
    "    for name, model_name in tokenizer_configs.items():\n",
    "        try:\n",
    "            tokenizers[name] = AutoTokenizer.from_pretrained(model_name)\n",
    "            print(f\"‚úì Loaded {name} tokenizer\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to load {name}: {e}\")\n",
    "    \n",
    "    # Test texts with different characteristics\n",
    "    test_texts = [\n",
    "        \"Hello world!\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"antidisestablishmentarianism\",  # Long word\n",
    "        \"COVID-19 pandemic\",  # Numbers and hyphens\n",
    "        \"üòä I'm happy!\",  # Emojis\n",
    "        \"user@domain.com visited https://example.com\",  # URLs and emails\n",
    "    ]\n",
    "    \n",
    "    # Compare tokenization\n",
    "    for text in test_texts:\n",
    "        print(f\"\\nText: '{text}'\")\n",
    "        print(\"-\" * (len(text) + 8))\n",
    "        \n",
    "        for name, tokenizer in tokenizers.items():\n",
    "            try:\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "                \n",
    "                print(f\"  {name:12}: {len(tokens):2d} tokens - {tokens}\")\n",
    "                \n",
    "                # Show how it gets reconstructed\n",
    "                decoded = tokenizer.decode(token_ids)\n",
    "                if decoded.strip() != text.strip():\n",
    "                    print(f\"  {' ':12}   Decoded: '{decoded}'\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  {name:12}: Error - {e}\")\n",
    "    \n",
    "    # Analyze tokenization efficiency\n",
    "    print(\"\\n\\nTokenization Efficiency Analysis:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    sample_texts = [ex['text'] for ex in test_set.select(range(50))]\n",
    "    \n",
    "    efficiency_data = []\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        token_counts = []\n",
    "        char_counts = []\n",
    "        \n",
    "        for text in sample_texts:\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            token_counts.append(len(tokens))\n",
    "            char_counts.append(len(text))\n",
    "        \n",
    "        # Calculate compression ratio (characters per token)\n",
    "        compression_ratio = np.mean([chars/tokens for chars, tokens in zip(char_counts, token_counts)])\n",
    "        \n",
    "        efficiency_data.append({\n",
    "            'tokenizer': name,\n",
    "            'avg_tokens': np.mean(token_counts),\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'vocab_size': len(tokenizer)\n",
    "        })\n",
    "        \n",
    "        print(f\"{name:12}: Avg tokens: {np.mean(token_counts):5.1f}, \"\n",
    "              f\"Compression: {compression_ratio:.2f} chars/token, \"\n",
    "              f\"Vocab: {len(tokenizer):,}\")\n",
    "    \n",
    "    # Visualize tokenization comparison\n",
    "    df = pd.DataFrame(efficiency_data)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Average tokens per text\n",
    "    axes[0].bar(df['tokenizer'], df['avg_tokens'], alpha=0.7)\n",
    "    axes[0].set_title('Average Tokens per Text')\n",
    "    axes[0].set_ylabel('Number of Tokens')\n",
    "    \n",
    "    # Compression ratio\n",
    "    axes[1].bar(df['tokenizer'], df['compression_ratio'], alpha=0.7, color='orange')\n",
    "    axes[1].set_title('Compression Efficiency')\n",
    "    axes[1].set_ylabel('Characters per Token')\n",
    "    \n",
    "    # Vocabulary size\n",
    "    axes[2].bar(df['tokenizer'], df['vocab_size'], alpha=0.7, color='green')\n",
    "    axes[2].set_title('Vocabulary Size')\n",
    "    axes[2].set_ylabel('Number of Tokens')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "tokenization_comparison_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Production-Ready Sentiment Analyzer\n",
    "\n",
    "Let's create a robust, production-ready sentiment analyzer with proper error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionSentimentAnalyzer:\n",
    "    \"\"\"Production-ready sentiment analyzer with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.model_name = model_name\n",
    "        self.pipeline = None\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.max_length = 512\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model with error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model: {self.model_name}\")\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                device=0 if device.type == 'cuda' else -1,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            print(\"‚úì Model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text with comprehensive cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Handle empty or whitespace-only text\n",
    "        if not text.strip():\n",
    "            return \"[EMPTY]\"\n",
    "        \n",
    "        # Basic cleaning (you can extend this)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Check if text is too long\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(tokens) > self.max_length:\n",
    "            # Truncate and decode to maintain readable text\n",
    "            truncated_tokens = tokens[:self.max_length-1] + [self.tokenizer.sep_token_id]\n",
    "            text = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict sentiment for a single text\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            \n",
    "            # Get prediction\n",
    "            result = self.pipeline(processed_text)[0]\n",
    "            \n",
    "            # Normalize output\n",
    "            label = result['label'].upper()\n",
    "            confidence = result['score']\n",
    "            \n",
    "            # Convert to standardized format\n",
    "            if 'POS' in label or label in ['POSITIVE', '1', 'LABEL_1']:\n",
    "                sentiment = 'positive'\n",
    "                score = confidence\n",
    "            else:\n",
    "                sentiment = 'negative'\n",
    "                score = confidence\n",
    "            \n",
    "            return {\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': score,\n",
    "                'processed_text': processed_text,\n",
    "                'raw_output': result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': text,\n",
    "                'sentiment': 'error',\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"Predict sentiment for a batch of texts\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process in batches for memory efficiency\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Preprocess batch\n",
    "                processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "                \n",
    "                # Get predictions\n",
    "                batch_results = self.pipeline(processed_batch)\n",
    "                \n",
    "                # Process results\n",
    "                for j, (original_text, processed_text, result) in enumerate(zip(batch, processed_batch, batch_results)):\n",
    "                    label = result['label'].upper()\n",
    "                    confidence = result['score']\n",
    "                    \n",
    "                    if 'POS' in label or label in ['POSITIVE', '1', 'LABEL_1']:\n",
    "                        sentiment = 'positive'\n",
    "                    else:\n",
    "                        sentiment = 'negative'\n",
    "                    \n",
    "                    results.append({\n",
    "                        'text': original_text,\n",
    "                        'sentiment': sentiment,\n",
    "                        'confidence': confidence,\n",
    "                        'processed_text': processed_text\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Handle batch errors\n",
    "                for text in batch:\n",
    "                    results.append({\n",
    "                        'text': text,\n",
    "                        'sentiment': 'error',\n",
    "                        'confidence': 0.0,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_dataset(self, dataset, text_column='text', sample_size=None):\n",
    "        \"\"\"Analyze sentiment for an entire dataset\"\"\"\n",
    "        print(f\"Analyzing dataset with {len(dataset)} examples...\")\n",
    "        \n",
    "        if sample_size:\n",
    "            dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
    "            print(f\"Using sample of {len(dataset)} examples\")\n",
    "        \n",
    "        texts = [ex[text_column] for ex in dataset]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = self.predict_batch(texts)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        sentiments = [r['sentiment'] for r in results if r['sentiment'] != 'error']\n",
    "        sentiment_counts = Counter(sentiments)\n",
    "        \n",
    "        errors = [r for r in results if r['sentiment'] == 'error']\n",
    "        \n",
    "        print(f\"\\nAnalysis Results:\")\n",
    "        print(f\"  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"  Speed: {len(texts)/processing_time:.1f} examples/second\")\n",
    "        print(f\"  Sentiment distribution: {dict(sentiment_counts)}\")\n",
    "        print(f\"  Errors: {len(errors)}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"  Sample errors: {[e.get('error', 'Unknown') for e in errors[:3]]}\")\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'sentiment_distribution': sentiment_counts,\n",
    "            'processing_time': processing_time,\n",
    "            'errors': errors\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get information about the loaded model\"\"\"\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'max_length': self.max_length,\n",
    "            'batch_size': self.batch_size,\n",
    "            'device': str(device),\n",
    "            'tokenizer_vocab_size': len(self.tokenizer) if self.tokenizer else None\n",
    "        }\n",
    "\n",
    "# Create production analyzer\n",
    "production_analyzer = ProductionSentimentAnalyzer()\n",
    "\n",
    "# Test with various inputs\n",
    "test_cases = [\n",
    "    \"I love this product!\",  # Normal positive\n",
    "    \"This is terrible\",      # Normal negative  \n",
    "    \"\",                      # Empty\n",
    "    None,                    # None\n",
    "    123,                     # Number\n",
    "    \"   \",                   # Whitespace only\n",
    "    \"A\" * 1000,             # Very long text\n",
    "    \"üòä Great! üëç\",          # With emojis\n",
    "]\n",
    "\n",
    "print(\"Testing Production Analyzer:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for test_text in test_cases:\n",
    "    result = production_analyzer.predict_single(test_text)\n",
    "    print(f\"Input: {repr(test_text)}\")\n",
    "    print(f\"  ‚Üí Sentiment: {result['sentiment']}, Confidence: {result['confidence']:.3f}\")\n",
    "    if 'error' in result:\n",
    "        print(f\"  ‚Üí Error: {result['error']}\")\n",
    "    print()\n",
    "\n",
    "# Test batch processing\n",
    "print(\"\\nBatch Processing Test:\")\n",
    "batch_texts = [ex['text'] for ex in test_set.select(range(20))]\n",
    "batch_results = production_analyzer.predict_batch(batch_texts)\n",
    "\n",
    "positive_count = sum(1 for r in batch_results if r['sentiment'] == 'positive')\n",
    "negative_count = sum(1 for r in batch_results if r['sentiment'] == 'negative')\n",
    "\n",
    "print(f\"Processed {len(batch_results)} texts:\")\n",
    "print(f\"  Positive: {positive_count}\")\n",
    "print(f\"  Negative: {negative_count}\")\n",
    "\n",
    "# Show model info\n",
    "print(f\"\\nModel Information:\")\n",
    "model_info = production_analyzer.get_model_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Final Performance Benchmark\n",
    "\n",
    "Let's create a comprehensive benchmark of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_benchmark():\n",
    "    \"\"\"Run comprehensive benchmark of the sentiment analysis system\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE SENTIMENT ANALYSIS BENCHMARK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different dataset sizes\n",
    "    test_sizes = [10, 50, 100, 500]\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        print(f\"\\nBenchmarking with {size} examples...\")\n",
    "        \n",
    "        # Get test data\n",
    "        test_data = test_set.select(range(min(size, len(test_set))))\n",
    "        \n",
    "        # Run analysis\n",
    "        start_time = time.time()\n",
    "        results = production_analyzer.analyze_dataset(\n",
    "            test_data, \n",
    "            sample_size=size\n",
    "        )\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate accuracy if we have true labels\n",
    "        if 'label' in test_data[0]:\n",
    "            true_labels = [ex['label'] for ex in test_data]\n",
    "            pred_labels = [\n",
    "                1 if r['sentiment'] == 'positive' else 0 \n",
    "                for r in results['results']\n",
    "                if r['sentiment'] != 'error'\n",
    "            ]\n",
    "            \n",
    "            if len(pred_labels) == len(true_labels):\n",
    "                accuracy = accuracy_score(true_labels, pred_labels)\n",
    "            else:\n",
    "                accuracy = None\n",
    "        else:\n",
    "            accuracy = None\n",
    "        \n",
    "        benchmark_results.append({\n",
    "            'size': size,\n",
    "            'total_time': total_time,\n",
    "            'examples_per_second': size / total_time,\n",
    "            'accuracy': accuracy,\n",
    "            'errors': len(results['errors']),\n",
    "            'positive_ratio': results['sentiment_distribution'].get('positive', 0) / size\n",
    "        })\n",
    "        \n",
    "        print(f\"  Time: {total_time:.2f}s, Speed: {size/total_time:.1f} ex/s, Accuracy: {accuracy:.3f if accuracy else 'N/A'}\")\n",
    "    \n",
    "    # Create benchmark visualization\n",
    "    df_benchmark = pd.DataFrame(benchmark_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Sentiment Analysis System Benchmark', fontsize=16)\n",
    "    \n",
    "    # Processing speed vs dataset size\n",
    "    axes[0, 0].plot(df_benchmark['size'], df_benchmark['examples_per_second'], 'bo-')\n",
    "    axes[0, 0].set_xlabel('Dataset Size')\n",
    "    axes[0, 0].set_ylabel('Examples per Second')\n",
    "    axes[0, 0].set_title('Processing Speed vs Dataset Size')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Total processing time\n",
    "    axes[0, 1].plot(df_benchmark['size'], df_benchmark['total_time'], 'ro-')\n",
    "    axes[0, 1].set_xlabel('Dataset Size')\n",
    "    axes[0, 1].set_ylabel('Total Time (seconds)')\n",
    "    axes[0, 1].set_title('Total Processing Time')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy (if available)\n",
    "    if any(r['accuracy'] for r in benchmark_results if r['accuracy'] is not None):\n",
    "        valid_accuracy = [(r['size'], r['accuracy']) for r in benchmark_results if r['accuracy'] is not None]\n",
    "        sizes, accuracies = zip(*valid_accuracy)\n",
    "        axes[1, 0].plot(sizes, accuracies, 'go-')\n",
    "        axes[1, 0].set_xlabel('Dataset Size')\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].set_title('Accuracy vs Dataset Size')\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error rate\n",
    "    error_rates = [r['errors'] / r['size'] * 100 for r in benchmark_results]\n",
    "    axes[1, 1].bar(df_benchmark['size'], error_rates, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Dataset Size')\n",
    "    axes[1, 1].set_ylabel('Error Rate (%)')\n",
    "    axes[1, 1].set_title('Error Rate vs Dataset Size')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n\\nBenchmark Summary:\")\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Average processing speed: {np.mean([r['examples_per_second'] for r in benchmark_results]):.1f} examples/second\")\n",
    "    \n",
    "    if any(r['accuracy'] for r in benchmark_results if r['accuracy'] is not None):\n",
    "        valid_accuracies = [r['accuracy'] for r in benchmark_results if r['accuracy'] is not None]\n",
    "        print(f\"Average accuracy: {np.mean(valid_accuracies):.3f}\")\n",
    "    \n",
    "    total_errors = sum(r['errors'] for r in benchmark_results)\n",
    "    total_examples = sum(r['size'] for r in benchmark_results)\n",
    "    print(f\"Overall error rate: {total_errors/total_examples*100:.2f}%\")\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark_results = comprehensive_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary and Key Learnings\n",
    "\n",
    "Congratulations! You've successfully built a complete sentiment analysis system that demonstrates the integration of:\n",
    "\n",
    "### üéØ **What We Accomplished**\n",
    "\n",
    "1. **Data Pipeline**: Created a robust data processing pipeline using the datasets library\n",
    "2. **Model Comparison**: Systematically compared different transformer models\n",
    "3. **Tokenization Analysis**: Deep dive into how different tokenizers handle text\n",
    "4. **Error Handling**: Built production-ready code with comprehensive error handling\n",
    "5. **Performance Optimization**: Implemented batch processing and caching\n",
    "6. **Comprehensive Evaluation**: Created thorough benchmarking and visualization\n",
    "\n",
    "### üìö **Key Integration Points**\n",
    "\n",
    "- **Transformers + Datasets**: Seamless integration for model training and evaluation\n",
    "- **Tokenizers + Models**: Understanding how tokenization affects model performance\n",
    "- **Datasets + Processing**: Efficient data handling for large-scale processing\n",
    "- **All Three Together**: Building end-to-end ML pipelines\n",
    "\n",
    "### üöÄ **Production-Ready Features**\n",
    "\n",
    "- ‚úÖ Error handling for edge cases (empty text, None values, etc.)\n",
    "- ‚úÖ Batch processing for efficiency\n",
    "- ‚úÖ Memory optimization\n",
    "- ‚úÖ Performance benchmarking\n",
    "- ‚úÖ Comprehensive logging and monitoring\n",
    "- ‚úÖ Flexible model switching\n",
    "\n",
    "### üîç **Key Insights Discovered**\n",
    "\n",
    "1. **Model Performance Varies**: Different models excel at different types of text\n",
    "2. **Tokenization Matters**: Tokenizer choice significantly impacts results\n",
    "3. **Batch Processing**: Much more efficient than individual processing\n",
    "4. **Edge Cases**: Production systems must handle all input types gracefully\n",
    "5. **Speed vs Accuracy**: There's always a trade-off to consider\n",
    "\n",
    "### üìà **What's Next?**\n",
    "\n",
    "This mini-project provides a solid foundation for:\n",
    "- **Fine-tuning** models on custom datasets (Notebook 05)\n",
    "- **Advanced training techniques** (Notebook 06)\n",
    "- **Specialized applications** like summarization (Notebook 07) and QA (Notebook 08)\n",
    "- **Advanced techniques** like LoRA and RLHF (Notebooks 09-10)\n",
    "\n",
    "### üéì **Skills Mastered**\n",
    "\n",
    "- **Hugging Face Ecosystem**: Confident usage of transformers, tokenizers, and datasets\n",
    "- **Production ML**: Building robust, scalable ML systems\n",
    "- **Performance Optimization**: Efficient processing and resource management\n",
    "- **Model Evaluation**: Comprehensive testing and benchmarking\n",
    "- **Error Handling**: Graceful handling of edge cases and errors\n",
    "\n",
    "**Great job!** You've successfully combined all the concepts from the first three notebooks into a comprehensive, production-ready sentiment analysis system. This foundation will serve you well as we dive into more advanced topics in the upcoming notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to continue your journey? Head to **Notebook 05: Fine-Tuning with Trainer API** to learn how to train your own models!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
