{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/02_tokenizers.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/02_tokenizers.ipynb)\n",
    "\n",
    "# 02 - Tokenizers: The Foundation of Transformer Models\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What tokenization is and why it's crucial for NLP\n",
    "- Different tokenization strategies (word, subword, character)\n",
    "- How Hugging Face tokenizers work\n",
    "- Special tokens and their purposes\n",
    "- Handling padding, truncation, and attention masks\n",
    "- Advanced tokenizer features and customization\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of converting text into a sequence of tokens (usually integers) that machine learning models can understand. It's the bridge between human-readable text and numerical data that models process.\n",
    "\n",
    "## Why is Tokenization Important?\n",
    "\n",
    "- **Neural networks work with numbers**, not text\n",
    "- **Vocabulary management**: Handling unknown words and rare terms\n",
    "- **Efficiency**: Balancing vocabulary size with representation quality\n",
    "- **Consistency**: Ensuring the same text is always tokenized the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, BertTokenizer, GPT2Tokenizer, T5Tokenizer,\n",
    "    RobertaTokenizer, DistilBertTokenizer\n",
    ")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "    print(\"Environment variables loaded from .env.local\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env.local loading\")\n",
    "\n",
    "# Credential management function\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Get API key from environment or Colab secrets.\"\"\"\n",
    "    try:\n",
    "        # Try to import Colab userdata (only available in Colab)\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(key_name)\n",
    "    except (ImportError, Exception):\n",
    "        # Fall back to local environment variable\n",
    "        api_key = os.getenv(key_name)\n",
    "        if not api_key:\n",
    "            print(f\"Info: {key_name} not found. Public models will work without authentication.\")\n",
    "            return None\n",
    "        return api_key\n",
    "\n",
    "# Device detection function\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for training/inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup authentication and device\n",
    "hf_token = get_api_key('HF_TOKEN')\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"Hugging Face token configured\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"\\n=== Setup Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif device.type == 'mps':\n",
    "    print(\"Apple Silicon GPU (MPS) detected\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Tokenization Concepts\n",
    "\n",
    "Let's start with understanding different levels of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"Hello! How are you doing today? I'm learning about tokenization.\"\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Text length: {len(sample_text)} characters\\n\")\n",
    "\n",
    "# Character-level tokenization\n",
    "char_tokens = list(sample_text)\n",
    "print(f\"Character tokens: {char_tokens[:20]}...\")  # Show first 20\n",
    "print(f\"Number of character tokens: {len(char_tokens)}\\n\")\n",
    "\n",
    "# Simple word-level tokenization (split by spaces)\n",
    "word_tokens = sample_text.split()\n",
    "print(f\"Word tokens: {word_tokens}\")\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\\n\")\n",
    "\n",
    "# Punctuation handling\n",
    "import string\n",
    "text_no_punct = sample_text.translate(str.maketrans('', '', string.punctuation))\n",
    "word_tokens_no_punct = text_no_punct.split()\n",
    "print(f\"Word tokens (no punctuation): {word_tokens_no_punct}\")\n",
    "print(f\"Number of tokens without punctuation: {len(word_tokens_no_punct)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hugging Face Tokenizers in Action\n",
    "\n",
    "Let's explore how different transformer models tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers\n",
    "tokenizers = {\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "    \"DistilBERT\": AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "}\n",
    "\n",
    "# Test text with some challenging words\n",
    "test_text = \"The unhappiness of the students was unfathomable.\"\n",
    "\n",
    "print(f\"Test text: {test_text}\\n\")\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"{name} tokenization:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Number of tokens: {len(tokens)}\")\n",
    "    print(f\"  Vocabulary size: {len(tokenizer)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Subword Tokenization\n",
    "\n",
    "Subword tokenization is key to modern transformer models. Let's see how it handles out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer for detailed analysis\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Test words with varying complexity\n",
    "test_words = [\n",
    "    \"hello\",           # Common word\n",
    "    \"tokenization\",    # Technical term\n",
    "    \"unhappiness\",     # Complex word with prefix/suffix\n",
    "    \"antidisestablishmentarianism\",  # Very long word\n",
    "    \"COVID-19\",        # Recent term\n",
    "    \"ChatGPT\",         # Proper noun/brand\n",
    "    \"unfathomable\"     # Less common word\n",
    "]\n",
    "\n",
    "print(\"Subword Tokenization Analysis:\\n\")\n",
    "\n",
    "for word in test_words:\n",
    "    tokens = bert_tokenizer.tokenize(word)\n",
    "    token_ids = bert_tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Token IDs: {token_ids}\")\n",
    "    print(f\"  Number of tokens: {len(tokens)}\")\n",
    "    print(f\"  Reconstructed: '{bert_tokenizer.decode(token_ids)}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of tokenization differences\n",
    "def compare_tokenization(text, tokenizers_dict):\n",
    "    results = []\n",
    "    \n",
    "    for name, tokenizer in tokenizers_dict.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Tokens': len(tokens),\n",
    "            'Vocab_Size': len(tokenizer)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare tokenization for different texts\n",
    "texts = [\n",
    "    \"Hello world\",\n",
    "    \"The quick brown fox jumps\",\n",
    "    \"Antidisestablishmentarianism is difficult\",\n",
    "    \"COVID-19 pandemic affected everyone\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Tokenization Comparison Across Different Models', fontsize=16)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    df = compare_tokenization(text, tokenizers)\n",
    "    \n",
    "    axes[row, col].bar(df['Model'], df['Tokens'])\n",
    "    axes[row, col].set_title(f'Text: \"{text[:30]}...\"' if len(text) > 30 else f'Text: \"{text}\"')\n",
    "    axes[row, col].set_ylabel('Number of Tokens')\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Special Tokens\n",
    "\n",
    "Special tokens serve specific purposes in transformer models. Let's explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine special tokens for different models\n",
    "def analyze_special_tokens(tokenizer, model_name):\n",
    "    print(f\"{model_name} Special Tokens:\")\n",
    "    \n",
    "    special_tokens = {\n",
    "        'CLS': getattr(tokenizer, 'cls_token', None),\n",
    "        'SEP': getattr(tokenizer, 'sep_token', None),\n",
    "        'PAD': getattr(tokenizer, 'pad_token', None),\n",
    "        'UNK': getattr(tokenizer, 'unk_token', None),\n",
    "        'MASK': getattr(tokenizer, 'mask_token', None),\n",
    "        'BOS': getattr(tokenizer, 'bos_token', None),\n",
    "        'EOS': getattr(tokenizer, 'eos_token', None)\n",
    "    }\n",
    "    \n",
    "    for token_type, token in special_tokens.items():\n",
    "        if token is not None:\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            print(f\"  {token_type}: '{token}' (ID: {token_id})\")\n",
    "        else:\n",
    "            print(f\"  {token_type}: Not available\")\n",
    "    print()\n",
    "\n",
    "# Analyze special tokens for each model\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    analyze_special_tokens(tokenizer, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how special tokens are used\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello world\"\n",
    "\n",
    "# Tokenize without special tokens\n",
    "tokens_no_special = bert_tokenizer.tokenize(text)\n",
    "ids_no_special = bert_tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "# Tokenize with special tokens\n",
    "tokens_with_special = bert_tokenizer.tokenize(text, add_special_tokens=True)\n",
    "ids_with_special = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(f\"Original text: '{text}'\\n\")\n",
    "\n",
    "print(\"Without special tokens:\")\n",
    "print(f\"  Tokens: {tokens_no_special}\")\n",
    "print(f\"  IDs: {ids_no_special}\")\n",
    "print(f\"  Decoded: '{bert_tokenizer.decode(ids_no_special)}'\\n\")\n",
    "\n",
    "print(\"With special tokens:\")\n",
    "print(f\"  Tokens: {tokens_with_special}\")\n",
    "print(f\"  IDs: {ids_with_special}\")\n",
    "print(f\"  Decoded: '{bert_tokenizer.decode(ids_with_special)}'\")\n",
    "\n",
    "# Show what each ID represents\n",
    "print(\"\\nToken ID breakdown:\")\n",
    "for i, token_id in enumerate(ids_with_special):\n",
    "    token = bert_tokenizer.convert_ids_to_tokens(token_id)\n",
    "    print(f\"  Position {i}: ID {token_id} -> '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Padding, Truncation, and Attention Masks\n",
    "\n",
    "When processing batches of text, we need to handle sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create texts of different lengths\n",
    "texts = [\n",
    "    \"Short\",\n",
    "    \"This is a medium length sentence.\",\n",
    "    \"This is a much longer sentence that contains more words and information about various topics.\",\n",
    "    \"Very long sentence that goes on and on and contains lots of information about many different subjects and concepts that might be relevant to natural language processing tasks.\"\n",
    "]\n",
    "\n",
    "print(\"Original texts:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"{i+1}. '{text}' (length: {len(text.split())} words)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Tokenize without padding/truncation\n",
    "print(\"\\nTokenization without padding:\")\n",
    "for i, text in enumerate(texts):\n",
    "    tokens = bert_tokenizer.encode(text)\n",
    "    print(f\"{i+1}. Length: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different padding strategies\n",
    "max_length = 20\n",
    "\n",
    "padding_strategies = {\n",
    "    \"No padding\": {\"padding\": False, \"truncation\": False},\n",
    "    \"Pad to max_length\": {\"padding\": \"max_length\", \"max_length\": max_length, \"truncation\": True},\n",
    "    \"Pad to longest\": {\"padding\": \"longest\", \"truncation\": True, \"max_length\": max_length}\n",
    "}\n",
    "\n",
    "for strategy_name, params in padding_strategies.items():\n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    try:\n",
    "        result = bert_tokenizer(texts, return_tensors=\"pt\", **params)\n",
    "        \n",
    "        print(f\"  Input IDs shape: {result['input_ids'].shape}\")\n",
    "        print(f\"  Attention mask shape: {result['attention_mask'].shape}\")\n",
    "        \n",
    "        # Show first example in detail\n",
    "        print(f\"  First example input_ids: {result['input_ids'][0].tolist()}\")\n",
    "        print(f\"  First example attention_mask: {result['attention_mask'][0].tolist()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention masks\n",
    "texts = [\"Hello\", \"Hello world\", \"Hello world how are you\"]\n",
    "\n",
    "# Tokenize with padding\n",
    "encoded = bert_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Attention Mask Visualization:\")\n",
    "print(\"1 = attend to this token, 0 = ignore (padding)\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Create heatmap of attention masks\n",
    "attention_masks = encoded['attention_mask'].numpy()\n",
    "input_ids = encoded['input_ids'].numpy()\n",
    "\n",
    "# Get token strings for labels\n",
    "token_labels = []\n",
    "for i in range(attention_masks.shape[0]):\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "    token_labels.append(tokens)\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(attention_masks, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cmap='RdYlBu_r',\n",
    "            yticklabels=[f\"Text {i+1}\" for i in range(len(texts))],\n",
    "            xticklabels=token_labels[0],  # Use tokens from first example\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Attention Masks for Batched Input')\n",
    "ax.set_xlabel('Token Position')\n",
    "ax.set_ylabel('Example')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show detailed breakdown\n",
    "for i, text in enumerate(texts):\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "    mask = attention_masks[i]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}: '{text}'\")\n",
    "    print(\"Token\\t\\tMask\")\n",
    "    print(\"-\" * 20)\n",
    "    for token, mask_val in zip(tokens, mask):\n",
    "        status = \"ATTEND\" if mask_val == 1 else \"IGNORE\"\n",
    "        print(f\"{token:<15} {mask_val} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Tokenizer Features\n",
    "\n",
    "Let's explore some advanced features of Hugging Face tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer configuration\n",
    "def demonstrate_tokenizer_options(text):\n",
    "    \"\"\"Show different tokenizer configuration options\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        \"Default\": {},\n",
    "        \"No special tokens\": {\"add_special_tokens\": False},\n",
    "        \"Return tensors\": {\"return_tensors\": \"pt\"},\n",
    "        \"Return attention mask\": {\"return_attention_mask\": True},\n",
    "        \"Return token type IDs\": {\"return_token_type_ids\": True},\n",
    "        \"Return offsets\": {\"return_offsets_mapping\": True},\n",
    "        \"Truncate to 10\": {\"max_length\": 10, \"truncation\": True},\n",
    "        \"Pad to 15\": {\"max_length\": 15, \"padding\": \"max_length\"}\n",
    "    }\n",
    "    \n",
    "    print(f\"Text: '{text}'\\n\")\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"{config_name}:\")\n",
    "        try:\n",
    "            result = bert_tokenizer(text, **config)\n",
    "            \n",
    "            if isinstance(result, dict):\n",
    "                for key, value in result.items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        print(f\"  {key}: {value.tolist()}\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(f\"  Result: {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "        print()\n",
    "\n",
    "demonstrate_tokenizer_options(\"Hello world, how are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Fast vs Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare fast vs slow tokenizers\n",
    "text_list = [\"This is a test sentence for tokenization speed.\"] * 1000\n",
    "\n",
    "# Fast tokenizer (default)\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Slow tokenizer (Python-based)\n",
    "try:\n",
    "    slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=False)\n",
    "    \n",
    "    # Time fast tokenizer\n",
    "    start_time = time.time()\n",
    "    fast_result = fast_tokenizer(text_list, padding=True, truncation=True)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Time slow tokenizer\n",
    "    start_time = time.time()\n",
    "    slow_result = slow_tokenizer(text_list, padding=True, truncation=True)\n",
    "    slow_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Fast tokenizer time: {fast_time:.4f} seconds\")\n",
    "    print(f\"Slow tokenizer time: {slow_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {slow_time/fast_time:.2f}x\")\n",
    "    \n",
    "    # Check if results are identical\n",
    "    identical = (fast_result['input_ids'] == slow_result['input_ids'])\n",
    "    print(f\"Results identical: {identical}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load slow tokenizer: {e}\")\n",
    "    print(\"Fast tokenizers are the default and recommended option.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Working with Multiple Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual tokenizer\n",
    "multilingual_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Test texts in different languages\n",
    "multilingual_texts = {\n",
    "    \"English\": \"Hello, how are you today?\",\n",
    "    \"Spanish\": \"Hola, ¬øc√≥mo est√°s hoy?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous aujourd'hui?\",\n",
    "    \"German\": \"Hallo, wie geht es dir heute?\",\n",
    "    \"Chinese\": \"‰Ω†Â•ΩÔºå‰Ω†‰ªäÂ§©ÊÄé‰πàÊ†∑Ôºü\",\n",
    "    \"Japanese\": \"„Åì„Çì„Å´„Å°„ÅØ„ÄÅ‰ªäÊó•„ÅØ„ÅÑ„Åã„Åå„Åß„Åô„ÅãÔºü\"\n",
    "}\n",
    "\n",
    "print(\"Multilingual Tokenization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for language, text in multilingual_texts.items():\n",
    "    tokens = multilingual_tokenizer.tokenize(text)\n",
    "    token_ids = multilingual_tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\n{language}:\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Number of tokens: {len(tokens)}\")\n",
    "    print(f\"  Reconstructed: {multilingual_tokenizer.decode(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Tokenizer Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tokenizer vocabulary\n",
    "def analyze_vocabulary(tokenizer, model_name, sample_size=1000):\n",
    "    \"\"\"Analyze tokenizer vocabulary characteristics\"\"\"\n",
    "    \n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # Sample some tokens for analysis\n",
    "    sample_tokens = list(vocab.keys())[:sample_size]\n",
    "    \n",
    "    # Analyze token characteristics\n",
    "    token_lengths = [len(token) for token in sample_tokens]\n",
    "    special_tokens = [token for token in sample_tokens if token.startswith('[') or token.startswith('<') or token.startswith('ƒ†')]\n",
    "    \n",
    "    print(f\"\\n{model_name} Vocabulary Analysis:\")\n",
    "    print(f\"  Total vocabulary size: {vocab_size:,}\")\n",
    "    print(f\"  Average token length (first {sample_size}): {np.mean(token_lengths):.2f}\")\n",
    "    print(f\"  Min token length: {min(token_lengths)}\")\n",
    "    print(f\"  Max token length: {max(token_lengths)}\")\n",
    "    print(f\"  Special tokens in sample: {len(special_tokens)}\")\n",
    "    \n",
    "    # Show some example tokens\n",
    "    print(f\"  Sample tokens: {sample_tokens[:10]}\")\n",
    "    \n",
    "    return {\n",
    "        'vocab_size': vocab_size,\n",
    "        'avg_token_length': np.mean(token_lengths),\n",
    "        'token_lengths': token_lengths\n",
    "    }\n",
    "\n",
    "# Analyze different tokenizers\n",
    "vocab_stats = {}\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    vocab_stats[name] = analyze_vocabulary(tokenizer, name)\n",
    "\n",
    "# Create comparison plot\n",
    "models = list(vocab_stats.keys())\n",
    "vocab_sizes = [vocab_stats[model]['vocab_size'] for model in models]\n",
    "avg_lengths = [vocab_stats[model]['avg_token_length'] for model in models]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Vocabulary sizes\n",
    "ax1.bar(models, vocab_sizes)\n",
    "ax1.set_title('Vocabulary Sizes')\n",
    "ax1.set_ylabel('Number of tokens')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average token lengths\n",
    "ax2.bar(models, avg_lengths)\n",
    "ax2.set_title('Average Token Length')\n",
    "ax2.set_ylabel('Characters per token')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Common Tokenization Pitfalls and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common issues and their solutions\n",
    "def demonstrate_common_issues():\n",
    "    \"\"\"Show common tokenization issues and how to handle them\"\"\"\n",
    "    \n",
    "    issues = {\n",
    "        \"Empty strings\": [\"\", \"   \", \"\\n\\t\"],\n",
    "        \"Very long text\": [\"word \" * 1000],  # 1000 words\n",
    "        \"Special characters\": [\"Hello @user! Check out https://example.com üòä\"],\n",
    "        \"Mixed languages\": [\"Hello ‰∏ñÁïå bonjour\"],\n",
    "        \"Numbers and dates\": [\"Born on 1990-01-01, phone: +1-555-123-4567\"]\n",
    "    }\n",
    "    \n",
    "    for issue_type, examples in issues.items():\n",
    "        print(f\"\\n{issue_type.upper()}:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for text in examples:\n",
    "            display_text = repr(text) if len(text) < 50 else f\"{repr(text[:47])}...\"\n",
    "            print(f\"\\nText: {display_text}\")\n",
    "            \n",
    "            try:\n",
    "                # Basic tokenization\n",
    "                tokens = bert_tokenizer.tokenize(text)\n",
    "                print(f\"Tokens ({len(tokens)}): {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "                \n",
    "                # Safe tokenization with limits\n",
    "                safe_encoded = bert_tokenizer(\n",
    "                    text,\n",
    "                    max_length=50,\n",
    "                    truncation=True,\n",
    "                    padding=False,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                print(f\"Safe encoding shape: {safe_encoded['input_ids'].shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "demonstrate_common_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Best Practices and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices demonstration\n",
    "def tokenization_best_practices():\n",
    "    \"\"\"Demonstrate tokenization best practices\"\"\"\n",
    "    \n",
    "    print(\"TOKENIZATION BEST PRACTICES:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Practice 1: Always handle edge cases\n",
    "    print(\"\\n1. Handle edge cases safely:\")\n",
    "    \n",
    "    def safe_tokenize(text, tokenizer, max_length=512):\n",
    "        \"\"\"Safely tokenize text with error handling\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        if len(text.strip()) == 0:\n",
    "            text = \"[EMPTY]\"\n",
    "        \n",
    "        return tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # Test with problematic inputs\n",
    "    test_inputs = [\"\", None, 12345, \"Normal text\"]\n",
    "    \n",
    "    for inp in test_inputs:\n",
    "        try:\n",
    "            result = safe_tokenize(inp, bert_tokenizer)\n",
    "            print(f\"  Input: {repr(inp)} -> Tokens: {result['input_ids'].shape[1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Input: {repr(inp)} -> Error: {e}\")\n",
    "    \n",
    "    # Practice 2: Batch processing\n",
    "    print(\"\\n2. Use batch processing for efficiency:\")\n",
    "    \n",
    "    texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "    \n",
    "    # Inefficient: one by one\n",
    "    start_time = time.time()\n",
    "    individual_results = [bert_tokenizer(text, return_tensors=\"pt\") for text in texts]\n",
    "    individual_time = time.time() - start_time\n",
    "    \n",
    "    # Efficient: batch processing\n",
    "    start_time = time.time()\n",
    "    batch_result = bert_tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "    batch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Individual processing: {individual_time:.6f} seconds\")\n",
    "    print(f\"  Batch processing: {batch_time:.6f} seconds\")\n",
    "    print(f\"  Speedup: {individual_time/batch_time:.2f}x\")\n",
    "    \n",
    "    # Practice 3: Memory considerations\n",
    "    print(\"\\n3. Consider memory usage:\")\n",
    "    \n",
    "    # Memory-efficient approach for large datasets\n",
    "    def memory_efficient_tokenization(texts, tokenizer, batch_size=32):\n",
    "        \"\"\"Tokenize large datasets in batches\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_result = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            results.append(batch_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Example with larger dataset\n",
    "    large_dataset = [f\"Sample text {i}\" for i in range(100)]\n",
    "    batched_results = memory_efficient_tokenization(large_dataset, bert_tokenizer)\n",
    "    print(f\"  Processed {len(large_dataset)} texts in {len(batched_results)} batches\")\n",
    "\n",
    "tokenization_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Tokenization Fundamentals**: Understanding what tokenization is and why it's important\n",
    "2. **Hugging Face Tokenizers**: How different models tokenize text differently\n",
    "3. **Subword Tokenization**: How modern tokenizers handle complex and unknown words\n",
    "4. **Special Tokens**: Understanding CLS, SEP, PAD, UNK, MASK and their purposes\n",
    "5. **Padding and Attention Masks**: Handling variable-length sequences in batches\n",
    "6. **Advanced Features**: Tokenizer configuration options and performance considerations\n",
    "7. **Multilingual Support**: Working with multiple languages\n",
    "8. **Vocabulary Analysis**: Understanding tokenizer vocabularies\n",
    "9. **Common Pitfalls**: Issues you might encounter and how to handle them\n",
    "10. **Best Practices**: Efficient and safe tokenization patterns\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Subword tokenization** balances vocabulary size with representation quality\n",
    "- **Special tokens** serve important roles in model architecture\n",
    "- **Attention masks** are crucial for handling padded sequences\n",
    "- **Different models** use different tokenization strategies\n",
    "- **Batch processing** is more efficient than individual tokenization\n",
    "- **Always handle edge cases** in production code\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 03**: Working with the datasets library\n",
    "- **Notebook 04**: Mini-project integrating tokenization with datasets\n",
    "\n",
    "Understanding tokenization is fundamental to working effectively with transformer models. The concepts learned here will be essential for all subsequent notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}