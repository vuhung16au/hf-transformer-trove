{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/09_peft_lora_qlora.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/09_peft_lora_qlora.ipynb)\n",
    "\n",
    "# 09 - PEFT LoRA QLoRA: Parameter Efficient Fine-Tuning\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Parameter Efficient Fine-Tuning (PEFT) concepts\n",
    "- Low-Rank Adaptation (LoRA) technique and implementation\n",
    "- Quantized LoRA (QLoRA) for memory-efficient training\n",
    "- Comparing full fine-tuning vs PEFT approaches\n",
    "- Practical implementation with PEFT library\n",
    "- Performance and memory optimization strategies\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of transformers and fine-tuning (refer to [Notebook 05](05_fine_tuning_trainer.ipynb))\n",
    "- Understanding of model architectures\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **PEFT Introduction**: Concepts and motivation\n",
    "2. **LoRA Theory**: Low-rank adaptation mathematics\n",
    "3. **LoRA Implementation**: Using PEFT library\n",
    "4. **QLoRA Technique**: Quantization + LoRA\n",
    "5. **Model Comparison**: Full vs PEFT fine-tuning\n",
    "6. **Memory Analysis**: Efficiency measurements\n",
    "7. **Advanced Techniques**: Different PEFT methods\n",
    "8. **Production Usage**: Best practices and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Parameter Efficient Fine-Tuning\n",
    "\n",
    "Parameter Efficient Fine-Tuning (PEFT) addresses the challenge of fine-tuning large language models efficiently:\n",
    "\n",
    "### The Problem:\n",
    "- **Full Fine-tuning**: Updates all model parameters (expensive, memory-intensive)\n",
    "- **Large Models**: Billions of parameters require massive computational resources\n",
    "- **Storage**: Each fine-tuned model requires full parameter storage\n",
    "\n",
    "### PEFT Solutions:\n",
    "- **LoRA (Low-Rank Adaptation)**: Learns low-rank decomposition of weight updates\n",
    "- **QLoRA**: Combines quantization with LoRA for even greater efficiency\n",
    "- **AdaLoRA**: Adaptive budget allocation for LoRA\n",
    "- **Prefix Tuning**: Only fine-tune prefix tokens\n",
    "\n",
    "### Benefits:\n",
    "- **Memory Efficient**: Significantly reduced memory requirements\n",
    "- **Storage Efficient**: Only store adapter weights (few MB vs GB)\n",
    "- **Fast Training**: Fewer parameters to update\n",
    "- **Modularity**: Can combine multiple adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, TaskType, get_peft_model, \n",
    "    prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device detection\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for PEFT efficiency)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(\"\\nüìö Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PEFT available: {'‚úÖ' if 'peft' in globals() else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive notebook, we explored Parameter Efficient Fine-Tuning techniques:\n",
    "\n",
    "### üéØ **What We Accomplished**\n",
    "1. **PEFT Concepts**: Understanding the motivation and benefits\n",
    "2. **LoRA Theory**: Low-rank adaptation mathematics and intuition\n",
    "3. **Implementation**: Practical LoRA usage with PEFT library\n",
    "4. **QLoRA**: Combining quantization with LoRA for efficiency\n",
    "5. **Comparisons**: Full fine-tuning vs PEFT approaches\n",
    "6. **Memory Analysis**: Understanding resource requirements\n",
    "7. **Advanced Methods**: Different PEFT techniques and trade-offs\n",
    "\n",
    "### üîë **Key Concepts Mastered**\n",
    "- **Low-Rank Adaptation**: Decomposing weight updates into smaller matrices\n",
    "- **Quantization**: Reducing precision for memory efficiency\n",
    "- **Adapter Modules**: Modular approach to fine-tuning\n",
    "- **Memory Efficiency**: Dramatic reduction in computational requirements\n",
    "- **Task-Specific Adaptation**: Tailoring models for specific tasks\n",
    "\n",
    "### üìà **Best Practices Learned**\n",
    "- **Rank Selection**: Choose appropriate rank based on model size and task\n",
    "- **Target Modules**: Select which layers to adapt for optimal performance\n",
    "- **Quantization Strategy**: Balance efficiency and performance\n",
    "- **Monitoring**: Track memory usage and training metrics\n",
    "- **Modular Design**: Design adapters for reusability and composability\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "- **Notebook 10**: LLMs and Reinforcement Learning from Human Feedback\n",
    "- **Documentation**: [PEFT Best Practices](../docs/peft-best-practices.md)\n",
    "- **External Resources**: [PEFT Documentation](https://huggingface.co/docs/peft/index)\n",
    "\n",
    "PEFT techniques like LoRA and QLoRA have democratized fine-tuning of large language models, making it accessible to researchers and practitioners with limited computational resources!\n",
    "\n",
    "---\n",
    "\n",
    "*Ready for the final notebook? Head to **Notebook 10: LLMs RLHF** to learn about aligning models with human feedback!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}