{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.8/vLLM/performance_comparison.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic2.8/vLLM/performance_comparison.ipynb)\n",
    "\n",
    "# vLLM Performance Comparison and Benchmarking\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Performance characteristics of vLLM compared to baseline HF implementations\n",
    "- How to benchmark inference throughput and latency\n",
    "- Memory efficiency gains from PagedAttention\n",
    "- Optimal configurations for different use cases\n",
    "- Real-world performance implications for hate speech detection\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "- Basic understanding of transformer inference\n",
    "- Familiarity with Docker and containerization\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Completed vLLM setup guide\n",
    "\n",
    "## ðŸ“š What We'll Cover\n",
    "1. **Setup**: Environment and performance monitoring tools\n",
    "2. **Baseline Measurements**: Standard HuggingFace inference performance\n",
    "3. **vLLM Benchmarks**: Throughput and latency measurements\n",
    "4. **Memory Analysis**: PagedAttention vs standard attention\n",
    "5. **Batch Processing**: Scaling characteristics\n",
    "6. **Real-world Scenarios**: Hate speech detection at scale\n",
    "7. **Cost Analysis**: Performance vs resource trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Let's start by setting up our benchmarking environment with comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for benchmarking\n",
    "# !pip install transformers torch datasets vllm psutil GPUtil matplotlib seaborn pandas numpy\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory and system monitoring\n",
    "import psutil\n",
    "try:\n",
    "    import GPUtil\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"âš ï¸ GPUtil not available, GPU monitoring disabled\")\n",
    "\n",
    "# HuggingFace transformers for baseline comparison\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline, TextClassificationPipeline\n",
    ")\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"ðŸš€ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"ðŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"ðŸŽ Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"ðŸ’» Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup device and plotting\n",
    "device = get_device()\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"\\n=== Benchmarking Environment ===\\n\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / 1e9:.1f}GB\")\n",
    "print(f\"Ready for performance comparison! ðŸ“ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline HuggingFace Performance\n",
    "\n",
    "First, let's establish baseline performance metrics using standard HuggingFace transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineHFBenchmark:\n",
    "    \"\"\"Benchmark class for standard HuggingFace inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"cardiffnlp/twitter-roberta-base-hate-latest\"):\n",
    "        \"\"\"Initialize with preferred hate speech detection model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = get_device()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        print(f\"ðŸ¤– Loading baseline model: {model_name}\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device.type == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device.type == \"cuda\" else None\n",
    "            )\n",
    "            \n",
    "            if self.device.type != \"cuda\":\n",
    "                self.model = self.model.to(self.device)\n",
    "            \n",
    "            # Create pipeline for easier benchmarking\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=0 if self.device.type == \"cuda\" else -1,\n",
    "                return_all_scores=True\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… Baseline model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def benchmark_single_inference(self, texts: List[str], warmup_runs: int = 3) -> Dict:\n",
    "        \"\"\"Benchmark single-text inference performance.\"\"\"\n",
    "        print(f\"ðŸ”¥ Warming up with {warmup_runs} runs...\")\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = self.pipeline(texts[0])\n",
    "        \n",
    "        # Actual benchmarking\n",
    "        times = []\n",
    "        results = []\n",
    "        \n",
    "        print(f\"â±ï¸ Running benchmark on {len(texts)} texts...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            start_time = time.perf_counter()\n",
    "            result = self.pipeline(text)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            results.append(result)\n",
    "        \n",
    "        return {\n",
    "            \"times\": times,\n",
    "            \"results\": results,\n",
    "            \"avg_time\": np.mean(times),\n",
    "            \"std_time\": np.std(times),\n",
    "            \"throughput\": len(texts) / sum(times)\n",
    "        }\n",
    "    \n",
    "    def benchmark_batch_inference(self, texts: List[str], batch_sizes: List[int]) -> Dict:\n",
    "        \"\"\"Benchmark batch processing performance.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"ðŸ“¦ Testing batch size: {batch_size}\")\n",
    "            \n",
    "            # Create batches\n",
    "            batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "            \n",
    "            times = []\n",
    "            total_processed = 0\n",
    "            \n",
    "            for batch in batches:\n",
    "                if len(batch) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                start_time = time.perf_counter()\n",
    "                try:\n",
    "                    # Process batch\n",
    "                    _ = self.pipeline(batch)\n",
    "                    end_time = time.perf_counter()\n",
    "                    \n",
    "                    times.append(end_time - start_time)\n",
    "                    total_processed += len(batch)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Batch processing error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if times:\n",
    "                results[batch_size] = {\n",
    "                    \"avg_batch_time\": np.mean(times),\n",
    "                    \"total_time\": sum(times),\n",
    "                    \"throughput\": total_processed / sum(times) if sum(times) > 0 else 0,\n",
    "                    \"processed\": total_processed\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize baseline benchmark\n",
    "baseline = BaselineHFBenchmark()\n",
    "baseline.load_model()\n",
    "\n",
    "print(\"âœ… Baseline benchmark ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. vLLM Performance Testing\n",
    "\n",
    "Now let's set up vLLM benchmarking to compare against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMBenchmark:\n",
    "    \"\"\"Benchmark class for vLLM inference performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url: str = \"http://localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        self.base_url = f\"{server_url}/v1\"\n",
    "        \n",
    "    def is_server_running(self) -> bool:\n",
    "        \"\"\"Check if vLLM server is running.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/health\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except requests.RequestException:\n",
    "            return False\n",
    "    \n",
    "    def start_server_if_needed(self) -> bool:\n",
    "        \"\"\"Start vLLM server if not running.\"\"\"\n",
    "        if self.is_server_running():\n",
    "            print(\"âœ… vLLM server already running\")\n",
    "            return True\n",
    "        \n",
    "        print(\"ðŸš€ Starting vLLM server...\")\n",
    "        # Note: In a real implementation, you'd start the Docker container here\n",
    "        # For this demo, we'll assume manual server startup\n",
    "        print(\"ðŸ“‹ Please start vLLM server manually with:\")\n",
    "        print(\"   docker run --rm --runtime nvidia --gpus all -p 8000:8000 vllm-offline:latest\")\n",
    "        return False\n",
    "    \n",
    "    def benchmark_single_inference(self, texts: List[str], warmup_runs: int = 3) -> Dict:\n",
    "        \"\"\"Benchmark vLLM single-text inference.\"\"\"\n",
    "        if not self.is_server_running():\n",
    "            raise RuntimeError(\"vLLM server not running\")\n",
    "        \n",
    "        print(f\"ðŸ”¥ vLLM warmup with {warmup_runs} runs...\")\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            self._single_request(texts[0])\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        results = []\n",
    "        \n",
    "        print(f\"â±ï¸ Running vLLM benchmark on {len(texts)} texts...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            start_time = time.perf_counter()\n",
    "            result = self._single_request(text)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            results.append(result)\n",
    "        \n",
    "        return {\n",
    "            \"times\": times,\n",
    "            \"results\": results,\n",
    "            \"avg_time\": np.mean(times),\n",
    "            \"std_time\": np.std(times),\n",
    "            \"throughput\": len(texts) / sum(times)\n",
    "        }\n",
    "    \n",
    "    def benchmark_concurrent_requests(self, texts: List[str], concurrent_users: List[int]) -> Dict:\n",
    "        \"\"\"Benchmark concurrent request handling.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for num_concurrent in concurrent_users:\n",
    "            print(f\"ðŸ‘¥ Testing {num_concurrent} concurrent users\")\n",
    "            \n",
    "            # Select subset of texts for concurrent testing\n",
    "            test_texts = texts[:num_concurrent * 2]  # 2 requests per user\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=num_concurrent) as executor:\n",
    "                futures = []\n",
    "                \n",
    "                for text in test_texts:\n",
    "                    future = executor.submit(self._single_request, text)\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Wait for all requests to complete\n",
    "                completed_requests = 0\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        _ = future.result(timeout=30)\n",
    "                        completed_requests += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Request failed: {e}\")\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            results[num_concurrent] = {\n",
    "                \"total_time\": total_time,\n",
    "                \"completed_requests\": completed_requests,\n",
    "                \"throughput\": completed_requests / total_time,\n",
    "                \"avg_time_per_request\": total_time / completed_requests if completed_requests > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _single_request(self, text: str) -> Dict:\n",
    "        \"\"\"Make a single inference request to vLLM.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": \"hate-speech-detector\",\n",
    "            \"prompt\": f\"Classify this text for hate speech (safe/unsafe): {text}\\nClassification:\",\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/completions\",\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Initialize vLLM benchmark\n",
    "vllm_bench = vLLMBenchmark()\n",
    "\n",
    "# Check if server is running\n",
    "if vllm_bench.is_server_running():\n",
    "    print(\"âœ… vLLM server detected and ready!\")\n",
    "else:\n",
    "    print(\"âš ï¸ vLLM server not detected\")\n",
    "    print(\"ðŸ“‹ To start the server, run:\")\n",
    "    print(\"   cd examples/basic2.8/vLLM && python basic_usage.py\")\n",
    "    vllm_bench.start_server_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Dataset Preparation\n",
    "\n",
    "Let's prepare a comprehensive test dataset for benchmarking hate speech detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset() -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Create a comprehensive test dataset for hate speech detection benchmarking.\"\"\"\n",
    "    \n",
    "    # Safe examples (positive, neutral, and constructive content)\n",
    "    safe_examples = [\n",
    "        \"I love learning about artificial intelligence and machine learning.\",\n",
    "        \"This community is so supportive and welcoming to everyone.\",\n",
    "        \"Great job on the presentation! Very informative and well-structured.\",\n",
    "        \"I appreciate the diverse perspectives shared in this discussion.\",\n",
    "        \"Technology is advancing rapidly and creating amazing opportunities.\",\n",
    "        \"Thank you for taking the time to help me understand this concept.\",\n",
    "        \"Looking forward to collaborating with the team on this project.\",\n",
    "        \"The documentation is clear and easy to follow.\",\n",
    "        \"I enjoy participating in constructive debates and discussions.\",\n",
    "        \"This solution works well for our use case.\",\n",
    "        \"Machine learning models are becoming more accessible to developers.\",\n",
    "        \"I respect different opinions and viewpoints on this topic.\",\n",
    "        \"The research findings are fascinating and well-documented.\",\n",
    "        \"Community feedback has been incredibly valuable for improvement.\",\n",
    "        \"Open source projects benefit from collaborative contributions.\"\n",
    "    ]\n",
    "    \n",
    "    # Challenging but safe examples (edge cases that should be classified as safe)\n",
    "    challenging_safe = [\n",
    "        \"I disagree with this approach, but I understand the reasoning behind it.\",\n",
    "        \"While I don't personally like this, others might find it useful.\",\n",
    "        \"The criticism was harsh but constructive and fair.\",\n",
    "        \"This is not my preferred method, but it has merit.\",\n",
    "        \"I'm frustrated with the bug, but the team is working on it.\",\n",
    "        \"The debate was intense but remained respectful throughout.\",\n",
    "        \"Different cultures have different perspectives on this issue.\",\n",
    "        \"I question the methodology but appreciate the effort.\",\n",
    "        \"The policy has both advantages and disadvantages to consider.\",\n",
    "        \"Competition in the market drives innovation and improvement.\"\n",
    "    ]\n",
    "    \n",
    "    # Combine all safe examples\n",
    "    all_safe = safe_examples + challenging_safe\n",
    "    \n",
    "    # Create labels (all safe examples get 'safe' label)\n",
    "    labels = ['safe'] * len(all_safe)\n",
    "    \n",
    "    print(f\"ðŸ“Š Created test dataset:\")\n",
    "    print(f\"   Total examples: {len(all_safe)}\")\n",
    "    print(f\"   Safe examples: {len(all_safe)}\")\n",
    "    print(f\"   Focus: Educational content with positive/neutral sentiment\")\n",
    "    \n",
    "    return all_safe, labels\n",
    "\n",
    "# Create test dataset\n",
    "test_texts, test_labels = create_test_dataset()\n",
    "\n",
    "# Display sample texts\n",
    "print(\"\\nðŸ“ Sample test texts:\")\n",
    "for i, text in enumerate(test_texts[:5], 1):\n",
    "    print(f\"{i}. '{text}'\")\n",
    "\n",
    "print(f\"\\nâœ… Test dataset prepared with {len(test_texts)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Now let's run the comprehensive benchmark comparing HuggingFace baseline with vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"Run comprehensive performance comparison between HF baseline and vLLM.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"baseline\": {},\n",
    "        \"vllm\": {},\n",
    "        \"comparison\": {}\n",
    "    }\n",
    "    \n",
    "    # Test subset for quick benchmarking\n",
    "    benchmark_texts = test_texts[:20]  # Use first 20 texts for benchmarking\n",
    "    \n",
    "    print(\"ðŸ COMPREHENSIVE PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # 1. Single inference benchmark (HF Baseline)\n",
    "    print(\"\\nðŸ¤– Running HuggingFace baseline benchmark...\")\n",
    "    try:\n",
    "        baseline_single = baseline.benchmark_single_inference(benchmark_texts)\n",
    "        results[\"baseline\"][\"single\"] = baseline_single\n",
    "        \n",
    "        print(f\"âœ… Baseline single inference:\")\n",
    "        print(f\"   Average time: {baseline_single['avg_time']:.3f}s\")\n",
    "        print(f\"   Throughput: {baseline_single['throughput']:.2f} texts/second\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Baseline benchmark failed: {e}\")\n",
    "        results[\"baseline\"][\"single\"] = None\n",
    "    \n",
    "    # 2. Single inference benchmark (vLLM)\n",
    "    print(\"\\nâš¡ Running vLLM benchmark...\")\n",
    "    try:\n",
    "        if vllm_bench.is_server_running():\n",
    "            vllm_single = vllm_bench.benchmark_single_inference(benchmark_texts)\n",
    "            results[\"vllm\"][\"single\"] = vllm_single\n",
    "            \n",
    "            print(f\"âœ… vLLM single inference:\")\n",
    "            print(f\"   Average time: {vllm_single['avg_time']:.3f}s\")\n",
    "            print(f\"   Throughput: {vllm_single['throughput']:.2f} texts/second\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ vLLM server not running, skipping vLLM benchmarks\")\n",
    "            results[\"vllm\"][\"single\"] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ vLLM benchmark failed: {e}\")\n",
    "        results[\"vllm\"][\"single\"] = None\n",
    "    \n",
    "    # 3. Batch processing benchmark (HF Baseline)\n",
    "    print(\"\\nðŸ“¦ Running batch processing benchmarks...\")\n",
    "    batch_sizes = [1, 4, 8, 16]\n",
    "    \n",
    "    try:\n",
    "        baseline_batch = baseline.benchmark_batch_inference(benchmark_texts, batch_sizes)\n",
    "        results[\"baseline\"][\"batch\"] = baseline_batch\n",
    "        \n",
    "        print(f\"âœ… Baseline batch results:\")\n",
    "        for size, metrics in baseline_batch.items():\n",
    "            print(f\"   Batch {size}: {metrics['throughput']:.2f} texts/second\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Baseline batch benchmark failed: {e}\")\n",
    "        results[\"baseline\"][\"batch\"] = None\n",
    "    \n",
    "    # 4. Concurrent requests benchmark (vLLM)\n",
    "    if vllm_bench.is_server_running():\n",
    "        try:\n",
    "            concurrent_users = [1, 2, 4, 8]\n",
    "            vllm_concurrent = vllm_bench.benchmark_concurrent_requests(\n",
    "                benchmark_texts, concurrent_users\n",
    "            )\n",
    "            results[\"vllm\"][\"concurrent\"] = vllm_concurrent\n",
    "            \n",
    "            print(f\"âœ… vLLM concurrent results:\")\n",
    "            for users, metrics in vllm_concurrent.items():\n",
    "                print(f\"   {users} users: {metrics['throughput']:.2f} requests/second\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ vLLM concurrent benchmark failed: {e}\")\n",
    "            results[\"vllm\"][\"concurrent\"] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comprehensive benchmark\n",
    "print(\"ðŸš€ Starting comprehensive benchmark...\")\n",
    "benchmark_results = run_comprehensive_benchmark()\n",
    "print(\"\\nâœ… Benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Visualization\n",
    "\n",
    "Let's visualize the performance comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_benchmark_results(results: Dict):\n",
    "    \"\"\"Create comprehensive visualizations of benchmark results.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ðŸš€ vLLM vs HuggingFace Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Single Inference Throughput Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    frameworks = []\n",
    "    throughputs = []\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    if results[\"baseline\"][\"single\"]:\n",
    "        frameworks.append('HuggingFace\\nBaseline')\n",
    "        throughputs.append(results[\"baseline\"][\"single\"][\"throughput\"])\n",
    "    \n",
    "    if results[\"vllm\"][\"single\"]:\n",
    "        frameworks.append('vLLM')\n",
    "        throughputs.append(results[\"vllm\"][\"single\"][\"throughput\"])\n",
    "    \n",
    "    if frameworks:\n",
    "        bars = ax1.bar(frameworks, throughputs, color=colors[:len(frameworks)])\n",
    "        ax1.set_title('Single Inference Throughput', fontweight='bold')\n",
    "        ax1.set_ylabel('Texts/Second')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, throughputs):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.2f}', ha='center', va='bottom')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax1.transAxes)\n",
    "        ax1.set_title('Single Inference Throughput (No Data)')\n",
    "    \n",
    "    # 2. Batch Processing Performance (HF only)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if results[\"baseline\"][\"batch\"]:\n",
    "        batch_sizes = list(results[\"baseline\"][\"batch\"].keys())\n",
    "        batch_throughputs = [results[\"baseline\"][\"batch\"][size][\"throughput\"] \n",
    "                           for size in batch_sizes]\n",
    "        \n",
    "        ax2.plot(batch_sizes, batch_throughputs, 'o-', color='#FF6B6B', linewidth=2, markersize=8)\n",
    "        ax2.set_title('HF Batch Processing Scaling', fontweight='bold')\n",
    "        ax2.set_xlabel('Batch Size')\n",
    "        ax2.set_ylabel('Throughput (texts/second)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for x, y in zip(batch_sizes, batch_throughputs):\n",
    "            ax2.annotate(f'{y:.1f}', (x, y), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No batch data available', ha='center', va='center', \n",
    "                transform=ax2.transAxes)\n",
    "        ax2.set_title('Batch Processing (No Data)')\n",
    "    \n",
    "    # 3. vLLM Concurrent Users Performance\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    if results[\"vllm\"][\"concurrent\"]:\n",
    "        concurrent_users = list(results[\"vllm\"][\"concurrent\"].keys())\n",
    "        concurrent_throughputs = [results[\"vllm\"][\"concurrent\"][users][\"throughput\"] \n",
    "                                for users in concurrent_users]\n",
    "        \n",
    "        ax3.plot(concurrent_users, concurrent_throughputs, 'o-', color='#4ECDC4', \n",
    "                linewidth=2, markersize=8)\n",
    "        ax3.set_title('vLLM Concurrent User Scaling', fontweight='bold')\n",
    "        ax3.set_xlabel('Concurrent Users')\n",
    "        ax3.set_ylabel('Requests/Second')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for x, y in zip(concurrent_users, concurrent_throughputs):\n",
    "            ax3.annotate(f'{y:.1f}', (x, y), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No concurrent data available', ha='center', va='center', \n",
    "                transform=ax3.transAxes)\n",
    "        ax3.set_title('Concurrent Users (No Data)')\n",
    "    \n",
    "    # 4. Latency Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    latency_frameworks = []\n",
    "    avg_latencies = []\n",
    "    std_latencies = []\n",
    "    \n",
    "    if results[\"baseline\"][\"single\"]:\n",
    "        latency_frameworks.append('HuggingFace')\n",
    "        avg_latencies.append(results[\"baseline\"][\"single\"][\"avg_time\"] * 1000)  # Convert to ms\n",
    "        std_latencies.append(results[\"baseline\"][\"single\"][\"std_time\"] * 1000)\n",
    "    \n",
    "    if results[\"vllm\"][\"single\"]:\n",
    "        latency_frameworks.append('vLLM')\n",
    "        avg_latencies.append(results[\"vllm\"][\"single\"][\"avg_time\"] * 1000)\n",
    "        std_latencies.append(results[\"vllm\"][\"single\"][\"std_time\"] * 1000)\n",
    "    \n",
    "    if latency_frameworks:\n",
    "        bars = ax4.bar(latency_frameworks, avg_latencies, yerr=std_latencies, \n",
    "                      color=colors[:len(latency_frameworks)], capsize=5)\n",
    "        ax4.set_title('Average Latency Comparison', fontweight='bold')\n",
    "        ax4.set_ylabel('Latency (milliseconds)')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, avg_latencies):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.1f}ms', ha='center', va='bottom')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No latency data available', ha='center', va='center', \n",
    "                transform=ax4.transAxes)\n",
    "        ax4.set_title('Latency Comparison (No Data)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nðŸ“Š PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    if results[\"baseline\"][\"single\"] and results[\"vllm\"][\"single\"]:\n",
    "        hf_throughput = results[\"baseline\"][\"single\"][\"throughput\"]\n",
    "        vllm_throughput = results[\"vllm\"][\"single\"][\"throughput\"]\n",
    "        speedup = vllm_throughput / hf_throughput\n",
    "        \n",
    "        print(f\"ðŸš€ vLLM Speedup: {speedup:.2f}x faster than HuggingFace\")\n",
    "        print(f\"ðŸ“ˆ Throughput Improvement: {(speedup - 1) * 100:.1f}%\")\n",
    "        \n",
    "        hf_latency = results[\"baseline\"][\"single\"][\"avg_time\"] * 1000\n",
    "        vllm_latency = results[\"vllm\"][\"single\"][\"avg_time\"] * 1000\n",
    "        latency_reduction = (hf_latency - vllm_latency) / hf_latency * 100\n",
    "        \n",
    "        print(f\"âš¡ Latency Reduction: {latency_reduction:.1f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸ Insufficient data for comparison\")\n",
    "        print(\"   Make sure both HF baseline and vLLM server are running\")\n",
    "\n",
    "# Visualize the benchmark results\n",
    "visualize_benchmark_results(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Analysis\n",
    "\n",
    "Let's analyze memory usage patterns between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze and compare memory usage patterns.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ’¾ MEMORY USAGE ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Get current memory stats\n",
    "    cpu_memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"ðŸ–¥ï¸  System Memory:\")\n",
    "    print(f\"   Total RAM: {cpu_memory.total / 1e9:.1f}GB\")\n",
    "    print(f\"   Available: {cpu_memory.available / 1e9:.1f}GB\")\n",
    "    print(f\"   Used: {cpu_memory.percent:.1f}%\")\n",
    "    \n",
    "    # GPU memory analysis\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nðŸŽ® GPU Memory:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "            cached = torch.cuda.memory_reserved(i) / 1e9\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            \n",
    "            print(f\"   GPU {i}: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total\")\n",
    "    \n",
    "    elif GPU_AVAILABLE:\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            for gpu in gpus:\n",
    "                print(f\"\\nðŸŽ® GPU {gpu.id} ({gpu.name}):\")\n",
    "                print(f\"   Memory Used: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB\")\n",
    "                print(f\"   Utilization: {gpu.memoryUtil * 100:.1f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ GPU monitoring error: {e}\")\n",
    "    \n",
    "    # Memory efficiency insights\n",
    "    print(f\"\\nðŸ§  Memory Efficiency Insights:\")\n",
    "    print(f\"   â€¢ vLLM PagedAttention reduces memory fragmentation\")\n",
    "    print(f\"   â€¢ Dynamic batching optimizes memory allocation\")\n",
    "    print(f\"   â€¢ KV cache reuse improves efficiency for repeated queries\")\n",
    "    print(f\"   â€¢ Model quantization can reduce memory by 50-75%\")\n",
    "    \n",
    "    return {\n",
    "        \"cpu_memory_gb\": cpu_memory.total / 1e9,\n",
    "        \"cpu_memory_used_percent\": cpu_memory.percent,\n",
    "        \"gpu_available\": torch.cuda.is_available(),\n",
    "        \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "    }\n",
    "\n",
    "# Run memory analysis\n",
    "memory_stats = analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Deployment Recommendations\n",
    "\n",
    "Based on our benchmarking results, let's provide deployment recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deployment_recommendations(results: Dict, memory_stats: Dict):\n",
    "    \"\"\"Generate deployment recommendations based on benchmark results.\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"high_throughput\": {\n",
    "            \"title\": \"ðŸš€ High Throughput Applications\",\n",
    "            \"description\": \"Social media content moderation, batch processing\",\n",
    "            \"recommendation\": \"vLLM with Docker deployment\",\n",
    "            \"reasoning\": [\n",
    "                \"Continuous batching maximizes GPU utilization\",\n",
    "                \"PagedAttention reduces memory overhead\",\n",
    "                \"Handles concurrent requests efficiently\",\n",
    "                \"Scales horizontally with multiple containers\"\n",
    "            ]\n",
    "        },\n",
    "        \"low_latency\": {\n",
    "            \"title\": \"âš¡ Low Latency Requirements\", \n",
    "            \"description\": \"Real-time chat moderation, interactive applications\",\n",
    "            \"recommendation\": \"vLLM with optimized configuration\",\n",
    "            \"reasoning\": [\n",
    "                \"Pre-loaded models eliminate cold start delays\",\n",
    "                \"Efficient attention computation reduces inference time\",\n",
    "                \"Connection pooling minimizes request overhead\",\n",
    "                \"GPU acceleration provides consistent performance\"\n",
    "            ]\n",
    "        },\n",
    "        \"development\": {\n",
    "            \"title\": \"ðŸ”§ Development & Prototyping\",\n",
    "            \"description\": \"Research, experimentation, small-scale testing\",\n",
    "            \"recommendation\": \"HuggingFace Pipeline API\",\n",
    "            \"reasoning\": [\n",
    "                \"Simpler setup and configuration\",\n",
    "                \"Direct access to model internals\",\n",
    "                \"Better debugging capabilities\",\n",
    "                \"Faster iteration for research\"\n",
    "            ]\n",
    "        },\n",
    "        \"resource_constrained\": {\n",
    "            \"title\": \"ðŸ’» Resource-Constrained Environments\",\n",
    "            \"description\": \"Limited GPU memory, edge devices, cost optimization\",\n",
    "            \"recommendation\": \"Model quantization + vLLM\",\n",
    "            \"reasoning\": [\n",
    "                \"Quantization reduces memory requirements by 50-75%\",\n",
    "                \"vLLM optimizes memory usage patterns\",\n",
    "                \"Dynamic batching adapts to available resources\",\n",
    "                \"Better performance per dollar spent\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario_key, scenario in recommendations.items():\n",
    "        print(f\"\\n{scenario['title']}\")\n",
    "        print(f\"Use Case: {scenario['description']}\")\n",
    "        print(f\"ðŸ’¡ Recommendation: {scenario['recommendation']}\")\n",
    "        print(f\"ðŸ“‹ Reasoning:\")\n",
    "        for reason in scenario['reasoning']:\n",
    "            print(f\"   â€¢ {reason}\")\n",
    "    \n",
    "    # Hardware-specific recommendations\n",
    "    print(f\"\\nðŸ–¥ï¸  HARDWARE RECOMMENDATIONS\")\n",
    "    print(f\"=\" * 30)\n",
    "    \n",
    "    if memory_stats[\"gpu_available\"]:\n",
    "        gpu_memory = memory_stats[\"gpu_memory_gb\"]\n",
    "        \n",
    "        if gpu_memory >= 24:\n",
    "            print(f\"ðŸŽ® High-end GPU ({gpu_memory:.0f}GB):\")\n",
    "            print(f\"   â€¢ Run large models (7B+ parameters)\")\n",
    "            print(f\"   â€¢ Use fp16 precision for optimal performance\")\n",
    "            print(f\"   â€¢ Enable tensor parallelism for massive models\")\n",
    "        \n",
    "        elif gpu_memory >= 8:\n",
    "            print(f\"ðŸŽ® Mid-range GPU ({gpu_memory:.0f}GB):\")\n",
    "            print(f\"   â€¢ Focus on smaller models (3B parameters or less)\")\n",
    "            print(f\"   â€¢ Use quantization for memory efficiency\")\n",
    "            print(f\"   â€¢ Optimize batch sizes for your use case\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"ðŸŽ® Entry-level GPU ({gpu_memory:.0f}GB):\")\n",
    "            print(f\"   â€¢ Use heavily quantized models (4-bit)\")\n",
    "            print(f\"   â€¢ Consider CPU inference for some workloads\")\n",
    "            print(f\"   â€¢ Batch size 1-2 for memory safety\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"ðŸ’» CPU-only environment:\")\n",
    "        print(f\"   â€¢ Consider llama.cpp for better CPU performance\")\n",
    "        print(f\"   â€¢ Use quantized models (4-bit/8-bit)\")\n",
    "        print(f\"   â€¢ Optimize for latency over throughput\")\n",
    "    \n",
    "    # Cost-benefit analysis\n",
    "    print(f\"\\nðŸ’° COST-BENEFIT ANALYSIS\")\n",
    "    print(f\"=\" * 25)\n",
    "    \n",
    "    cost_scenarios = [\n",
    "        {\n",
    "            \"scenario\": \"Small-scale deployment (< 1000 requests/day)\",\n",
    "            \"recommendation\": \"HuggingFace on CPU or small GPU\",\n",
    "            \"cost\": \"Low\",\n",
    "            \"complexity\": \"Low\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Medium-scale deployment (1K-100K requests/day)\",\n",
    "            \"recommendation\": \"vLLM with Docker on cloud GPU\",\n",
    "            \"cost\": \"Medium\",\n",
    "            \"complexity\": \"Medium\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Large-scale deployment (100K+ requests/day)\",\n",
    "            \"recommendation\": \"vLLM with auto-scaling + load balancing\",\n",
    "            \"cost\": \"High\",\n",
    "            \"complexity\": \"High\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in cost_scenarios:\n",
    "        print(f\"\\nðŸ“Š {scenario['scenario']}\")\n",
    "        print(f\"   Recommendation: {scenario['recommendation']}\")\n",
    "        print(f\"   Cost: {scenario['cost']} | Complexity: {scenario['complexity']}\")\n",
    "\n",
    "# Generate deployment recommendations\n",
    "generate_deployment_recommendations(benchmark_results, memory_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Summary\n",
    "\n",
    "### ðŸ”‘ Key Concepts Mastered\n",
    "- **vLLM Performance**: Understanding PagedAttention and continuous batching benefits\n",
    "- **Benchmarking Methodology**: Systematic performance comparison techniques\n",
    "- **Production Deployment**: Real-world considerations for inference optimization\n",
    "- **Resource Management**: Memory and computational efficiency trade-offs\n",
    "\n",
    "### ðŸ“ˆ Best Practices Learned\n",
    "- Comprehensive benchmarking includes throughput, latency, and memory analysis\n",
    "- vLLM excels in high-throughput scenarios with concurrent requests\n",
    "- Hardware specifications directly impact deployment recommendations\n",
    "- Cost-benefit analysis is crucial for production deployment decisions\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "- **Advanced Configuration**: Explore vLLM advanced settings and optimizations\n",
    "- **Production Monitoring**: Implement comprehensive monitoring and alerting\n",
    "- **Auto-scaling**: Set up dynamic scaling based on request patterns\n",
    "- **Documentation**: Review [vLLM Documentation](https://docs.vllm.ai/) for deeper insights\n",
    "\n",
    "---\n",
    "\n",
    "*Ready for edge deployment? Check out **llama.cpp CPU inference** for optimized CPU performance!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- ðŸŒ **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- ðŸ’¼ **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- ðŸ’» **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}