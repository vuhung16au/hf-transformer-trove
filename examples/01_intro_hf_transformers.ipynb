{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/01_intro_hf_transformers.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/01_intro_hf_transformers.ipynb)\n",
    "\n",
    "# 01 - Introduction to Hugging Face and the Transformers Library\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand what Hugging Face is and its ecosystem\n",
    "- Know how to install and import the transformers library\n",
    "- Load and use pre-trained models for various NLP tasks\n",
    "- Understand the basic workflow with pipelines\n",
    "- Explore the Model Hub and find relevant models\n",
    "\n",
    "## What is Hugging Face?\n",
    "\n",
    "Hugging Face is an AI company that has built a comprehensive ecosystem for machine learning, particularly focused on Natural Language Processing (NLP). The company provides:\n",
    "\n",
    "1. **The Transformers Library**: A unified API for using transformer models\n",
    "2. **Model Hub**: A repository of thousands of pre-trained models\n",
    "3. **Datasets Library**: Easy access to NLP datasets\n",
    "4. **Spaces**: Platform for hosting ML demos and applications\n",
    "5. **Tokenizers**: Fast tokenization library\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only once)\n",
    "# !pip install transformers torch datasets tokenizers\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env.local for local development\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env.local', override=True)\n",
    "except ImportError:\n",
    "    # dotenv not available - that's fine for Colab\n",
    "    pass\n",
    "\n",
    "# Credential loading utility for API keys\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Get API key from environment or Colab secrets.\"\"\"\n",
    "    try:\n",
    "        # Try to import Colab userdata (only available in Colab)\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(key_name)\n",
    "    except ImportError:\n",
    "        # Not in Colab - check local environment\n",
    "        api_key = os.getenv(key_name)\n",
    "        if not api_key:\n",
    "            print(f\"⚠️  {key_name} not found. Some features may be limited.\")\n",
    "            print(f\"   For local use: Add {key_name} to .env.local\")\n",
    "            print(f\"   For Colab: Add {key_name} to secrets manager\")\n",
    "            return None\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        # In Colab but key not found\n",
    "        print(f\"⚠️  {key_name} not found in Colab secrets.\")\n",
    "        return None\n",
    "\n",
    "# Try to load Hugging Face token (useful for gated models)\n",
    "hf_token = get_api_key('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(\"✓ Hugging Face token loaded successfully\")\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Device awareness: Automatic optimization for CUDA, MPS (Apple Silicon), and CPU\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for training/inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\") \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif device.type == 'mps':\n",
    "    print(\"Using Apple Silicon (MPS) acceleration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Using Pipelines - The Quickest Way to Get Started\n",
    "\n",
    "Pipelines provide a high-level API for common NLP tasks. They handle:\n",
    "- Model loading\n",
    "- Tokenization\n",
    "- Inference\n",
    "- Post-processing\n",
    "\n",
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# This uses a default model trained for sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test with some examples\n",
    "texts = [\n",
    "    \"I love using Hugging Face transformers!\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"The weather today is okay, nothing special.\"\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "results = classifier(texts)\n",
    "\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Confidence: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "generated = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=2,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "for i, result in enumerate(generated):\n",
    "    print(f\"Generation {i+1}: {result['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Define context and questions\n",
    "context = \"\"\"\n",
    "Hugging Face is an American company based in New York City that develops tools for building \n",
    "applications using machine learning. It is most notable for its transformers library built \n",
    "for natural language processing applications and its platform that allows users to share \n",
    "machine learning models and datasets.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Where is Hugging Face based?\",\n",
    "    \"What is Hugging Face most notable for?\",\n",
    "    \"What can users share on the platform?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer['answer']}\")\n",
    "    print(f\"Confidence: {answer['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with Models and Tokenizers Directly\n",
    "\n",
    "While pipelines are convenient, sometimes you need more control over the process. Let's see how to work with models and tokenizers directly.\n",
    "\n",
    "### Loading a Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific model for sequence classification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model configuration: {model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to classify\n",
    "text = \"I really enjoyed this movie, it was fantastic!\"\n",
    "\n",
    "# Step 1: Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "print(\"Tokenized inputs:\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# Step 2: Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nModel outputs shape: {outputs.logits.shape}\")\n",
    "print(f\"Raw logits: {outputs.logits}\")\n",
    "\n",
    "# Step 3: Convert logits to probabilities\n",
    "probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "# Step 4: Get predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "print(f\"\\nPrediction: {labels[predicted_class]} (confidence: {confidence:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploring the Model Hub\n",
    "\n",
    "The Hugging Face Model Hub contains thousands of pre-trained models. Let's explore how to find and use different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of popular models for different tasks\n",
    "popular_models = {\n",
    "    \"Text Classification\": [\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "    ],\n",
    "    \"Question Answering\": [\n",
    "        \"distilbert-base-cased-distilled-squad\",\n",
    "        \"deepset/roberta-base-squad2\",\n",
    "        \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "    ],\n",
    "    \"Text Generation\": [\n",
    "        \"gpt2\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"distilgpt2\"\n",
    "    ],\n",
    "    \"Summarization\": [\n",
    "        \"facebook/bart-large-cnn\",\n",
    "        \"t5-small\",\n",
    "        \"sshleifer/distilbart-cnn-12-6\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for task, models in popular_models.items():\n",
    "    print(f\"\\n{task}:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sentiment analysis models\n",
    "models_to_compare = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "]\n",
    "\n",
    "test_text = \"The new iPhone available in Sydney stores is amazing, but the price is too high.\"\n",
    "\n",
    "print(f\"Test text: {test_text}\\n\")\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "        result = classifier(test_text)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Result: {result[0]['label']} (confidence: {result[0]['score']:.4f})\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Model Architectures\n",
    "\n",
    "Different transformer architectures are suited for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different model architectures\n",
    "architectures = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"GPT-2\": \"gpt2\"\n",
    "}\n",
    "\n",
    "for arch_name, model_name in architectures.items():\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(f\"\\n{arch_name} ({model_name}):\")\n",
    "        print(f\"  Parameters: {model.num_parameters():,}\")\n",
    "        print(f\"  Max position embeddings: {model.config.max_position_embeddings if hasattr(model.config, 'max_position_embeddings') else 'N/A'}\")\n",
    "        print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "        print(f\"  Number of attention heads: {model.config.num_attention_heads}\")\n",
    "        print(f\"  Number of layers: {model.config.num_hidden_layers}\")\n",
    "        \n",
    "        # Show vocabulary size\n",
    "        print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {arch_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding Model Outputs\n",
    "\n",
    "Let's examine what models actually output and how to interpret these outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello world, this is BERT!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs with different options\n",
    "with torch.no_grad():\n",
    "    # Basic output\n",
    "    basic_output = model(**inputs)\n",
    "    \n",
    "    # Output with attention weights\n",
    "    attention_output = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Output with hidden states\n",
    "    hidden_states_output = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "print(\"Model Output Analysis:\")\n",
    "print(f\"Last hidden state shape: {basic_output.last_hidden_state.shape}\")\n",
    "print(f\"Pooler output shape: {basic_output.pooler_output.shape}\")\n",
    "\n",
    "if attention_output.attentions:\n",
    "    print(f\"Number of attention layers: {len(attention_output.attentions)}\")\n",
    "    print(f\"Attention shape (layer 0): {attention_output.attentions[0].shape}\")\n",
    "\n",
    "if hidden_states_output.hidden_states:\n",
    "    print(f\"Number of hidden state layers: {len(hidden_states_output.hidden_states)}\")\n",
    "    print(f\"Hidden state shape (layer 0): {hidden_states_output.hidden_states[0].shape}\")\n",
    "\n",
    "# Show token-level representations\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Token representations shape: {basic_output.last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Tips and Common Patterns\n",
    "\n",
    "Here are some practical tips for working with Hugging Face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 1: Check model capabilities\n",
    "def analyze_model_capabilities(model_name):\n",
    "    \"\"\"Analyze what a model can do\"\"\"\n",
    "    try:\n",
    "        # Try different task types\n",
    "        tasks_to_try = [\n",
    "            \"text-classification\",\n",
    "            \"question-answering\", \n",
    "            \"text-generation\",\n",
    "            \"fill-mask\",\n",
    "            \"summarization\"\n",
    "        ]\n",
    "        \n",
    "        compatible_tasks = []\n",
    "        \n",
    "        for task in tasks_to_try:\n",
    "            try:\n",
    "                pipeline(task, model=model_name)\n",
    "                compatible_tasks.append(task)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return compatible_tasks\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test with BERT\n",
    "bert_capabilities = analyze_model_capabilities(\"bert-base-uncased\")\n",
    "print(f\"BERT capabilities: {bert_capabilities}\")\n",
    "\n",
    "# Test with GPT-2\n",
    "gpt2_capabilities = analyze_model_capabilities(\"gpt2\")\n",
    "print(f\"GPT-2 capabilities: {gpt2_capabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 2: Memory-efficient loading\n",
    "def load_model_efficiently(model_name, task=None):\n",
    "    \"\"\"Load model with memory optimization\"\"\"\n",
    "    \n",
    "    if task:\n",
    "        # Use pipeline for specific tasks\n",
    "        return pipeline(task, model=model_name, device=0 if device.type == 'cuda' else -1)\n",
    "    else:\n",
    "        # Load with optimizations\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        return model, tokenizer\n",
    "\n",
    "# Example usage\n",
    "efficient_classifier = load_model_efficiently(\"distilbert-base-uncased\", \"text-classification\")\n",
    "result = efficient_classifier(\"This is a test.\")\n",
    "print(f\"Efficient loading result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Hugging Face Overview**: Understanding the ecosystem and its components\n",
    "2. **Pipelines**: Quick and easy way to use pre-trained models\n",
    "3. **Manual Model Usage**: Working with tokenizers and models directly for more control\n",
    "4. **Model Hub Exploration**: Finding and comparing different models\n",
    "5. **Model Architectures**: Understanding different transformer architectures\n",
    "6. **Model Outputs**: Interpreting what models return\n",
    "7. **Practical Tips**: Efficient loading and usage patterns\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 02**: Deep dive into tokenizers\n",
    "- **Notebook 03**: Working with the datasets library\n",
    "- **Notebook 04**: Mini-project combining concepts from 01-03\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Pipelines are great for quick experimentation and prototyping\n",
    "- Direct model usage gives you more control and insight\n",
    "- The Model Hub has models for almost every NLP task\n",
    "- Different architectures (BERT, GPT-2, RoBERTa) have different strengths\n",
    "- Always consider memory usage and efficiency in your applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- 🌐 **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- 💼 **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- 💻 **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}