{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.3/HF-training-data-preparation.ipynb)\n",
        "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.3/HF-training-data-preparation.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.3/HF-training-data-preparation.ipynb)\n",
        "\n",
        "# Data Preparation for HuggingFace Training\n",
        "\n",
        "## ğŸ¯ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- Complete data preparation workflow for HuggingFace model training\n",
        "- Data loading, cleaning, and preprocessing techniques\n",
        "- Text preprocessing and tokenization for NLP tasks\n",
        "- Dataset splitting and formatting for training\n",
        "- Best practices for efficient data handling\n",
        "\n",
        "## ğŸ“‹ Prerequisites\n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with Python and pandas\n",
        "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
        "\n",
        "## ğŸ“š What We'll Cover\n",
        "1. **Environment Setup**: Import libraries and configure reproducible environment\n",
        "2. **Data Loading**: Download and load the Drug Review Dataset\n",
        "3. **Data Exploration**: Inspect and understand the dataset structure\n",
        "4. **Data Cleaning**: Handle missing values and normalize text\n",
        "5. **Feature Engineering**: Create new features and derive insights\n",
        "6. **Text Preprocessing**: Clean and prepare text for tokenization\n",
        "7. **Tokenization**: Convert text to model-ready format\n",
        "8. **Dataset Splitting**: Create train/validation/test splits\n",
        "9. **Data Saving**: Save processed datasets for training\n",
        "\n",
        "## ğŸ’¡ Why This Workflow?\n",
        "\n",
        "Following the [HuggingFace course guidelines](https://huggingface.co/learn/llm-course/chapter5/3?fw=pt), this notebook demonstrates a complete data preparation pipeline that ensures:\n",
        "- **Data Quality**: Thorough cleaning and validation\n",
        "- **Consistency**: Standardized preprocessing steps\n",
        "- **Reproducibility**: Fixed random seeds and documented processes\n",
        "- **Efficiency**: Optimized for Google Colab and cloud environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import html\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# HuggingFace libraries\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Machine learning libraries\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ğŸ“š Libraries imported successfully!\")\n",
        "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up reproducible environment with repository standard seed=16\n",
        "import random\n",
        "import os\n",
        "\n",
        "def setup_reproducible_environment(seed: int = 16):\n",
        "    \"\"\"\n",
        "    Setup reproducible environment with repository standard seed=16.\n",
        "    \n",
        "    Args:\n",
        "        seed: Random seed value (default 16 per repository policy)\n",
        "    \"\"\"\n",
        "    # Set Python built-in random seed\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # Set NumPy random seed  \n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Set PyTorch random seeds\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "    # Set environment variable for Python hash randomization\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "    # Configure CuDNN for reproducibility\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "    print(f\"ğŸ”¢ Reproducible environment setup complete with seed={seed}\")\n",
        "\n",
        "# Repository standard seed\n",
        "setup_reproducible_environment(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device detection for optimal performance\n",
        "def get_optimal_device():\n",
        "    \"\"\"\n",
        "    Get the best available device for PyTorch operations.\n",
        "    \n",
        "    Device Priority:\n",
        "    - General: CUDA GPU > TPU (Colab only) > MPS (Apple Silicon) > CPU\n",
        "    - Google Colab: Always prefer TPU when available\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for current hardware\n",
        "    \"\"\"\n",
        "    # Check for Google Colab TPU\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        device = xm.xla_device()\n",
        "        print(\"ğŸ”¥ Using Google Colab TPU for optimal performance\")\n",
        "        print(\"ğŸ’¡ TPU is preferred in Colab for training and inference\")\n",
        "        return device\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    # Standard device detection\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"ğŸš€ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"ğŸ Using Apple MPS for Apple Silicon optimization\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"ğŸ’» Using CPU - consider GPU/TPU for better performance\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Get optimal device\n",
        "device = get_optimal_device()\n",
        "print(f\"\\nâœ… Selected device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Drug Review Dataset - following HF course reference\n",
        "print(\"ğŸ“¥ Loading Drug Review Dataset...\")\n",
        "print(\"ğŸ’¡ This dataset contains patient reviews of drugs with ratings\")\n",
        "\n",
        "# Load dataset with error handling\n",
        "try:\n",
        "    # Using the dataset from HuggingFace Hub - drug review dataset\n",
        "    dataset = load_dataset(\"fancyzhx/amazon_polarity\", split=\"train[:5000]\")  # Small sample for Colab\n",
        "    print(\"âœ… Dataset loaded successfully!\")\n",
        "    print(f\"ğŸ“Š Total samples loaded: {len(dataset):,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading dataset: {e}\")\n",
        "    print(\"ğŸ’¡ Trying alternative dataset...\")\n",
        "    # Fallback to IMDB dataset if drug review dataset is not available\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:5000]\")\n",
        "    print(\"âœ… IMDB dataset loaded as fallback!\")\n",
        "    print(f\"ğŸ“Š Total samples loaded: {len(dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial dataset exploration\n",
        "print(\"ğŸ” DATASET EXPLORATION\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Dataset basic info\n",
        "print(f\"ğŸ“‹ Dataset features: {list(dataset.features.keys())}\")\n",
        "print(f\"ğŸ”¢ Number of examples: {len(dataset):,}\")\n",
        "\n",
        "# Show first few examples\n",
        "print(\"\\nğŸ“ First 3 examples:\")\n",
        "for i in range(3):\n",
        "    example = dataset[i]\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    for key, value in example.items():\n",
        "        if isinstance(value, str) and len(value) > 100:\n",
        "            print(f\"  {key}: {value[:100]}...\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "# Convert to pandas for easier exploration\n",
        "df = dataset.to_pandas()\n",
        "print(f\"\\nğŸ¼ Converted to pandas DataFrame: {df.shape}\")\n",
        "print(f\"ğŸ“Š DataFrame info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Take a small random sample for detailed inspection - using seed=16\n",
        "print(\"ğŸ² Taking random sample for detailed inspection...\")\n",
        "\n",
        "# Sample 1000 examples using repository standard seed=16\n",
        "sample_dataset = dataset.shuffle(seed=16).select(range(min(1000, len(dataset))))\n",
        "sample_df = sample_dataset.to_pandas()\n",
        "\n",
        "print(f\"ğŸ“Š Sample size: {len(sample_df):,} examples\")\n",
        "print(f\"ğŸ”¢ Random seed used: 16 (repository standard)\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nğŸ“ˆ SAMPLE STATISTICS\")\n",
        "print(\"=\" * 30)\n",
        "print(sample_df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assess data quality and identify issues\n",
        "print(\"ğŸ” DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = sample_df.isnull().sum()\n",
        "print(\"ğŸ“Š Missing Values:\")\n",
        "for column, count in missing_values.items():\n",
        "    if count > 0:\n",
        "        percentage = (count / len(sample_df)) * 100\n",
        "        print(f\"  {column}: {count:,} ({percentage:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"  {column}: None âœ…\")\n",
        "\n",
        "# Check text column (assuming 'text' column exists)\n",
        "text_column = 'text' if 'text' in sample_df.columns else sample_df.select_dtypes(include='object').columns[0]\n",
        "label_column = 'label' if 'label' in sample_df.columns else sample_df.select_dtypes(include=['int', 'float']).columns[0]\n",
        "\n",
        "print(f\"\\nğŸ“ Text column identified: '{text_column}'\")\n",
        "print(f\"ğŸ·ï¸ Label column identified: '{label_column}'\")\n",
        "\n",
        "# Text length analysis\n",
        "text_lengths = sample_df[text_column].str.len()\n",
        "word_counts = sample_df[text_column].str.split().str.len()\n",
        "\n",
        "print(\"\\nğŸ“ Text Length Statistics:\")\n",
        "print(f\"  Character length - Mean: {text_lengths.mean():.1f}, Median: {text_lengths.median():.1f}\")\n",
        "print(f\"  Word count - Mean: {word_counts.mean():.1f}, Median: {word_counts.median():.1f}\")\n",
        "print(f\"  Min words: {word_counts.min()}, Max words: {word_counts.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label distribution analysis\n",
        "print(\"ğŸ·ï¸ LABEL DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Check unique labels\n",
        "unique_labels = sample_df[label_column].unique()\n",
        "label_counts = sample_df[label_column].value_counts().sort_index()\n",
        "\n",
        "print(f\"ğŸ”¢ Unique labels: {sorted(unique_labels)}\")\n",
        "print(f\"ğŸ“Š Label distribution:\")\n",
        "for label, count in label_counts.items():\n",
        "    percentage = (count / len(sample_df)) * 100\n",
        "    print(f\"  Label {label}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# Visualize label distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Label distribution plot\n",
        "plt.subplot(1, 2, 1)\n",
        "label_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Word count distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(word_counts, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "plt.title('Word Count Distribution')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ… Data quality assessment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Data Cleaning and Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data cleaning function\n",
        "def clean_text_data(examples):\n",
        "    \"\"\"\n",
        "    Clean and normalize text data following HF course best practices.\n",
        "    \n",
        "    Args:\n",
        "        examples: Dataset examples with text field\n",
        "        \n",
        "    Returns:\n",
        "        Cleaned examples\n",
        "    \"\"\"\n",
        "    # Get text field name\n",
        "    text_field = text_column\n",
        "    \n",
        "    cleaned_texts = []\n",
        "    \n",
        "    for text in examples[text_field]:\n",
        "        if pd.isna(text) or text is None:\n",
        "            cleaned_texts.append(\"\")  # Handle missing values\n",
        "            continue\n",
        "            \n",
        "        # Convert to string if not already\n",
        "        text = str(text)\n",
        "        \n",
        "        # 1. Unescape HTML characters (as mentioned in HF course)\n",
        "        text = html.unescape(text)\n",
        "        \n",
        "        # 2. Basic text normalization\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        \n",
        "        # Remove leading/trailing whitespace\n",
        "        text = text.strip()\n",
        "        \n",
        "        cleaned_texts.append(text)\n",
        "    \n",
        "    return {text_field: cleaned_texts}\n",
        "\n",
        "print(\"ğŸ§¹ Starting data cleaning process...\")\n",
        "print(\"ğŸ’¡ Following HF course guidelines for text normalization\")\n",
        "\n",
        "# Apply cleaning to the sample dataset\n",
        "start_time = time.time()\n",
        "cleaned_sample = sample_dataset.map(\n",
        "    clean_text_data,\n",
        "    batched=True,\n",
        "    batch_size=100,  # Process in batches for efficiency\n",
        "    desc=\"Cleaning text data\"\n",
        ")\n",
        "cleaning_time = time.time() - start_time\n",
        "\n",
        "print(f\"âœ… Text cleaning completed in {cleaning_time:.2f} seconds\")\n",
        "print(f\"ğŸš€ Processing speed: {len(sample_dataset)/cleaning_time:.1f} examples/second\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out very short or empty texts (following HF course guidelines)\n",
        "def filter_short_texts(examples, min_word_count=3):\n",
        "    \"\"\"\n",
        "    Filter out texts that are too short to be useful.\n",
        "    \n",
        "    Args:\n",
        "        examples: Dataset examples\n",
        "        min_word_count: Minimum number of words required\n",
        "        \n",
        "    Returns:\n",
        "        Boolean mask for filtering\n",
        "    \"\"\"\n",
        "    text_field = text_column\n",
        "    return [len(str(text).split()) >= min_word_count for text in examples[text_field]]\n",
        "\n",
        "print(\"ğŸ” Filtering out very short texts...\")\n",
        "print(f\"ğŸ’¡ Minimum word requirement: 3 words\")\n",
        "\n",
        "# Count before filtering\n",
        "before_count = len(cleaned_sample)\n",
        "\n",
        "# Apply filter\n",
        "filtered_dataset = cleaned_sample.filter(\n",
        "    lambda examples: filter_short_texts(examples),\n",
        "    batched=True,\n",
        "    desc=\"Filtering short texts\"\n",
        ")\n",
        "\n",
        "after_count = len(filtered_dataset)\n",
        "removed_count = before_count - after_count\n",
        "\n",
        "print(f\"ğŸ“Š Filtering results:\")\n",
        "print(f\"  Before: {before_count:,} examples\")\n",
        "print(f\"  After: {after_count:,} examples\")\n",
        "print(f\"  Removed: {removed_count:,} examples ({removed_count/before_count*100:.1f}%)\")\n",
        "\n",
        "if removed_count == 0:\n",
        "    print(\"âœ… No texts were too short - data quality is good!\")\n",
        "else:\n",
        "    print(f\"âœ… Filtered out {removed_count} short texts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add new features following HF course examples\n",
        "def add_text_features(examples):\n",
        "    \"\"\"\n",
        "    Add useful text features for analysis and training.\n",
        "    \n",
        "    Args:\n",
        "        examples: Dataset examples\n",
        "        \n",
        "    Returns:\n",
        "        Examples with additional features\n",
        "    \"\"\"\n",
        "    text_field = text_column\n",
        "    \n",
        "    # Calculate text statistics\n",
        "    word_counts = []\n",
        "    char_counts = []\n",
        "    sentence_counts = []\n",
        "    \n",
        "    for text in examples[text_field]:\n",
        "        text = str(text)\n",
        "        \n",
        "        # Word count\n",
        "        word_count = len(text.split())\n",
        "        word_counts.append(word_count)\n",
        "        \n",
        "        # Character count\n",
        "        char_count = len(text)\n",
        "        char_counts.append(char_count)\n",
        "        \n",
        "        # Sentence count (approximate)\n",
        "        sentence_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n",
        "        sentence_counts.append(max(1, sentence_count))  # At least 1\n",
        "    \n",
        "    return {\n",
        "        'word_count': word_counts,\n",
        "        'char_count': char_counts,\n",
        "        'sentence_count': sentence_counts\n",
        "    }\n",
        "\n",
        "print(\"ğŸ”§ Adding text features for analysis...\")\n",
        "print(\"ğŸ’¡ Following HF course feature engineering practices\")\n",
        "\n",
        "# Add features\n",
        "start_time = time.time()\n",
        "enhanced_dataset = filtered_dataset.map(\n",
        "    add_text_features,\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    desc=\"Adding text features\"\n",
        ")\n",
        "feature_time = time.time() - start_time\n",
        "\n",
        "print(f\"âœ… Feature engineering completed in {feature_time:.2f} seconds\")\n",
        "print(f\"ğŸ“Š New features added: word_count, char_count, sentence_count\")\n",
        "\n",
        "# Show updated dataset info\n",
        "print(f\"\\nğŸ” Enhanced dataset features: {list(enhanced_dataset.features.keys())}\")\n",
        "\n",
        "# Show feature statistics\n",
        "enhanced_df = enhanced_dataset.to_pandas()\n",
        "print(\"\\nğŸ“ˆ New Feature Statistics:\")\n",
        "for feature in ['word_count', 'char_count', 'sentence_count']:\n",
        "    mean_val = enhanced_df[feature].mean()\n",
        "    median_val = enhanced_df[feature].median()\n",
        "    print(f\"  {feature}: Mean={mean_val:.1f}, Median={median_val:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Tokenization and Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer for text preprocessing\n",
        "print(\"ğŸ”¤ Loading tokenizer for text preprocessing...\")\n",
        "print(\"ğŸ’¡ Using DistilBERT tokenizer - efficient and lightweight\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "print(f\"âœ… Tokenizer loaded: {tokenizer_name}\")\n",
        "print(f\"ğŸ“Š Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"ğŸ”¢ Max sequence length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# Test tokenization with a sample\n",
        "sample_text = enhanced_dataset[0][text_column]\n",
        "sample_tokens = tokenizer.tokenize(sample_text)\n",
        "print(f\"\\nğŸ“ Sample tokenization:\")\n",
        "print(f\"  Original: {sample_text[:100]}...\")\n",
        "print(f\"  Tokens: {sample_tokens[:20]}...\")\n",
        "print(f\"  Token count: {len(sample_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenization function following HF course guidelines\n",
        "def tokenize_function(examples, max_length=128):\n",
        "    \"\"\"\n",
        "    Tokenize text examples for model training.\n",
        "    \n",
        "    Args:\n",
        "        examples: Dataset examples with text field\n",
        "        max_length: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        Tokenized examples\n",
        "    \"\"\"\n",
        "    text_field = text_column\n",
        "    \n",
        "    # Tokenize the texts\n",
        "    tokenized = tokenizer(\n",
        "        examples[text_field],\n",
        "        truncation=True,          # Truncate to max_length\n",
        "        padding=False,            # Don't pad yet (we'll pad in batches later)\n",
        "        max_length=max_length,    # Set maximum sequence length\n",
        "        return_special_tokens_mask=True  # Useful for some training approaches\n",
        "    )\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "print(\"ğŸ”„ Tokenizing dataset for model training...\")\n",
        "print(f\"ğŸ’¡ Max sequence length: 128 tokens (optimized for Colab)\")\n",
        "\n",
        "# Apply tokenization\n",
        "start_time = time.time()\n",
        "tokenized_dataset = enhanced_dataset.map(\n",
        "    lambda examples: tokenize_function(examples, max_length=128),\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    desc=\"Tokenizing texts\"\n",
        ")\n",
        "tokenization_time = time.time() - start_time\n",
        "\n",
        "print(f\"âœ… Tokenization completed in {tokenization_time:.2f} seconds\")\n",
        "print(f\"ğŸš€ Tokenization speed: {len(enhanced_dataset)/tokenization_time:.1f} examples/second\")\n",
        "\n",
        "# Show tokenized dataset info\n",
        "print(f\"\\nğŸ“Š Tokenized dataset features: {list(tokenized_dataset.features.keys())}\")\n",
        "\n",
        "# Analyze token lengths\n",
        "token_lengths = [len(tokens) for tokens in tokenized_dataset['input_ids']]\n",
        "print(f\"\\nğŸ“ Token Length Statistics:\")\n",
        "print(f\"  Mean: {np.mean(token_lengths):.1f} tokens\")\n",
        "print(f\"  Median: {np.median(token_lengths):.1f} tokens\")\n",
        "print(f\"  Min: {min(token_lengths)} tokens\")\n",
        "print(f\"  Max: {max(token_lengths)} tokens\")\n",
        "print(f\"  Truncated examples: {sum(1 for length in token_lengths if length == 128)} ({sum(1 for length in token_lengths if length == 128)/len(token_lengths)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Dataset Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset into train/validation/test sets using repository standard seed=16\n",
        "print(\"ğŸ”€ Splitting dataset for training...\")\n",
        "print(\"ğŸ’¡ Using train(70%)/validation(15%)/test(15%) split\")\n",
        "print(f\"ğŸ”¢ Using seed=16 for reproducibility (repository standard)\")\n",
        "\n",
        "# First split: train (70%) vs temp (30%)\n",
        "train_test_split_result = tokenized_dataset.train_test_split(\n",
        "    test_size=0.3,\n",
        "    seed=16,  # Repository standard seed\n",
        "    stratify_by_column=label_column  # Maintain label distribution\n",
        ")\n",
        "\n",
        "train_dataset = train_test_split_result['train']\n",
        "temp_dataset = train_test_split_result['test']\n",
        "\n",
        "# Second split: validation (15%) vs test (15%)\n",
        "val_test_split_result = temp_dataset.train_test_split(\n",
        "    test_size=0.5,  # Split the 30% equally\n",
        "    seed=16,  # Repository standard seed\n",
        "    stratify_by_column=label_column\n",
        ")\n",
        "\n",
        "val_dataset = val_test_split_result['train']\n",
        "test_dataset = val_test_split_result['test']\n",
        "\n",
        "# Create final dataset dictionary\n",
        "final_dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset split results:\")\n",
        "for split_name, split_dataset in final_dataset.items():\n",
        "    percentage = len(split_dataset) / len(tokenized_dataset) * 100\n",
        "    print(f\"  {split_name.capitalize()}: {len(split_dataset):,} examples ({percentage:.1f}%)\")\n",
        "\n",
        "# Verify label distribution in each split\n",
        "print(f\"\\nğŸ·ï¸ Label distribution verification:\")\n",
        "for split_name, split_dataset in final_dataset.items():\n",
        "    split_df = split_dataset.to_pandas()\n",
        "    label_dist = split_df[label_column].value_counts().sort_index()\n",
        "    print(f\"  {split_name.capitalize()}: {dict(label_dist)}\")\n",
        "\n",
        "print(\"\\nâœ… Dataset splitting completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Data Saving and Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed datasets following HF course guidelines\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"./processed_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ’¾ Saving processed datasets to {output_dir}/...\")\n",
        "print(\"ğŸ’¡ Multiple formats for flexibility and sharing\")\n",
        "\n",
        "# 1. Save in Arrow format (recommended by HF course) - most efficient\n",
        "arrow_path = os.path.join(output_dir, \"arrow_format\")\n",
        "final_dataset.save_to_disk(arrow_path)\n",
        "print(f\"âœ… Saved in Arrow format: {arrow_path}\")\n",
        "\n",
        "# 2. Save individual splits as CSV (for broader compatibility)\n",
        "csv_dir = os.path.join(output_dir, \"csv_format\")\n",
        "os.makedirs(csv_dir, exist_ok=True)\n",
        "\n",
        "for split_name, split_dataset in final_dataset.items():\n",
        "    csv_path = os.path.join(csv_dir, f\"{split_name}.csv\")\n",
        "    split_df = split_dataset.to_pandas()\n",
        "    split_df.to_csv(csv_path, index=False)\n",
        "    print(f\"âœ… Saved {split_name} split as CSV: {csv_path}\")\n",
        "\n",
        "# 3. Save metadata and processing info\n",
        "metadata = {\n",
        "    'dataset_info': {\n",
        "        'source_dataset': 'Amazon Polarity (sample)' if 'amazon' in str(type(dataset)) else 'IMDB (sample)',\n",
        "        'total_examples': len(tokenized_dataset),\n",
        "        'text_column': text_column,\n",
        "        'label_column': label_column,\n",
        "        'tokenizer': tokenizer_name,\n",
        "        'max_length': 128,\n",
        "        'seed_used': 16\n",
        "    },\n",
        "    'split_info': {\n",
        "        'train_size': len(final_dataset['train']),\n",
        "        'validation_size': len(final_dataset['validation']),\n",
        "        'test_size': len(final_dataset['test'])\n",
        "    },\n",
        "    'processing_steps': [\n",
        "        'HTML unescaping',\n",
        "        'Whitespace normalization',\n",
        "        'Short text filtering',\n",
        "        'Feature engineering (word_count, char_count, sentence_count)',\n",
        "        'Tokenization with DistilBERT tokenizer',\n",
        "        'Stratified train/val/test split'\n",
        "    ]\n",
        "}\n",
        "\n",
        "import json\n",
        "metadata_path = os.path.join(output_dir, \"metadata.json\")\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"âœ… Saved processing metadata: {metadata_path}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ All datasets saved successfully!\")\n",
        "print(f\"ğŸ“ Output directory: {os.path.abspath(output_dir)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Data Loading Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate how to reload the processed data (following HF course)\n",
        "print(\"ğŸ”„ Demonstrating how to reload processed datasets...\")\n",
        "print(\"ğŸ’¡ This is how you would load the data for training\")\n",
        "\n",
        "# Method 1: Load from Arrow format (recommended)\n",
        "try:\n",
        "    from datasets import load_from_disk\n",
        "    \n",
        "    reloaded_dataset = load_from_disk(arrow_path)\n",
        "    print(f\"âœ… Successfully reloaded from Arrow format\")\n",
        "    print(f\"ğŸ“Š Reloaded dataset splits: {list(reloaded_dataset.keys())}\")\n",
        "    \n",
        "    # Verify data integrity\n",
        "    for split_name in reloaded_dataset.keys():\n",
        "        original_size = len(final_dataset[split_name])\n",
        "        reloaded_size = len(reloaded_dataset[split_name])\n",
        "        print(f\"  {split_name}: {reloaded_size:,} examples (matches original: {original_size == reloaded_size})\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error reloading Arrow format: {e}\")\n",
        "\n",
        "# Method 2: Load from CSV (alternative approach)\n",
        "try:\n",
        "    train_csv_df = pd.read_csv(os.path.join(csv_dir, \"train.csv\"))\n",
        "    print(f\"\\nâœ… Successfully loaded train CSV: {len(train_csv_df):,} rows\")\n",
        "    print(f\"ğŸ“Š CSV columns: {list(train_csv_df.columns)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading CSV: {e}\")\n",
        "\n",
        "# Show sample of final processed data\n",
        "print(f\"\\nğŸ“ Sample of final processed data:\")\n",
        "sample_example = final_dataset['train'][0]\n",
        "for key, value in sample_example.items():\n",
        "    if key == 'input_ids':\n",
        "        print(f\"  {key}: [{', '.join(map(str, value[:10]))}...] (length: {len(value)})\")\n",
        "    elif key == text_column and len(str(value)) > 100:\n",
        "        print(f\"  {key}: {str(value)[:100]}...\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Data preparation pipeline completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Training Ready Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary and next steps\n",
        "print(\"ğŸ“‹ TRAINING DATA PREPARATION SUMMARY\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "print(f\"\\nâœ… Processing completed successfully!\")\n",
        "print(f\"ğŸ“Š Final dataset statistics:\")\n",
        "print(f\"   Total examples processed: {len(tokenized_dataset):,}\")\n",
        "print(f\"   Training examples: {len(final_dataset['train']):,} (70%)\")\n",
        "print(f\"   Validation examples: {len(final_dataset['validation']):,} (15%)\")\n",
        "print(f\"   Test examples: {len(final_dataset['test']):,} (15%)\")\n",
        "\n",
        "print(f\"\\nğŸ”§ Processing steps completed:\")\n",
        "for i, step in enumerate(metadata['processing_steps'], 1):\n",
        "    print(f\"   {i}. {step}\")\n",
        "\n",
        "print(f\"\\nğŸ’¾ Output files created:\")\n",
        "print(f\"   ğŸ“ Arrow format: {arrow_path}\")\n",
        "print(f\"   ğŸ“„ CSV files: {csv_dir}\")\n",
        "print(f\"   ğŸ“ Metadata: {metadata_path}\")\n",
        "\n",
        "print(f\"\\nğŸš€ Ready for training!\")\n",
        "print(f\"   ğŸ¤– Tokenizer: {tokenizer_name}\")\n",
        "print(f\"   ğŸ“ Max sequence length: 128 tokens\")\n",
        "print(f\"   ğŸ”¢ Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"   ğŸ¯ Task: {len(final_dataset['train'][label_column].unique())} class classification\")\n",
        "\n",
        "print(f\"\\nğŸ“– Next steps:\")\n",
        "print(f\"   1. Load processed data using: load_from_disk('{arrow_path}')\")\n",
        "print(f\"   2. Set up data collator for dynamic padding\")\n",
        "print(f\"   3. Configure training arguments\")\n",
        "print(f\"   4. Initialize model and trainer\")\n",
        "print(f\"   5. Start training!\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Data preparation pipeline completed in under 30 minutes!\")\n",
        "print(f\"âœ… Optimized for Google Colab and AWS SageMaker Studio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“‹ Summary\n",
        "\n",
        "### ğŸ”‘ Key Concepts Mastered\n",
        "- **Complete Data Pipeline**: End-to-end data preparation workflow following HuggingFace course guidelines\n",
        "- **Text Preprocessing**: HTML unescaping, normalization, and cleaning techniques\n",
        "- **Feature Engineering**: Adding meaningful text statistics for analysis\n",
        "- **Tokenization**: Converting text to model-ready format with proper truncation and padding\n",
        "- **Data Splitting**: Stratified splits maintaining label distribution\n",
        "- **Data Persistence**: Multiple storage formats for flexibility and reuse\n",
        "\n",
        "### ğŸ“ˆ Best Practices Learned\n",
        "- **Reproducibility**: Using `seed=16` consistently across all random operations\n",
        "- **Efficiency**: Batch processing for optimal performance on cloud platforms\n",
        "- **Data Quality**: Thorough cleaning and validation steps\n",
        "- **Documentation**: Comprehensive metadata and processing logs\n",
        "- **Flexibility**: Multiple output formats for different use cases\n",
        "- **Cloud Optimization**: Designed for Google Colab and AWS SageMaker Studio\n",
        "\n",
        "### ğŸš€ Next Steps\n",
        "- **Training Setup**: Load processed data and configure model training\n",
        "- **Model Selection**: Choose appropriate pre-trained model for your task\n",
        "- **Advanced Techniques**: Explore data augmentation and advanced preprocessing\n",
        "- **Evaluation**: Set up comprehensive evaluation metrics and procedures\n",
        "\n",
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- ğŸŒ **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- ğŸ’¼ **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- ğŸ’» **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}