{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic3.6/HF-full-training-demo.ipynb)\n",
        "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic3.6/HF-full-training-demo.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic3.6/HF-full-training-demo.ipynb)\n",
        "\n",
        "# HF Full Training Demo - Fast Fine-tuning for Cloud Environments\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "- How to efficiently fine-tune DistilBERT for sentiment classification\n",
        "- Optimizing training for AWS SageMaker Studio and cloud environments\n",
        "- Using GLUE SST-2 dataset for rapid convergence\n",
        "- Best practices for fast, cost-effective fine-tuning\n",
        "- Knowledge distillation benefits in production scenarios\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with Python and PyTorch\n",
        "- Knowledge of NLP fundamentals\n",
        "\n",
        "## üöÄ What We'll Cover\n",
        "1. **Environment Setup**: Optimized imports and device detection\n",
        "2. **Dataset Loading**: GLUE SST-2 for binary sentiment classification\n",
        "3. **Model Setup**: DistilBERT - 40% smaller, 97% performance\n",
        "4. **Training Configuration**: Fast convergence settings\n",
        "5. **Fine-tuning Execution**: 15-30 minute training cycle\n",
        "6. **Evaluation & Testing**: Performance validation\n",
        "7. **Model Deployment**: Saving for production use\n",
        "\n",
        "## üí° Why This Combination?\n",
        "\n",
        "| Component | Recommendation | Efficiency Reason |\n",
        "| :--- | :--- | :--- |\n",
        "| **Model** | `distilbert-base-uncased` | 40% fewer parameters than BERT, faster training |\n",
        "| **Task** | Binary Sentiment Classification | Simplest downstream task, maximum speed |\n",
        "| **Dataset** | GLUE SST-2 | ~67K samples, rapid convergence in 3-4 epochs |\n",
        "\n",
        "**Expected Training Time**: 15-30 minutes on AWS ml.g4dn.xlarge or similar GPU instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup and Imports\n",
        "\n",
        "First, let's set up our environment with the necessary libraries optimized for cloud training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for fast fine-tuning\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ Environment setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {__import__('transformers').__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Device Detection - AWS SageMaker Optimized\n",
        "\n",
        "Automatically detect the best available device, with special consideration for AWS SageMaker Studio environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimal_device():\n",
        "    \"\"\"\n",
        "    Get the best available device for training, optimized for AWS environments.\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for current hardware\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"üî• Using CUDA GPU: {gpu_name}\")\n",
        "        print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        # AWS SageMaker instance detection\n",
        "        if 'ml.g4dn' in gpu_name or 'ml.g5' in gpu_name:\n",
        "            print(\"‚òÅÔ∏è  AWS SageMaker GPU instance detected - optimal for training!\")\n",
        "            \n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "# Get optimal device\n",
        "device = get_optimal_device()\n",
        "print(f\"\\n‚úÖ Selected device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: GLUE SST-2 Dataset Loading\n",
        "\n",
        "Load the Stanford Sentiment Treebank v2 dataset - perfect for fast binary sentiment classification training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GLUE SST-2 dataset for binary sentiment classification\n",
        "print(\"üì• Loading GLUE SST-2 dataset...\")\n",
        "\n",
        "# Load the complete dataset\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìä Training examples: {len(dataset['train']):,}\")\n",
        "print(f\"üî¨ Validation examples: {len(dataset['validation']):,}\")\n",
        "\n",
        "# Examine the dataset structure\n",
        "print(f\"\\nüìã Dataset features: {dataset['train'].features}\")\n",
        "\n",
        "# Show example data\n",
        "train_example = dataset['train'][0]\n",
        "print(f\"\\nüìù Example training sample:\")\n",
        "print(f\"   Sentence: '{train_example['sentence']}'\")\n",
        "print(f\"   Label: {train_example['label']} ({'Positive' if train_example['label'] == 1 else 'Negative'})\")\n",
        "\n",
        "# Dataset statistics\n",
        "from collections import Counter\n",
        "train_labels = [ex['label'] for ex in dataset['train']]\n",
        "val_labels = [ex['label'] for ex in dataset['validation']]\n",
        "\n",
        "print(f\"\\nüìà Label distribution:\")\n",
        "print(f\"   Training: {Counter(train_labels)}\")\n",
        "print(f\"   Validation: {Counter(val_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: DistilBERT Model Setup\n",
        "\n",
        "Load DistilBERT - a distilled version of BERT that's 40% smaller while maintaining 97% of BERT's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration for fast training\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "num_labels = 2  # Binary classification\n",
        "\n",
        "print(f\"üîÑ Loading {model_name}...\")\n",
        "print(f\"üí° DistilBERT info: 40% fewer parameters than BERT, 97% performance retained\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model for sequence classification\n",
        "try:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
        "        label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    )\n",
        "    \n",
        "    # Move model to optimal device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    print(f\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"üìä Parameters: {model.num_parameters():,} (vs ~110M for BERT-base)\")\n",
        "    print(f\"üéØ Model moved to: {device}\")\n",
        "    \n",
        "    # Model architecture info\n",
        "    print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
        "    print(f\"   Hidden size: {model.config.hidden_size}\")\n",
        "    print(f\"   Attention heads: {model.config.num_attention_heads}\")\n",
        "    print(f\"   Hidden layers: {model.config.num_hidden_layers} (vs 12 for BERT)\")\n",
        "    print(f\"   Max position embeddings: {model.config.max_position_embeddings}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Ensure pad token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úÖ Pad token configured\")\n",
        "\n",
        "print(f\"\\n‚ö° Knowledge Distillation Benefits:\")\n",
        "print(f\"   ‚Ä¢ 40% fewer parameters = faster training\")\n",
        "print(f\"   ‚Ä¢ Lower memory usage = larger batch sizes possible\")\n",
        "print(f\"   ‚Ä¢ Faster inference = better production performance\")\n",
        "print(f\"   ‚Ä¢ 97% of BERT performance retained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Training Configuration and Fine-tuning\n",
        "\n",
        "Configure training for optimal performance on cloud GPU instances and start fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenization function optimized for speed\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['sentence'],\n",
        "        truncation=True,\n",
        "        padding=False,  # Dynamic padding during training\n",
        "        max_length=128  # Shorter sequences = faster training\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"üîÑ Tokenizing datasets...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['sentence', 'idx']  # Remove unnecessary columns\n",
        ")\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# Training configuration for AWS SageMaker\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-sst2-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=(device.type == 'cuda'),\n",
        "    dataloader_pin_memory=True,\n",
        "    seed=42,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print(\"üéØ Training configuration:\")\n",
        "print(f\"   üìÖ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   üì¶ Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   üìà Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   ‚ö° Mixed precision: {training_args.fp16}\")\n",
        "print(f\"   üéØ Expected time: 15-30 minutes on GPU\")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nüöÄ Starting fine-tuning...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training completed in {training_time:.2f}s ({training_time/60:.1f} min)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Evaluation and Model Testing\n",
        "\n",
        "Evaluate the fine-tuned model and test it with real examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation\n",
        "print(\"üî¨ Running final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nüìä Final Results:\")\n",
        "for metric, value in eval_results.items():\n",
        "    if metric.startswith('eval_'):\n",
        "        metric_name = metric.replace('eval_', '').title()\n",
        "        if 'loss' in metric:\n",
        "            print(f\"   {metric_name}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"   {metric_name}: {value:.4f} ({value*100:.2f}%)\")\n",
        "\n",
        "# Test with examples\n",
        "test_examples = [\n",
        "    \"This movie is absolutely fantastic!\",\n",
        "    \"What a terrible waste of time.\",\n",
        "    \"The acting was mediocre but engaging.\",\n",
        "    \"Outstanding performance by the actors!\",\n",
        "    \"Worst movie I've ever seen.\",\n",
        "    \"A masterpiece of cinema.\"\n",
        "]\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "    return 'POSITIVE' if predicted_class == 1 else 'NEGATIVE', confidence\n",
        "\n",
        "print(\"\\nüß™ Testing model with examples:\")\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    prediction, confidence = predict_sentiment(example)\n",
        "    print(f\"{i}. '{example[:50]}...'\")\n",
        "    print(f\"   ‚Üí {prediction} (confidence: {confidence:.3f})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Model Saving and Deployment\n",
        "\n",
        "Save the fine-tuned model for production deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_save_path = \"./distilbert-sst2-finetuned\"\n",
        "print(f\"üíæ Saving model to: {model_save_path}\")\n",
        "\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Create model card\n",
        "accuracy = eval_results.get('eval_accuracy', 0)\n",
        "model_card = f\"\"\"\n",
        "# DistilBERT Fine-tuned for Sentiment Classification\n",
        "\n",
        "## Model Description\n",
        "Fine-tuned DistilBERT on GLUE SST-2 for binary sentiment classification.\n",
        "\n",
        "## Performance\n",
        "- Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n",
        "- Training Time: {training_time/60:.1f} minutes\n",
        "- Parameters: {model.num_parameters():,} (40% smaller than BERT)\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{model_save_path}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"{model_save_path}\")\n",
        "\n",
        "text = \"This movie is great!\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "Optimized for AWS SageMaker Studio deployment.\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{model_save_path}/README.md\", \"w\") as f:\n",
        "    f.write(model_card)\n",
        "\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "print(f\"üìä Final accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"‚è±Ô∏è  Total training time: {training_time/60:.1f} minutes\")\n",
        "print(f\"üöÄ Ready for deployment on AWS SageMaker!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Summary\n",
        "\n",
        "### üîë Key Concepts Mastered\n",
        "- **Knowledge Distillation**: DistilBERT provides 97% of BERT performance with 40% fewer parameters\n",
        "- **Efficient Fine-tuning**: GLUE SST-2 enables rapid convergence in 15-30 minutes\n",
        "- **Cloud Optimization**: AWS SageMaker Studio specific configurations for maximum efficiency\n",
        "- **Production Pipeline**: Complete workflow from data loading to model deployment\n",
        "\n",
        "### üìà Best Practices Learned\n",
        "- **Model Selection**: Choose distilled models for speed without sacrificing performance\n",
        "- **Dataset Strategy**: Use benchmark datasets like GLUE for reliable, fast convergence\n",
        "- **Training Optimization**: Mixed precision, dynamic padding, and early stopping\n",
        "- **Evaluation Strategy**: Comprehensive metrics and real-world testing examples\n",
        "\n",
        "### üöÄ Next Steps\n",
        "- **Advanced Techniques**: Explore PEFT techniques like LoRA for even more efficiency\n",
        "- **Model Serving**: Deploy on AWS SageMaker endpoints\n",
        "- **Domain Adaptation**: Apply to domain-specific datasets\n",
        "- **Performance Monitoring**: Implement MLOps practices\n",
        "\n",
        "### üí° AWS SageMaker Key Takeaways\n",
        "- **Instance Recommendation**: ml.g4dn.xlarge or ml.g5.xlarge optimal for this workflow\n",
        "- **Cost Efficiency**: 15-30 minute training keeps costs low while achieving high performance\n",
        "- **Scalability**: Approach scales well for larger datasets and production workloads\n",
        "- **Integration**: Works seamlessly with SageMaker Hugging Face containers\n",
        "\n",
        "---\n",
        "\n",
        "## About the Author\n",
        "\n",
        "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
        "\n",
        "Connect with me:\n",
        "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
        "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
        "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
        "\n",
        "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}