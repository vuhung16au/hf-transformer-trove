{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.6/grammar-correction.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic1.6/grammar-correction.ipynb)\n",
    "\n",
    "# Grammar Correction with T5 Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to use T5 models for grammar correction tasks\n",
    "- The text-to-text approach for fixing grammatical errors\n",
    "- Working with grammar correction across multiple languages (English, Vietnamese, Japanese)\n",
    "- Best practices for grammar correction with transformers\n",
    "- How to evaluate grammar correction quality\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- Understanding of encoder-decoder architectures (refer to [docs/encoder-decoder.md](../../docs/encoder-decoder.md))\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Introduction**: Grammar correction with T5\n",
    "2. **Basic Setup**: Loading models and tokenizers\n",
    "3. **English Grammar Correction**: Common error patterns\n",
    "4. **Multilingual Correction**: Vietnamese and Japanese examples\n",
    "5. **Advanced Techniques**: Fine-tuning and optimization\n",
    "6. **Evaluation**: Quality assessment methods\n",
    "\n",
    "## What is Grammar Correction?\n",
    "\n",
    "Grammar correction is a **sequence-to-sequence task** where we transform grammatically incorrect text into grammatically correct text. The T5 (Text-to-Text Transfer Transformer) model excels at this task by treating it as a text generation problem with the prefix **\"grammar: \"**.\n",
    "\n",
    "### Why T5 for Grammar Correction?\n",
    "\n",
    "- **Unified Framework**: T5 treats all NLP tasks as text-to-text problems\n",
    "- **Context Understanding**: Encoder-decoder architecture captures full sentence context\n",
    "- **Flexible Output**: Can handle various types of corrections simultaneously\n",
    "- **Multilingual Support**: Works across different languages with appropriate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only once)\n",
    "# !pip install transformers torch datasets numpy pandas matplotlib seaborn tqdm\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the best available device for PyTorch operations.\n",
    "    \n",
    "    Priority order: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU - consider GPU for better performance\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Set up device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Grammar Correction with T5\n",
    "\n",
    "Let's start with a simple T5 model for grammar correction. We'll use the T5-base model which provides a good balance between performance and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 model for grammar correction\n",
    "def load_grammar_correction_model(model_name: str = \"t5-base\"):\n",
    "    \"\"\"\n",
    "    Load T5 model and tokenizer for grammar correction.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"üì• Loading {model_name} for grammar correction...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        # Move to optimal device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Enable evaluation mode for inference\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        print(f\"üìä Model size: {model.num_parameters():,} parameters\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"üí° Try using t5-small for lower memory requirements\")\n",
    "        raise\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_grammar_correction_model(\"t5-small\")  # Using t5-small for better compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(text: str, model, tokenizer, max_length: int = 128) -> str:\n",
    "    \"\"\"\n",
    "    Correct grammar in the given text using T5 model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with potential grammar errors\n",
    "        model: T5 model for grammar correction\n",
    "        tokenizer: T5 tokenizer\n",
    "        max_length: Maximum output length\n",
    "        \n",
    "    Returns:\n",
    "        Corrected text\n",
    "    \"\"\"\n",
    "    # Format input for T5 with grammar correction prefix\n",
    "    input_text = f\"grammar: {text}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate correction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,  # Use beam search for better quality\n",
    "            length_penalty=0.6,\n",
    "            early_stopping=True,\n",
    "            do_sample=False  # Deterministic output\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "# Test with a simple example\n",
    "test_sentence = \"I are going to the store yesterday.\"\n",
    "corrected = correct_grammar(test_sentence, model, tokenizer)\n",
    "\n",
    "print(\"üß™ BASIC GRAMMAR CORRECTION TEST\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original:  {test_sentence}\")\n",
    "print(f\"Corrected: {corrected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: English Grammar Correction Examples\n",
    "\n",
    "Let's test our grammar correction system with various types of English grammar errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English grammar correction examples\n",
    "english_examples = {\n",
    "    \"Subject-Verb Agreement\": [\n",
    "        \"The dogs runs in the park every day.\",\n",
    "        \"She don't like coffee in the morning.\",\n",
    "        \"There is many books on the shelf.\"\n",
    "    ],\n",
    "    \"Verb Tenses\": [\n",
    "        \"Yesterday, I go to the cinema with my friends.\",\n",
    "        \"She has went to the store already.\",\n",
    "        \"I will going to the meeting tomorrow.\"\n",
    "    ],\n",
    "    \"Articles\": [\n",
    "        \"I need a advice about this situation.\",\n",
    "        \"She is a honest person.\",\n",
    "        \"The cats are playing in a garden.\"\n",
    "    ],\n",
    "    \"Prepositions\": [\n",
    "        \"I am good in mathematics.\",\n",
    "        \"She arrived to the party late.\",\n",
    "        \"The book is laying on the table.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üá∫üá∏ ENGLISH GRAMMAR CORRECTION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, examples in english_examples.items():\n",
    "    print(f\"\\nüìö {category.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. Original:  {example}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            corrected = correct_grammar(example, model, tokenizer)\n",
    "            correction_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   Corrected: {corrected}\")\n",
    "            print(f\"   ‚è±Ô∏è  Time: {correction_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Vietnamese Grammar Correction\n",
    "\n",
    "Now let's try grammar correction with Vietnamese text. While T5-base was primarily trained on English, it can handle some basic corrections in other languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vietnamese grammar correction examples\n",
    "vietnamese_examples = {\n",
    "    \"Word Order\": [\n",
    "        \"T√¥i r·∫•t th√≠ch ƒÉn ph·ªü.\",  # Correct: \"I really like eating pho\"\n",
    "        \"ƒÇn ph·ªü t√¥i th√≠ch r·∫•t.\",  # Incorrect word order\n",
    "        \"H√¥m nay th·ªùi ti·∫øt ƒë·∫πp r·∫•t.\"\n",
    "    ],\n",
    "    \"Classifiers\": [\n",
    "        \"T√¥i c√≥ hai con ch√≥.\",  # Correct: \"I have two dogs\"\n",
    "        \"T√¥i c√≥ hai ch√≥.\",      # Missing classifier\n",
    "        \"Ba cu·ªën s√°ch n√†y r·∫•t hay.\"\n",
    "    ],\n",
    "    \"Common Errors\": [\n",
    "        \"T√¥i s·∫Ω ƒëi h·ªçc ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\",  # Going to study at university\n",
    "        \"Vi·ªác h·ªçc ti·∫øng Anh r·∫•t kh√≥.\",       # Learning English is difficult\n",
    "        \"Ch√∫ng t√¥i ƒë·∫øn t·ª´ Vi·ªát Nam.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üáªüá≥ VIETNAMESE GRAMMAR CORRECTION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, examples in vietnamese_examples.items():\n",
    "    print(f\"\\nüìö {category.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. üáªüá≥ Vietnamese: {example}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            # Note: Using the same model - in practice, you'd want a Vietnamese-specific model\n",
    "            corrected = correct_grammar(example, model, tokenizer)\n",
    "            correction_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   Processed: {corrected}\")\n",
    "            print(f\"   ‚è±Ô∏è  Time: {correction_time:.2f}s\")\n",
    "            print(f\"   üí° Note: Consider using mT5 for better multilingual support\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüîß FOR BETTER VIETNAMESE SUPPORT:\")\n",
    "print(\"Consider using multilingual models like:\")\n",
    "print(\"- google/mt5-base\")\n",
    "print(\"- VietAI/vit5-base\")\n",
    "print(\"- facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Japanese Grammar Correction\n",
    "\n",
    "Let's explore Japanese text processing. Similar to Vietnamese, specialized multilingual models work better for Japanese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japanese grammar correction examples\n",
    "japanese_examples = {\n",
    "    \"Particle Usage\": [\n",
    "        \"ÁßÅ„ÅØÂ≠¶Áîü„Åß„Åô„ÄÇ\",         # I am a student (correct)\n",
    "        \"ÁßÅ„ÅåÂ≠¶Áîü„Åß„Åô„ÄÇ\",         # Incorrect particle usage\n",
    "        \"Êú¨„ÇíË™≠„Åø„Åæ„Åô„ÄÇ\"           # I read books\n",
    "    ],\n",
    "    \"Verb Conjugation\": [\n",
    "        \"Êò®Êó•Êò†Áîª„ÇíË¶ã„Åæ„Åó„Åü„ÄÇ\",     # I watched a movie yesterday\n",
    "        \"ÊòéÊó•Êù±‰∫¨„Å´Ë°å„Åç„Åæ„Åô„ÄÇ\",     # I will go to Tokyo tomorrow\n",
    "        \"ÊØéÊó•Êó•Êú¨Ë™û„ÇíÂãâÂº∑„Åó„Åæ„Åô„ÄÇ\"   # I study Japanese every day\n",
    "    ],\n",
    "    \"Politeness Levels\": [\n",
    "        \"„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ\",   # Thank you (polite)\n",
    "        \"„Åô„Åø„Åæ„Åõ„Çì„ÄÇ\",           # Excuse me / Sorry\n",
    "        \"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ\"     # Good morning (polite)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üáØüáµ JAPANESE GRAMMAR CORRECTION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, examples in japanese_examples.items():\n",
    "    print(f\"\\nüìö {category.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. üáØüáµ Japanese: {example}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            # Note: T5-base has limited Japanese capability\n",
    "            corrected = correct_grammar(example, model, tokenizer)\n",
    "            correction_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   Processed: {corrected}\")\n",
    "            print(f\"   ‚è±Ô∏è  Time: {correction_time:.2f}s\")\n",
    "            print(f\"   üí° Note: T5-base has limited Japanese support\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüîß FOR BETTER JAPANESE SUPPORT:\")\n",
    "print(\"Consider using specialized models like:\")\n",
    "print(\"- rinna/japanese-gpt2-medium\")\n",
    "print(\"- cl-tohoku/bert-base-japanese\")\n",
    "print(\"- google/mt5-base (multilingual)\")\n",
    "print(\"- megagonlabs/t5-base-japanese-web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Grammar Correction Techniques\n",
    "\n",
    "Let's explore more advanced techniques for grammar correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grammar_correction(texts: List[str], model, tokenizer, batch_size: int = 4) -> List[str]:\n",
    "    \"\"\"\n",
    "    Correct grammar for multiple texts efficiently using batching.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input texts\n",
    "        model: T5 model\n",
    "        tokenizer: T5 tokenizer\n",
    "        batch_size: Number of texts to process simultaneously\n",
    "        \n",
    "    Returns:\n",
    "        List of corrected texts\n",
    "    \"\"\"\n",
    "    corrected_texts = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Format inputs for T5\n",
    "        input_texts = [f\"grammar: {text}\" for text in batch]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            input_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate corrections\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=2,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode outputs\n",
    "        batch_corrected = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        corrected_texts.extend(batch_corrected)\n",
    "    \n",
    "    return corrected_texts\n",
    "\n",
    "# Test batch processing\n",
    "test_batch = [\n",
    "    \"She don't like coffee.\",\n",
    "    \"I are going to school.\",\n",
    "    \"The book are on the table.\",\n",
    "    \"He have many friends.\",\n",
    "    \"They was happy yesterday.\"\n",
    "]\n",
    "\n",
    "print(\"‚ö° BATCH GRAMMAR CORRECTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "start_time = time.time()\n",
    "batch_results = batch_grammar_correction(test_batch, model, tokenizer, batch_size=2)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "for i, (original, corrected) in enumerate(zip(test_batch, batch_results), 1):\n",
    "    print(f\"\\n{i}. Original:  {original}\")\n",
    "    print(f\"   Corrected: {corrected}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {total_time:.2f}s\")\n",
    "print(f\"üöÄ Speed: {len(test_batch)/total_time:.1f} corrections/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Grammar Correction Quality Assessment\n",
    "\n",
    "Let's implement some basic methods to assess the quality of our grammar corrections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_correction_quality(original: str, corrected: str, reference: str = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Assess the quality of grammar correction.\n",
    "    \n",
    "    Args:\n",
    "        original: Original text with errors\n",
    "        corrected: Model-corrected text\n",
    "        reference: Reference correct text (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    original_words = original.split()\n",
    "    corrected_words = corrected.split()\n",
    "    \n",
    "    metrics['length_change'] = len(corrected_words) - len(original_words)\n",
    "    metrics['length_ratio'] = len(corrected_words) / len(original_words) if original_words else 0\n",
    "    \n",
    "    # Word-level changes\n",
    "    changed_words = sum(1 for o, c in zip(original_words, corrected_words) if o != c)\n",
    "    metrics['words_changed'] = changed_words\n",
    "    metrics['change_ratio'] = changed_words / len(original_words) if original_words else 0\n",
    "    \n",
    "    # If reference is provided, calculate similarity\n",
    "    if reference:\n",
    "        ref_words = reference.split()\n",
    "        \n",
    "        # Simple word overlap with reference\n",
    "        corrected_set = set(corrected_words)\n",
    "        reference_set = set(ref_words)\n",
    "        \n",
    "        overlap = len(corrected_set & reference_set)\n",
    "        union = len(corrected_set | reference_set)\n",
    "        \n",
    "        metrics['word_overlap_with_reference'] = overlap / union if union else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test quality assessment\n",
    "test_cases = [\n",
    "    {\n",
    "        'original': \"She don't like coffee.\",\n",
    "        'reference': \"She doesn't like coffee.\"\n",
    "    },\n",
    "    {\n",
    "        'original': \"I are going to school.\",\n",
    "        'reference': \"I am going to school.\"\n",
    "    },\n",
    "    {\n",
    "        'original': \"The books is on the table.\",\n",
    "        'reference': \"The books are on the table.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üìä GRAMMAR CORRECTION QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    original = test_case['original']\n",
    "    reference = test_case['reference']\n",
    "    \n",
    "    # Get model correction\n",
    "    corrected = correct_grammar(original, model, tokenizer)\n",
    "    \n",
    "    # Assess quality\n",
    "    quality_metrics = assess_correction_quality(original, corrected, reference)\n",
    "    \n",
    "    print(f\"\\n{i}. Test Case:\")\n",
    "    print(f\"   Original:  {original}\")\n",
    "    print(f\"   Corrected: {corrected}\")\n",
    "    print(f\"   Reference: {reference}\")\n",
    "    print(f\"   üìà Quality Metrics:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"      {metric}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"      {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Best Practices and Tips\n",
    "\n",
    "Here are some best practices for effective grammar correction with T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_grammar_correction_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for grammar correction.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_practices = {\n",
    "        \"üéØ Model Selection\": [\n",
    "            \"Use T5-base or T5-large for better English grammar correction\",\n",
    "            \"Consider mT5 for multilingual applications\",\n",
    "            \"Fine-tune on domain-specific data when possible\",\n",
    "            \"Start with smaller models (T5-small) for development\"\n",
    "        ],\n",
    "        \n",
    "        \"‚öôÔ∏è Generation Parameters\": [\n",
    "            \"Use num_beams=4-8 for better quality (slower)\",\n",
    "            \"Set length_penalty=0.6-0.8 to control output length\",\n",
    "            \"Enable early_stopping=True to avoid repetition\",\n",
    "            \"Use do_sample=False for consistent corrections\"\n",
    "        ],\n",
    "        \n",
    "        \"üìù Input Formatting\": [\n",
    "            \"Always use 'grammar:' prefix for T5 models\",\n",
    "            \"Keep input sentences reasonably short (<100 words)\",\n",
    "            \"Handle special characters and punctuation carefully\",\n",
    "            \"Consider sentence segmentation for long texts\"\n",
    "        ],\n",
    "        \n",
    "        \"‚ö° Performance Optimization\": [\n",
    "            \"Use batch processing for multiple texts\",\n",
    "            \"Cache model and tokenizer to avoid reloading\",\n",
    "            \"Use GPU acceleration when available\",\n",
    "            \"Consider model quantization for deployment\"\n",
    "        ],\n",
    "        \n",
    "        \"üåç Multilingual Support\": [\n",
    "            \"Use language-specific models for better results\",\n",
    "            \"Consider mT5 or mBART for multilingual tasks\",\n",
    "            \"Test thoroughly with target language examples\",\n",
    "            \"Be aware of cultural and linguistic nuances\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Quality Evaluation\": [\n",
    "            \"Compare against human-annotated references\",\n",
    "            \"Use multiple evaluation metrics (BLEU, METEOR, etc.)\",\n",
    "            \"Test on diverse error types and domains\",\n",
    "            \"Monitor for over-correction or under-correction\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üí° GRAMMAR CORRECTION BEST PRACTICES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, tips in best_practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"  ‚Ä¢ {tip}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üö® Common Pitfalls to Avoid:\")\n",
    "    \n",
    "    pitfalls = [\n",
    "        \"Don't rely solely on T5-base for non-English languages\",\n",
    "        \"Avoid processing very long texts without segmentation\",\n",
    "        \"Don't ignore context when correcting isolated sentences\",\n",
    "        \"Avoid over-correcting stylistic choices vs. actual errors\",\n",
    "        \"Don't assume all model outputs are improvements\"\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  ‚ùå {pitfall}\")\n",
    "\n",
    "demonstrate_grammar_correction_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Interactive Grammar Correction Demo\n",
    "\n",
    "Let's create a simple interactive demo for trying custom grammar corrections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_grammar_correction_demo():\n",
    "    \"\"\"\n",
    "    Simple demo for trying custom grammar corrections.\n",
    "    \"\"\"\n",
    "    print(\"üéÆ Interactive Grammar Correction Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Try correcting your own text!\")\n",
    "    print(\"(Note: In a real notebook, you could use input() for interaction)\")\n",
    "    \n",
    "    # Demo examples instead of interactive input\n",
    "    demo_texts = [\n",
    "        \"I has a dog and two cats.\",\n",
    "        \"She going to the market yesterday.\",\n",
    "        \"The childrens was playing in the park.\",\n",
    "        \"We seen that movie last week.\",\n",
    "        \"He don't know the answer.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù Demo Corrections:\")\n",
    "    \n",
    "    for i, demo_text in enumerate(demo_texts, 1):\n",
    "        print(f\"\\n{i}. Input: {demo_text}\")\n",
    "        \n",
    "        try:\n",
    "            corrected = correct_grammar(demo_text, model, tokenizer)\n",
    "            print(f\"   Output: {corrected}\")\n",
    "            \n",
    "            # Simple feedback\n",
    "            if demo_text.lower() != corrected.lower():\n",
    "                print(\"   ‚úÖ Grammar corrections applied\")\n",
    "            else:\n",
    "                print(\"   ‚ÑπÔ∏è  No changes suggested\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\nüí° Tips for better results:\")\n",
    "    print(\"  ‚Ä¢ Keep sentences reasonably short\")\n",
    "    print(\"  ‚Ä¢ Focus on grammatical errors vs. style preferences\")\n",
    "    print(\"  ‚Ä¢ Consider context when evaluating corrections\")\n",
    "\n",
    "interactive_grammar_correction_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **T5 for Grammar Correction**: Using text-to-text approach with \"grammar:\" prefix\n",
    "- **Multilingual Processing**: Working with English, Vietnamese, and Japanese text\n",
    "- **Quality Assessment**: Methods to evaluate correction effectiveness\n",
    "- **Batch Processing**: Efficient handling of multiple texts\n",
    "- **Best Practices**: Production-ready techniques and optimization strategies\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use appropriate model sizes based on computational constraints\n",
    "- Apply proper input formatting with task prefixes\n",
    "- Implement batch processing for efficiency\n",
    "- Consider language-specific models for non-English text\n",
    "- Evaluate corrections against reference standards\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Advanced Models**: Explore mT5, T5-large, or domain-specific models\n",
    "- **Fine-tuning**: Train on your specific grammar correction dataset\n",
    "- **Multilingual Support**: Implement with mT5 or language-specific models\n",
    "- **Production Deployment**: Build scalable grammar correction APIs\n",
    "- **Evaluation Metrics**: Implement BLEU, METEOR, or BERTScore for quality assessment\n",
    "\n",
    "### üìö Further Reading\n",
    "- [T5 Paper: \"Exploring the Limits of Transfer Learning\"](https://arxiv.org/abs/1910.10683)\n",
    "- [Hugging Face T5 Documentation](https://huggingface.co/docs/transformers/model_doc/t5)\n",
    "- [Grammar Error Correction Survey](https://arxiv.org/abs/2005.06600)\n",
    "- [Multilingual T5 (mT5) Paper](https://arxiv.org/abs/2010.11934)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}