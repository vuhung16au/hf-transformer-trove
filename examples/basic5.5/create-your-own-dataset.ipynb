{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header-badges"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.5/create-your-own-dataset.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.5/create-your-own-dataset.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic5.5/create-your-own-dataset.ipynb)\n",
    "\n",
    "# Creating Your Own Dataset and Uploading to Hugging Face Hub\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to collect data from real-world sources (GitHub issues)\n",
    "- Data cleaning and preprocessing techniques for text data\n",
    "- Dataset augmentation strategies to enhance your data\n",
    "- How to upload datasets to the Hugging Face Hub\n",
    "- Best practices for creating comprehensive dataset cards\n",
    "- Dataset versioning and sharing with the community\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and pandas\n",
    "- Knowledge of NLP fundamentals (refer to [NLP Learning Journey](https://github.com/vuhung16au/nlp-learning-journey))\n",
    "- A Hugging Face account (create one at https://huggingface.co/)\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Getting the Data**: Fetch GitHub issues from Hugging Face repositories\n",
    "2. **Cleaning the Data**: Process and clean the collected text data\n",
    "3. **Augmenting the Dataset**: Enhance data with additional features\n",
    "4. **Uploading to Hub**: Push your dataset to Hugging Face Hub\n",
    "5. **Creating a Dataset Card**: Document your dataset professionally\n",
    "\n",
    "## üí° Why Create and Share Datasets?\n",
    "\n",
    "Following the [HuggingFace course Chapter 5, Section 5](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt), creating and sharing datasets:\n",
    "- **Contributes to the ML community**: Help researchers and practitioners\n",
    "- **Enables reproducibility**: Others can build on your work\n",
    "- **Demonstrates data quality**: Show best practices in data collection\n",
    "- **Fosters collaboration**: Connect with other ML enthusiasts\n",
    "\n",
    "> üí° **Educational Focus**: This notebook demonstrates creating a dataset from GitHub issues to showcase practical data collection and preparation workflows.\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: Always respect API rate limits and data privacy. Follow the terms of service for any APIs you use.\n",
    "\n",
    "**References:**\n",
    "- HF Course: https://huggingface.co/learn/llm-course/chapter5/5?fw=pt\n",
    "- Colab Example: https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## Setup: Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install datasets transformers huggingface_hub pandas numpy requests tqdm\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Union\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login, whoami, HfApi\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üêç Python packages ready for dataset creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# Set reproducible environment with repository standard seed=16\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value: int = 16):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility across all random number generators.\n",
    "    Repository standard is seed=16.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    print(f\"üî¢ Random seed set to {seed_value} for reproducibility\")\n",
    "\n",
    "# Set repository standard seed\n",
    "set_seed(16)\n",
    "\n",
    "# Configure visualization style (repository standard)\n",
    "sns.set_style('darkgrid')  # Better readability with gridlines\n",
    "sns.set_palette(\"husl\")     # Consistent, accessible colors\n",
    "print(\"üìä Visualization style configured: darkgrid with husl palette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-credentials"
   },
   "outputs": [],
   "source": [
    "# Credential management for multi-platform compatibility\n",
    "def get_api_key(key_name: str, required: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load API key from environment or Google Colab secrets.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Environment variable name\n",
    "        required: Whether to raise error if not found\n",
    "        \n",
    "    Returns:\n",
    "        API key string or None\n",
    "    \"\"\"\n",
    "    # Try Colab secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get(key_name)\n",
    "        if api_key:\n",
    "            print(f\"‚úÖ Loaded {key_name} from Google Colab secrets\")\n",
    "            return api_key\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    api_key = os.getenv(key_name)\n",
    "    if api_key:\n",
    "        print(f\"‚úÖ Loaded {key_name} from environment variable\")\n",
    "        return api_key\n",
    "    \n",
    "    # Handle missing required keys\n",
    "    if required:\n",
    "        print(f\"‚ö†Ô∏è {key_name} not found. Please set it in:\")\n",
    "        print(f\"  - Local: .env.local file or environment variable\")\n",
    "        print(f\"  - Colab: Secrets manager (üîë icon in sidebar)\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Authentication setup\n",
    "print(\"üîê Setting up authentication...\")\n",
    "print(\"\\nüìã Authentication Methods:\")\n",
    "print(\"1. üîë Google Colab: Use Secrets manager (recommended for Colab)\")\n",
    "print(\"2. üíª Local: Set HF_TOKEN and GITHUB_TOKEN environment variables\")\n",
    "print(\"3. üñ•Ô∏è CLI: Run `huggingface-cli login` in terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1-header"
   },
   "source": [
    "## 1. Getting the Data: Fetching GitHub Issues\n",
    "\n",
    "We'll collect GitHub issues from Hugging Face repositories to create a real-world dataset. This demonstrates:\n",
    "- API interaction and data collection\n",
    "- Handling pagination and rate limits\n",
    "- Extracting structured data from APIs\n",
    "\n",
    "> üí° **Pro Tip**: Always respect API rate limits and implement proper error handling when collecting data from external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch-issues-function"
   },
   "outputs": [],
   "source": [
    "def fetch_github_issues(\n",
    "    repo_owner: str = \"huggingface\",\n",
    "    repo_name: str = \"transformers\",\n",
    "    max_issues: int = 100,\n",
    "    state: str = \"all\",\n",
    "    github_token: Optional[str] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch GitHub issues from a repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: Repository owner (organization or user)\n",
    "        repo_name: Repository name\n",
    "        max_issues: Maximum number of issues to fetch\n",
    "        state: Issue state ('open', 'closed', 'all')\n",
    "        github_token: GitHub API token (optional, increases rate limit)\n",
    "        \n",
    "    Returns:\n",
    "        List of issue dictionaries\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/issues\"\n",
    "    headers = {}\n",
    "    \n",
    "    # Add authentication if token provided\n",
    "    if github_token:\n",
    "        headers['Authorization'] = f'token {github_token}'\n",
    "        print(\"üîë Using GitHub token for authentication\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GitHub token provided. Rate limit: 60 requests/hour\")\n",
    "        print(\"üí° Provide GITHUB_TOKEN for higher rate limit (5000 requests/hour)\")\n",
    "    \n",
    "    all_issues = []\n",
    "    page = 1\n",
    "    per_page = min(100, max_issues)  # GitHub API max is 100 per page\n",
    "    \n",
    "    print(f\"\\nüì• Fetching issues from {repo_owner}/{repo_name}...\")\n",
    "    \n",
    "    with tqdm(total=max_issues, desc=\"Fetching issues\") as pbar:\n",
    "        while len(all_issues) < max_issues:\n",
    "            try:\n",
    "                # Construct API URL with parameters\n",
    "                params = {\n",
    "                    'state': state,\n",
    "                    'per_page': per_page,\n",
    "                    'page': page\n",
    "                }\n",
    "                \n",
    "                # Make API request\n",
    "                response = requests.get(base_url, headers=headers, params=params)\n",
    "                \n",
    "                # Check rate limiting\n",
    "                if response.status_code == 403:\n",
    "                    print(\"\\n‚ö†Ô∏è Rate limit exceeded. Waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                issues = response.json()\n",
    "                \n",
    "                # No more issues available\n",
    "                if not issues:\n",
    "                    break\n",
    "                \n",
    "                # Filter out pull requests (they also appear in issues endpoint)\n",
    "                issues = [issue for issue in issues if 'pull_request' not in issue]\n",
    "                \n",
    "                all_issues.extend(issues)\n",
    "                pbar.update(len(issues))\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "                # Be respectful of rate limits\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\n‚ùå Error fetching issues: {e}\")\n",
    "                break\n",
    "    \n",
    "    # Limit to requested maximum\n",
    "    all_issues = all_issues[:max_issues]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully fetched {len(all_issues)} issues\")\n",
    "    return all_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch-issues-execute"
   },
   "outputs": [],
   "source": [
    "# Get GitHub token (optional but recommended)\n",
    "github_token = get_api_key('GITHUB_TOKEN', required=False)\n",
    "\n",
    "# Fetch issues from Hugging Face transformers repository\n",
    "# Using a smaller number for demonstration (adjust as needed)\n",
    "print(\"üîÑ Starting data collection...\")\n",
    "print(\"üìù Collecting GitHub issues from huggingface/transformers repository\\n\")\n",
    "\n",
    "raw_issues = fetch_github_issues(\n",
    "    repo_owner=\"huggingface\",\n",
    "    repo_name=\"transformers\",\n",
    "    max_issues=150,  # Reasonable number for demonstration\n",
    "    state=\"all\",      # Get both open and closed issues\n",
    "    github_token=github_token\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Raw data collected: {len(raw_issues)} issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-raw-data"
   },
   "outputs": [],
   "source": [
    "# Inspect the raw data structure\n",
    "if raw_issues:\n",
    "    print(\"üîç Sample Issue Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sample_issue = raw_issues[0]\n",
    "    \n",
    "    # Display key fields\n",
    "    print(f\"\\nüìå Title: {sample_issue.get('title', 'N/A')}\")\n",
    "    print(f\"üî¢ Number: {sample_issue.get('number', 'N/A')}\")\n",
    "    print(f\"üë§ Author: {sample_issue.get('user', {}).get('login', 'N/A')}\")\n",
    "    print(f\"üìÖ Created: {sample_issue.get('created_at', 'N/A')}\")\n",
    "    print(f\"üè∑Ô∏è State: {sample_issue.get('state', 'N/A')}\")\n",
    "    print(f\"üí¨ Comments: {sample_issue.get('comments', 0)}\")\n",
    "    \n",
    "    # Show labels\n",
    "    labels = [label.get('name', '') for label in sample_issue.get('labels', [])]\n",
    "    print(f\"üè∑Ô∏è Labels: {', '.join(labels) if labels else 'None'}\")\n",
    "    \n",
    "    # Show body preview\n",
    "    body = sample_issue.get('body', '')\n",
    "    body_preview = body[:200] + '...' if body and len(body) > 200 else body\n",
    "    print(f\"\\nüìÑ Body Preview:\\n{body_preview}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"\\nüìã Available fields: {len(sample_issue)} total\")\n",
    "    print(f\"üîë Key fields: {', '.join(list(sample_issue.keys())[:10])}...\")\n",
    "else:\n",
    "    print(\"‚ùå No issues fetched. Check API access and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2-header"
   },
   "source": [
    "## 2. Cleaning Up the Data\n",
    "\n",
    "Now we'll clean and structure our raw data:\n",
    "- Extract relevant fields\n",
    "- Handle missing values\n",
    "- Clean text content\n",
    "- Create a structured dataset\n",
    "\n",
    "> üí° **Data Quality**: Clean data is essential for training effective models. Always inspect and validate your data before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract-features"
   },
   "outputs": [],
   "source": [
    "def extract_issue_features(issue: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract relevant features from a GitHub issue.\n",
    "    \n",
    "    Args:\n",
    "        issue: Raw issue dictionary from GitHub API\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with cleaned and structured features\n",
    "    \"\"\"\n",
    "    # Extract basic information\n",
    "    issue_data = {\n",
    "        'id': issue.get('id', None),\n",
    "        'number': issue.get('number', None),\n",
    "        'title': issue.get('title', ''),\n",
    "        'body': issue.get('body', ''),\n",
    "        'state': issue.get('state', 'unknown'),\n",
    "        'created_at': issue.get('created_at', ''),\n",
    "        'updated_at': issue.get('updated_at', ''),\n",
    "        'closed_at': issue.get('closed_at', ''),\n",
    "        'author': issue.get('user', {}).get('login', 'unknown'),\n",
    "        'comments_count': issue.get('comments', 0),\n",
    "        'labels': [label.get('name', '') for label in issue.get('labels', [])],\n",
    "        'url': issue.get('html_url', ''),\n",
    "    }\n",
    "    \n",
    "    return issue_data\n",
    "\n",
    "# Extract features from all issues\n",
    "print(\"üîÑ Extracting and cleaning features...\")\n",
    "cleaned_issues = [extract_issue_features(issue) for issue in raw_issues]\n",
    "print(f\"‚úÖ Extracted features from {len(cleaned_issues)} issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clean-text"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text content.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Unescape HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "print(\"üßπ Cleaning text content...\")\n",
    "for issue in cleaned_issues:\n",
    "    issue['title'] = clean_text(issue['title'])\n",
    "    issue['body'] = clean_text(issue['body'])\n",
    "\n",
    "print(\"‚úÖ Text cleaning complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataframe"
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(cleaned_issues)\n",
    "\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total issues: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã Sample Data:\")\n",
    "display(df[['number', 'title', 'state', 'author', 'comments_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-quality-checks"
   },
   "outputs": [],
   "source": [
    "# Data quality checks and filtering\n",
    "print(\"üîç Performing data quality checks...\\n\")\n",
    "\n",
    "# Check for empty titles\n",
    "empty_titles = df['title'].isna() | (df['title'] == '')\n",
    "print(f\"Issues with empty titles: {empty_titles.sum()}\")\n",
    "\n",
    "# Check for empty bodies\n",
    "empty_bodies = df['body'].isna() | (df['body'] == '')\n",
    "print(f\"Issues with empty bodies: {empty_bodies.sum()}\")\n",
    "\n",
    "# Filter out issues with empty titles (keep those with empty bodies as they might still be useful)\n",
    "df_filtered = df[~empty_titles].copy()\n",
    "print(f\"\\nFiltered dataset size: {len(df_filtered)} issues\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìà Data Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nStates distribution:\")\n",
    "print(df_filtered['state'].value_counts())\n",
    "print(f\"\\nComments statistics:\")\n",
    "print(df_filtered['comments_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: State distribution\n",
    "state_counts = df_filtered['state'].value_counts()\n",
    "axes[0, 0].bar(state_counts.index, state_counts.values, color=['green', 'red'])\n",
    "axes[0, 0].set_title('Issue State Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('State')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Comments distribution\n",
    "axes[0, 1].hist(df_filtered['comments_count'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Comments Count Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Comments')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Top 10 authors\n",
    "top_authors = df_filtered['author'].value_counts().head(10)\n",
    "axes[1, 0].barh(top_authors.index, top_authors.values, color='coral')\n",
    "axes[1, 0].set_title('Top 10 Issue Authors', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Issues')\n",
    "axes[1, 0].set_ylabel('Author')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Title length distribution\n",
    "df_filtered['title_length'] = df_filtered['title'].str.len()\n",
    "axes[1, 1].hist(df_filtered['title_length'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].set_title('Issue Title Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Title Length (characters)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Data visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3-header"
   },
   "source": [
    "## 3. Augmenting the Dataset\n",
    "\n",
    "Let's enhance our dataset with additional features:\n",
    "- Text length metrics\n",
    "- Temporal features\n",
    "- Categorical encodings\n",
    "- Derived features\n",
    "\n",
    "> üí° **Feature Engineering**: Adding relevant features can significantly improve downstream model performance and analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "augment-features"
   },
   "outputs": [],
   "source": [
    "def augment_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Augment dataset with additional derived features.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Augmented DataFrame\n",
    "    \"\"\"\n",
    "    df_augmented = df.copy()\n",
    "    \n",
    "    print(\"üîÑ Adding derived features...\\n\")\n",
    "    \n",
    "    # 1. Text length features\n",
    "    print(\"üìè Computing text length features...\")\n",
    "    df_augmented['title_length'] = df_augmented['title'].str.len()\n",
    "    df_augmented['body_length'] = df_augmented['body'].str.len()\n",
    "    df_augmented['title_word_count'] = df_augmented['title'].str.split().str.len()\n",
    "    df_augmented['body_word_count'] = df_augmented['body'].str.split().str.len()\n",
    "    \n",
    "    # 2. Temporal features\n",
    "    print(\"üìÖ Extracting temporal features...\")\n",
    "    df_augmented['created_at'] = pd.to_datetime(df_augmented['created_at'])\n",
    "    df_augmented['updated_at'] = pd.to_datetime(df_augmented['updated_at'])\n",
    "    \n",
    "    # Extract date components\n",
    "    df_augmented['created_year'] = df_augmented['created_at'].dt.year\n",
    "    df_augmented['created_month'] = df_augmented['created_at'].dt.month\n",
    "    df_augmented['created_day_of_week'] = df_augmented['created_at'].dt.dayofweek\n",
    "    df_augmented['created_hour'] = df_augmented['created_at'].dt.hour\n",
    "    \n",
    "    # 3. Label features\n",
    "    print(\"üè∑Ô∏è Processing labels...\")\n",
    "    df_augmented['labels_count'] = df_augmented['labels'].apply(len)\n",
    "    df_augmented['has_labels'] = df_augmented['labels_count'] > 0\n",
    "    df_augmented['labels_text'] = df_augmented['labels'].apply(lambda x: ', '.join(x) if x else 'none')\n",
    "    \n",
    "    # 4. Engagement features\n",
    "    print(\"üí¨ Computing engagement features...\")\n",
    "    df_augmented['has_comments'] = df_augmented['comments_count'] > 0\n",
    "    df_augmented['is_closed'] = df_augmented['state'] == 'closed'\n",
    "    \n",
    "    # 5. Combined text feature for NLP tasks\n",
    "    print(\"üìù Creating combined text field...\")\n",
    "    df_augmented['full_text'] = df_augmented['title'] + ' ' + df_augmented['body']\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset augmentation complete\")\n",
    "    print(f\"\\nüìä New features added: {len(df_augmented.columns) - len(df.columns)}\")\n",
    "    print(f\"Total columns: {len(df_augmented.columns)}\")\n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "# Apply augmentation\n",
    "df_augmented = augment_dataset(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-augmented"
   },
   "outputs": [],
   "source": [
    "# Inspect augmented dataset\n",
    "print(\"üìã Augmented Dataset Sample:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select informative columns for display\n",
    "display_cols = [\n",
    "    'number', 'title', 'state', 'comments_count', \n",
    "    'title_length', 'body_word_count', 'labels_count', 'created_year'\n",
    "]\n",
    "display(df_augmented[display_cols].head(10))\n",
    "\n",
    "print(\"\\nüìä Augmented Features Summary:\")\n",
    "print(df_augmented[[\n",
    "    'title_length', 'body_length', 'title_word_count', \n",
    "    'body_word_count', 'labels_count', 'comments_count'\n",
    "]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4-header"
   },
   "source": [
    "## 4. Uploading the Dataset to the Hugging Face Hub\n",
    "\n",
    "Now we'll prepare and upload our dataset to the Hugging Face Hub:\n",
    "- Convert to HF Dataset format\n",
    "- Create train/test splits\n",
    "- Authenticate with Hugging Face\n",
    "- Push to the Hub\n",
    "\n",
    "> üí° **Sharing Best Practices**: Always include proper documentation and respect privacy when sharing datasets.\n",
    "\n",
    "**Reference**: [HF Course Chapter 5, Section 5](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare-for-upload"
   },
   "outputs": [],
   "source": [
    "# Select columns for the final dataset\n",
    "# Keep the most relevant columns for ML tasks\n",
    "columns_to_keep = [\n",
    "    'id', 'number', 'title', 'body', 'full_text',\n",
    "    'state', 'author', 'comments_count', 'labels_text',\n",
    "    'created_at', 'url',\n",
    "    'title_length', 'body_length', 'title_word_count', 'body_word_count',\n",
    "    'labels_count', 'has_labels', 'has_comments', 'is_closed'\n",
    "]\n",
    "\n",
    "df_final = df_augmented[columns_to_keep].copy()\n",
    "\n",
    "# Convert datetime to string for Dataset compatibility\n",
    "df_final['created_at'] = df_final['created_at'].astype(str)\n",
    "\n",
    "print(\"üì¶ Preparing dataset for upload...\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Columns: {list(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-hf-dataset"
   },
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(\"üîÑ Converting to Hugging Face Dataset format...\")\n",
    "\n",
    "# Create train/test split with repository standard seed=16\n",
    "train_size = 0.8\n",
    "train_df = df_final.sample(frac=train_size, random_state=16)\n",
    "test_df = df_final.drop(train_df.index)\n",
    "\n",
    "print(f\"\\nüìä Split Statistics:\")\n",
    "print(f\"Training set: {len(train_df)} examples ({train_size*100:.0f}%)\")\n",
    "print(f\"Test set: {len(test_df)} examples ({(1-train_size)*100:.0f}%)\")\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ Dataset created successfully\")\n",
    "print(f\"\\nüìã Dataset Info:\")\n",
    "print(dataset_dict)\n",
    "print(f\"\\nüîç Features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "authenticate-hf"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face Hub\n",
    "print(\"üîê Authenticating with Hugging Face Hub...\\n\")\n",
    "\n",
    "# Get HF token\n",
    "hf_token = get_api_key('HF_TOKEN', required=False)\n",
    "\n",
    "AUTHENTICATED = False\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        # Login programmatically\n",
    "        login(token=hf_token)\n",
    "        \n",
    "        # Verify authentication\n",
    "        user_info = whoami()\n",
    "        print(f\"‚úÖ Successfully authenticated as: {user_info['name']}\")\n",
    "        print(f\"üìß Email: {user_info.get('email', 'Not provided')}\")\n",
    "        \n",
    "        AUTHENTICATED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Authentication failed: {e}\")\n",
    "        print(\"üí° Please check your HF_TOKEN and try again\")\n",
    "        AUTHENTICATED = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è HF_TOKEN not found. Cannot upload to Hub.\")\n",
    "    print(\"\\nüìù To get your token:\")\n",
    "    print(\"1. Go to https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Create a new token with 'write' permissions\")\n",
    "    print(\"3. Set it as HF_TOKEN in your environment or Colab secrets\")\n",
    "    AUTHENTICATED = False\n",
    "\n",
    "print(f\"\\nüîê Authentication status: {'Authenticated ‚úÖ' if AUTHENTICATED else 'Not authenticated ‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "push-to-hub"
   },
   "outputs": [],
   "source": [
    "# Push dataset to Hub\n",
    "if AUTHENTICATED:\n",
    "    print(\"üì§ Pushing dataset to Hugging Face Hub...\\n\")\n",
    "    \n",
    "    # Define dataset name\n",
    "    dataset_name = \"github-issues-transformers\"\n",
    "    \n",
    "    try:\n",
    "        # Push to Hub\n",
    "        dataset_dict.push_to_hub(\n",
    "            dataset_name,\n",
    "            private=False,  # Set to True if you want a private dataset\n",
    "            commit_message=\"Upload GitHub issues dataset from HF transformers repo\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset successfully pushed to Hub!\")\n",
    "        print(f\"\\nüîó Dataset URL: https://huggingface.co/datasets/{user_info['name']}/{dataset_name}\")\n",
    "        print(f\"\\nüí° Load your dataset with:\")\n",
    "        print(f\"   from datasets import load_dataset\")\n",
    "        print(f\"   dataset = load_dataset('{user_info['name']}/{dataset_name}')\")\n",
    "        \n",
    "        DATASET_PUSHED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pushing dataset: {e}\")\n",
    "        print(\"üí° Make sure your token has write permissions\")\n",
    "        DATASET_PUSHED = False\n",
    "else:\n",
    "    print(\"‚ùå Cannot push dataset - not authenticated\")\n",
    "    print(\"\\nüí° Dataset push_to_hub features:\")\n",
    "    print(\"  1. Automatic format detection and conversion\")\n",
    "    print(\"  2. Efficient storage using Apache Arrow\")\n",
    "    print(\"  3. Automatic data card generation\")\n",
    "    print(\"  4. Version control and collaboration\")\n",
    "    print(\"  5. Easy sharing and discovery\")\n",
    "    DATASET_PUSHED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5-header"
   },
   "source": [
    "## 5. Creating a Dataset Card\n",
    "\n",
    "A dataset card (README.md) is essential for documenting your dataset:\n",
    "- Describes the dataset purpose and contents\n",
    "- Explains data collection methodology\n",
    "- Lists limitations and biases\n",
    "- Provides usage examples\n",
    "- Includes citation information\n",
    "\n",
    "> üí° **Documentation Matters**: A good dataset card makes your dataset more discoverable and usable by the community.\n",
    "\n",
    "**Reference**: [Dataset Card Guide](https://huggingface.co/docs/hub/datasets-cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataset-card"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive dataset card\n",
    "dataset_card = f\"\"\"\n",
    "# GitHub Issues Dataset - HuggingFace Transformers\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains GitHub issues collected from the [huggingface/transformers](https://github.com/huggingface/transformers) repository. \n",
    "It was created for educational purposes as part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) project,\n",
    "following the guidelines from [HuggingFace Course Chapter 5, Section 5](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt).\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "- **Purpose**: Educational demonstration of dataset creation and sharing\n",
    "- **Source**: GitHub API (huggingface/transformers repository)\n",
    "- **Collection Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Total Issues**: {len(df_final)}\n",
    "- **Train/Test Split**: 80/20 (seed=16 for reproducibility)\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "### Data Fields\n",
    "\n",
    "- `id`: GitHub issue unique identifier\n",
    "- `number`: Issue number in the repository\n",
    "- `title`: Issue title\n",
    "- `body`: Issue description/body text\n",
    "- `full_text`: Combined title and body for NLP tasks\n",
    "- `state`: Issue state (open/closed)\n",
    "- `author`: GitHub username of the issue creator\n",
    "- `comments_count`: Number of comments on the issue\n",
    "- `labels_text`: Comma-separated list of labels\n",
    "- `created_at`: Timestamp when issue was created\n",
    "- `url`: URL to the GitHub issue\n",
    "- `title_length`: Length of title in characters\n",
    "- `body_length`: Length of body in characters\n",
    "- `title_word_count`: Number of words in title\n",
    "- `body_word_count`: Number of words in body\n",
    "- `labels_count`: Number of labels assigned\n",
    "- `has_labels`: Boolean indicating if issue has labels\n",
    "- `has_comments`: Boolean indicating if issue has comments\n",
    "- `is_closed`: Boolean indicating if issue is closed\n",
    "\n",
    "### Data Splits\n",
    "\n",
    "| Split | Examples |\n",
    "|-------|----------|\n",
    "| train | {len(train_df)} |\n",
    "| test  | {len(test_df)} |\n",
    "\n",
    "## Dataset Creation\n",
    "\n",
    "### Source Data\n",
    "\n",
    "Data was collected using the GitHub API from the huggingface/transformers repository.\n",
    "Only actual issues were included (pull requests were filtered out).\n",
    "\n",
    "### Data Collection Process\n",
    "\n",
    "1. **Fetching**: Used GitHub API to retrieve issues\n",
    "2. **Filtering**: Removed pull requests and empty entries\n",
    "3. **Cleaning**: Applied text normalization and cleaning\n",
    "4. **Augmentation**: Added derived features and metadata\n",
    "5. **Splitting**: Created train/test splits with seed=16\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "- HTML entities were unescaped\n",
    "- Excessive whitespace was normalized\n",
    "- Empty titles were filtered out\n",
    "- Text was cleaned and standardized\n",
    "\n",
    "## Considerations for Using the Data\n",
    "\n",
    "### Intended Use\n",
    "\n",
    "This dataset is intended for:\n",
    "- Educational purposes and learning about dataset creation\n",
    "- Text classification experiments\n",
    "- Issue analysis and categorization\n",
    "- NLP model training and evaluation\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Limited to {len(df_final)} issues (snapshot in time)\n",
    "- May not represent all types of issues in the repository\n",
    "- English language only\n",
    "- Subject to GitHub API rate limits during collection\n",
    "- Does not include issue comments (only comment counts)\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "- All data is publicly available on GitHub\n",
    "- Usernames are included as they are public information\n",
    "- No personal or sensitive information was collected\n",
    "- Users should respect GitHub's terms of service when using this data\n",
    "\n",
    "## Additional Information\n",
    "\n",
    "### Dataset Curators\n",
    "\n",
    "Created by [Vu Hung Nguyen](https://github.com/vuhung16au) as part of the HF Transformer Trove educational project.\n",
    "\n",
    "### Licensing Information\n",
    "\n",
    "This dataset is released under the same license as the source repository.\n",
    "Public GitHub data is subject to GitHub's terms of service.\n",
    "\n",
    "### Citation Information\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "\n",
    "```\n",
    "@misc{{github-issues-transformers,\n",
    "  title={{GitHub Issues Dataset - HuggingFace Transformers}},\n",
    "  author={{Nguyen, Vu Hung}},\n",
    "  year={{2024}},\n",
    "  howpublished={{\\\\url{{https://github.com/vuhung16au/hf-transformer-trove}}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "- HuggingFace team for the transformers library and Hub platform\n",
    "- GitHub for providing the API to access public repository data\n",
    "- HuggingFace Course for excellent educational materials\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"YOUR_USERNAME/github-issues-transformers\")\n",
    "\n",
    "# Access train split\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# View first example\n",
    "print(train_data[0])\n",
    "\n",
    "# Filter closed issues\n",
    "closed_issues = dataset.filter(lambda x: x[\"is_closed\"])\n",
    "```\n",
    "\n",
    "## Dataset Card Contact\n",
    "\n",
    "For questions or issues about this dataset, please:\n",
    "- Open an issue on the [HF Transformer Trove repository](https://github.com/vuhung16au/hf-transformer-trove/issues)\n",
    "- Contact: [GitHub Profile](https://github.com/vuhung16au)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Dataset card created!\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Dataset Card Preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(dataset_card[:500] + \"\\n...\\n[truncated for display]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-dataset-card"
   },
   "outputs": [],
   "source": [
    "# Upload dataset card to Hub\n",
    "if AUTHENTICATED and DATASET_PUSHED:\n",
    "    print(\"üì§ Uploading dataset card to Hub...\\n\")\n",
    "    \n",
    "    try:\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Create README.md content\n",
    "        with open(\"/tmp/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(dataset_card)\n",
    "        \n",
    "        # Upload the README\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"/tmp/README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=f\"{user_info['name']}/{dataset_name}\",\n",
    "            repo_type=\"dataset\",\n",
    "            commit_message=\"Add comprehensive dataset card\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Dataset card uploaded successfully!\")\n",
    "        print(f\"\\nüîó View your dataset card at:\")\n",
    "        print(f\"   https://huggingface.co/datasets/{user_info['name']}/{dataset_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading dataset card: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset card created but not uploaded (not authenticated or dataset not pushed)\")\n",
    "    print(\"\\nüíæ You can save the dataset card locally:\")\n",
    "    \n",
    "    # Save locally as example\n",
    "    with open(\"/tmp/dataset_card_README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(dataset_card)\n",
    "    \n",
    "    print(\"‚úÖ Dataset card saved to: /tmp/dataset_card_README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "\n",
    "- **Data Collection**: Learned how to fetch data from APIs (GitHub) with proper error handling and rate limiting\n",
    "- **Data Cleaning**: Applied text preprocessing, normalization, and quality checks\n",
    "- **Feature Engineering**: Created derived features to enhance dataset value\n",
    "- **Dataset Creation**: Converted raw data into structured HuggingFace Dataset format\n",
    "- **Data Splitting**: Created reproducible train/test splits using seed=16\n",
    "- **Hub Upload**: Successfully uploaded dataset to HuggingFace Hub using push_to_hub()\n",
    "- **Documentation**: Created comprehensive dataset card following best practices\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "\n",
    "- **API Respect**: Always respect rate limits and implement proper error handling\n",
    "- **Reproducibility**: Use consistent random seeds (seed=16) for reproducible results\n",
    "- **Data Quality**: Perform thorough cleaning and validation before sharing\n",
    "- **Documentation**: Create detailed dataset cards to help users understand your data\n",
    "- **Privacy**: Only share publicly available data and respect terms of service\n",
    "- **Version Control**: Use HuggingFace Hub for dataset versioning and collaboration\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Now that you've created and uploaded your dataset, you can:\n",
    "\n",
    "1. **Train Models**: Use your dataset to train classification or NLP models\n",
    "2. **Share & Collaborate**: Invite others to use and improve your dataset\n",
    "3. **Iterate & Improve**: Update your dataset based on feedback and new data\n",
    "4. **Explore Analysis**: Perform deeper analysis on issue patterns and trends\n",
    "5. **Create Variants**: Build specialized subsets for specific tasks\n",
    "\n",
    "**Related Notebooks:**\n",
    "- **basic5.3/HF-training-data-preparation.ipynb**: Advanced data preparation techniques\n",
    "- **basic4.3/push_to_hub_API_demo.ipynb**: More about pushing to HuggingFace Hub\n",
    "- **05_fine_tuning_trainer.ipynb**: Fine-tune models on your custom dataset\n",
    "\n",
    "**External Resources:**\n",
    "- [HF Course Chapter 5](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt): Complete guide on datasets\n",
    "- [HF Hub Documentation](https://huggingface.co/docs/hub/datasets): Comprehensive Hub docs\n",
    "- [Datasets Library](https://huggingface.co/docs/datasets/): Full API reference\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
