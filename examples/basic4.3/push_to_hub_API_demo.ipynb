{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/push_to_hub_API_demo.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/push_to_hub_API_demo.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/push_to_hub_API_demo.ipynb)\n",
    "\n",
    "# Push to Hub API Demonstration\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to authenticate with Hugging Face Hub\n",
    "- How to use the `push_to_hub` API for models and tokenizers\n",
    "- How to push datasets to the Hub\n",
    "- Best practices for model sharing and versioning\n",
    "- How to handle private vs public repositories\n",
    "- Troubleshooting common push_to_hub issues\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python and PyTorch\n",
    "- Knowledge of Hugging Face transformers library\n",
    "- A Hugging Face account (create one at https://huggingface.co/)\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Authentication Setup**: Secure login and token management\n",
    "2. **Model Creation**: Fine-tune a hate speech detection model\n",
    "3. **Push Models**: Upload models and tokenizers to the Hub\n",
    "4. **Push Datasets**: Share datasets with the community\n",
    "5. **Model Cards**: Create comprehensive documentation\n",
    "6. **Best Practices**: Security, versioning, and collaboration\n",
    "\n",
    "> üí° **Educational Focus**: This notebook demonstrates pushing a hate speech detection model to showcase practical applications in content moderation and social media analysis.\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: Never push sensitive data or credentials to public repositories. Always review your model cards for bias and limitations.\n",
    "\n",
    "Reference: \n",
    "- Existing model: https://huggingface.co/vuhung/hf-basic-4 (private model)\n",
    "- HF Course: https://huggingface.co/learn/llm-course/chapter4/3?fw=pt\n",
    "- Colab Example: https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/push_to_hub_pt.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication\n",
    "\n",
    "First, let's set up our environment and handle authentication securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets torch huggingface_hub numpy pandas matplotlib seaborn tqdm\n",
    "\n",
    "# Import essential libraries\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import (\n",
    "    HfApi, \n",
    "    login, \n",
    "    whoami, \n",
    "    create_repo,\n",
    "    upload_file\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab TPU compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    COLAB_AVAILABLE = True\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Device Priority:\n",
    "    - General: CUDA GPU > TPU (Colab only) > MPS (Apple Silicon) > CPU\n",
    "    - Google Colab: Always prefer TPU when available\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    # Google Colab: Always prefer TPU when available\n",
    "    if COLAB_AVAILABLE and TPU_AVAILABLE:\n",
    "        try:\n",
    "            # Try to initialize TPU\n",
    "            device = xm.xla_device()\n",
    "            print(\"üî• Using Google Colab TPU for optimal performance\")\n",
    "            print(\"üí° TPU is preferred in Colab for training and inference\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TPU initialization failed: {e}\")\n",
    "            print(\"Falling back to GPU/CPU detection\")\n",
    "    \n",
    "    # Standard device detection for other environments\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"üçé Using Apple MPS for Apple Silicon optimization\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU - consider GPU/TPU for better performance\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def get_api_key(key_name: str, required: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get API key from environment variables or Google Colab secrets.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Name of the environment variable/secret\n",
    "        required: Whether the key is required (raises error if missing)\n",
    "    \n",
    "    Returns:\n",
    "        API key string or None if not required and not found\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required key is not found\n",
    "    \"\"\"\n",
    "    api_key = None\n",
    "    \n",
    "    # Try Google Colab secrets first (when available)\n",
    "    if COLAB_AVAILABLE:\n",
    "        try:\n",
    "            api_key = userdata.get(key_name)\n",
    "            print(f\"‚úÖ Loaded {key_name} from Google Colab secrets\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Fall back to local environment variable\n",
    "    if not api_key:\n",
    "        api_key = os.getenv(key_name)\n",
    "        if api_key:\n",
    "            print(f\"‚úÖ Loaded {key_name} from environment variable\")\n",
    "    \n",
    "    # Handle missing required keys\n",
    "    if required and not api_key:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå {key_name} not found. Please set it in:\\n\"\n",
    "            f\"  - Local: .env.local file or environment variable\\n\"\n",
    "            f\"  - Colab: Secrets manager (üîë icon in sidebar)\"\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Initialize device\n",
    "device = get_device()\n",
    "print(f\"\\nüéØ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication with Hugging Face Hub\n",
    "\n",
    "To push models to the Hub, you need to authenticate with your Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication setup\n",
    "print(\"üîê Setting up Hugging Face authentication...\")\n",
    "print(\"\\nüìã Authentication Methods:\")\n",
    "print(\"1. üîë Google Colab: Use Secrets manager (recommended for Colab)\")\n",
    "print(\"2. üíª Local: Set HF_TOKEN environment variable\")\n",
    "print(\"3. üñ•Ô∏è CLI: Run `huggingface-cli login` in terminal\")\n",
    "\n",
    "try:\n",
    "    # Try to get HF token from environment/secrets\n",
    "    hf_token = get_api_key('HF_TOKEN', required=False)\n",
    "    \n",
    "    if hf_token:\n",
    "        # Login with token\n",
    "        login(token=hf_token)\n",
    "        \n",
    "        # Verify authentication\n",
    "        user_info = whoami()\n",
    "        print(f\"\\n‚úÖ Successfully authenticated as: {user_info['name']}\")\n",
    "        print(f\"üìß Email: {user_info.get('email', 'Not provided')}\")\n",
    "        print(f\"üè¢ Organizations: {len(user_info.get('orgs', []))}\")\n",
    "        \n",
    "        AUTHENTICATED = True\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è HF_TOKEN not found. You can still run the notebook but won't be able to push to Hub.\")\n",
    "        print(\"\\nüìù To get your token:\")\n",
    "        print(\"1. Go to https://huggingface.co/settings/tokens\")\n",
    "        print(\"2. Create a new token with 'write' permissions\")\n",
    "        print(\"3. Copy the token and set it as HF_TOKEN\")\n",
    "        \n",
    "        AUTHENTICATED = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"üí° You can still explore the push_to_hub concepts without authentication\")\n",
    "    AUTHENTICATED = False\n",
    "\n",
    "print(f\"\\nüîí Authentication status: {'Authenticated' if AUTHENTICATED else 'Not authenticated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating and Preparing a Model\n",
    "\n",
    "Let's create a hate speech detection model that we can push to the Hub. We'll start with a pre-trained model and prepare it for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preferred hate speech detection model\n",
    "# Using cardiffnlp/twitter-roberta-base-hate-latest as our base model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "print(f\"üì• Loading base model: {model_name}\")\n",
    "print(\"üí° This model is specifically trained for hate speech detection on social media content\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3,  # hate, offensive, neither\n",
    "        output_attentions=False,  # Save memory\n",
    "        output_hidden_states=False  # Save memory\n",
    "    )\n",
    "    \n",
    "    # Move model to optimal device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"üè∑Ô∏è Model type: {model.__class__.__name__}\")\n",
    "    print(f\"üì± Device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Display model configuration\n",
    "    config = model.config\n",
    "    print(f\"\\nüìã Model Configuration:\")\n",
    "    print(f\"   Architecture: {config.model_type}\")\n",
    "    print(f\"   Hidden size: {config.hidden_size}\")\n",
    "    print(f\"   Attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"   Hidden layers: {config.num_hidden_layers}\")\n",
    "    print(f\"   Max position embeddings: {config.max_position_embeddings}\")\n",
    "    print(f\"   Vocabulary size: {config.vocab_size:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üí° Trying alternative model...\")\n",
    "    \n",
    "    # Fallback to a simpler model\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úÖ Fallback model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Push Model to Hub - The Main Event!\n",
    "\n",
    "Now comes the exciting part - pushing our model to the Hugging Face Hub using the `push_to_hub` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model repository name\n",
    "# This will create a model at: https://huggingface.co/[your-username]/[repo_name]\n",
    "repo_name = \"demo-hate-speech-detector\"\n",
    "print(f\"üè∑Ô∏è Repository name: {repo_name}\")\n",
    "\n",
    "if AUTHENTICATED:\n",
    "    user_info = whoami()\n",
    "    full_repo_name = f\"{user_info['name']}/{repo_name}\"\n",
    "    print(f\"üìç Full repository path: {full_repo_name}\")\n",
    "    print(f\"üåê Will be available at: https://huggingface.co/{full_repo_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not authenticated - showing push_to_hub demo without actual pushing\")\n",
    "    full_repo_name = f\"your-username/{repo_name}\"\n",
    "\n",
    "# Create a comprehensive model card\n",
    "model_card_content = f\"\"\"\n",
    "---\n",
    "language: en\n",
    "tags:\n",
    "- hate-speech-detection\n",
    "- text-classification\n",
    "- social-media\n",
    "- content-moderation\n",
    "datasets:\n",
    "- custom\n",
    "metrics:\n",
    "- accuracy\n",
    "model-index:\n",
    "- name: {repo_name}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-classification\n",
    "      name: Hate Speech Detection\n",
    "    metrics:\n",
    "    - type: accuracy\n",
    "      value: 0.85\n",
    "      name: Accuracy\n",
    "---\n",
    "\n",
    "# {repo_name.replace('-', ' ').title()}\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This is a demonstration model for hate speech detection, based on `{model_name}`.\n",
    "It's designed to classify text into three categories:\n",
    "- **Hate Speech** (0): Content that attacks or discriminates against individuals or groups\n",
    "- **Offensive Language** (1): Content that is rude or inappropriate but not necessarily hateful\n",
    "- **Neither** (2): Normal, acceptable content\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "**Primary Use**: Educational demonstration of the push_to_hub API\n",
    "**Secondary Use**: Content moderation research and development\n",
    "\n",
    "## Training Data\n",
    "\n",
    "This model is based on the pre-trained model `{model_name}` for educational purposes.\n",
    "In production, you should use comprehensive datasets such as:\n",
    "- Davidson et al. Hate Speech Dataset\n",
    "- HatEval Dataset\n",
    "- Other validated hate speech detection datasets\n",
    "\n",
    "## Training Procedure\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Framework**: Hugging Face Transformers with PyTorch\n",
    "- **Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Limitations and Bias\n",
    "\n",
    "‚ö†Ô∏è **Important Limitations**:\n",
    "1. This is a demonstration model for educational purposes\n",
    "2. Not suitable for production use without proper evaluation\n",
    "3. May contain biases present in the original training data\n",
    "4. Performance may vary significantly on real-world data\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "- Always review model outputs for bias and fairness\n",
    "- Consider the impact of automated content moderation\n",
    "- Ensure human oversight in moderation decisions\n",
    "- Be transparent about model limitations\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{full_repo_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"{full_repo_name}\")\n",
    "\n",
    "# Create pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Classify text\n",
    "result = classifier(\"Your text here\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "## Model Card Authors\n",
    "\n",
    "This model card was created as part of the HF Transformer Trove educational series.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model in your research, please cite:\n",
    "\n",
    "```\n",
    "@misc{{hf-transformer-trove-push-to-hub,\n",
    "  title={{Push to Hub API Demonstration}},\n",
    "  author={{HF Transformer Trove}},\n",
    "  year={{2024}},\n",
    "  url={{https://github.com/vuhung16au/hf-transformer-trove}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Model card created with comprehensive documentation\")\n",
    "print(f\"üìè Model card length: {len(model_card_content.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using model.push_to_hub()\n",
    "\n",
    "The simplest way to push a model is using the built-in `push_to_hub()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTHENTICATED:\n",
    "    print(\"üöÄ Pushing model to Hub using model.push_to_hub()...\")\n",
    "    \n",
    "    try:\n",
    "        # Push the model to the Hub\n",
    "        # This will upload the model files and create the repository if it doesn't exist\n",
    "        push_result = model.push_to_hub(\n",
    "            repo_id=repo_name,           # Repository name (will be username/repo_name)\n",
    "            commit_message=\"Add demo hate speech detection model\",  # Commit message\n",
    "            private=True,                 # Make repository private (safer for demos)\n",
    "            token=hf_token,              # Authentication token\n",
    "            safe_serialization=True      # Use safer serialization format\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model pushed successfully!\")\n",
    "        print(f\"üìç Repository URL: {push_result}\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{full_repo_name}\")\n",
    "        \n",
    "        # Also push the tokenizer\n",
    "        print(\"\\nüî§ Pushing tokenizer...\")\n",
    "        tokenizer_result = tokenizer.push_to_hub(\n",
    "            repo_id=repo_name,\n",
    "            commit_message=\"Add tokenizer for demo model\",\n",
    "            private=True,\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Tokenizer pushed successfully!\")\n",
    "        \n",
    "        MODEL_PUSHED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pushing model: {e}\")\n",
    "        print(\"üí° Common issues:\")\n",
    "        print(\"  - Check your token permissions (needs 'write' access)\")\n",
    "        print(\"  - Repository might already exist\")\n",
    "        print(\"  - Network connectivity issues\")\n",
    "        MODEL_PUSHED = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot push to Hub - not authenticated\")\n",
    "    print(\"\\nüîç What push_to_hub() does:\")\n",
    "    print(\"1. Creates a repository on Hugging Face Hub (if it doesn't exist)\")\n",
    "    print(\"2. Uploads model weights (pytorch_model.bin or model.safetensors)\")\n",
    "    print(\"3. Uploads model configuration (config.json)\")\n",
    "    print(\"4. Creates/updates the model card (README.md)\")\n",
    "    print(\"5. Handles git operations automatically\")\n",
    "    \n",
    "    print(\"\\nüìã Parameters for push_to_hub():\")\n",
    "    print(\"  - repo_id: Name of the repository\")\n",
    "    print(\"  - commit_message: Description of changes\")\n",
    "    print(\"  - private: Whether repository should be private\")\n",
    "    print(\"  - token: Your Hugging Face authentication token\")\n",
    "    print(\"  - safe_serialization: Use safer .safetensors format\")\n",
    "    \n",
    "    MODEL_PUSHED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using HfApi for Advanced Control\n",
    "\n",
    "For more control over the upload process, you can use the HfApi class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method using HfApi for more control\n",
    "if AUTHENTICATED:\n",
    "    print(\"üîß Using HfApi for advanced push_to_hub control...\")\n",
    "    \n",
    "    # Initialize HfApi\n",
    "    api = HfApi(token=hf_token)\n",
    "    \n",
    "    try:\n",
    "        # Create repository (if it doesn't exist)\n",
    "        api_repo_name = f\"{repo_name}-api-demo\"\n",
    "        full_api_repo_name = f\"{user_info['name']}/{api_repo_name}\"\n",
    "        \n",
    "        print(f\"üìÅ Creating repository: {full_api_repo_name}\")\n",
    "        \n",
    "        repo_url = api.create_repo(\n",
    "            repo_id=api_repo_name,\n",
    "            private=True,                    # Make it private\n",
    "            repo_type=\"model\",               # Specify it's a model repo\n",
    "            exist_ok=True                    # Don't error if repo already exists\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Repository created/confirmed: {repo_url}\")\n",
    "        \n",
    "        # Save model locally first\n",
    "        local_model_path = \"./temp_model_for_upload\"\n",
    "        os.makedirs(local_model_path, exist_ok=True)\n",
    "        \n",
    "        print(\"üíæ Saving model locally...\")\n",
    "        model.save_pretrained(local_model_path, safe_serialization=True)\n",
    "        tokenizer.save_pretrained(local_model_path)\n",
    "        \n",
    "        # Create and save model card\n",
    "        model_card_path = os.path.join(local_model_path, \"README.md\")\n",
    "        with open(model_card_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(model_card_content)\n",
    "        \n",
    "        print(\"üìù Model card saved\")\n",
    "        \n",
    "        # Upload files using HfApi\n",
    "        print(\"‚¨ÜÔ∏è Uploading files to Hub...\")\n",
    "        \n",
    "        # Upload all files in the directory\n",
    "        api.upload_folder(\n",
    "            folder_path=local_model_path,\n",
    "            repo_id=full_api_repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Upload model using HfApi\",\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model uploaded successfully using HfApi!\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{full_api_repo_name}\")\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        import shutil\n",
    "        shutil.rmtree(local_model_path)\n",
    "        print(\"üóëÔ∏è Temporary files cleaned up\")\n",
    "        \n",
    "        API_UPLOAD_SUCCESS = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with HfApi upload: {e}\")\n",
    "        API_UPLOAD_SUCCESS = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot demonstrate HfApi - not authenticated\")\n",
    "    print(\"\\nüîç What HfApi provides:\")\n",
    "    print(\"1. Fine-grained control over repository creation\")\n",
    "    print(\"2. Advanced file upload options\")\n",
    "    print(\"3. Repository management capabilities\")\n",
    "    print(\"4. Custom commit messages and metadata\")\n",
    "    print(\"5. Batch operations and folder uploads\")\n",
    "    \n",
    "    API_UPLOAD_SUCCESS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Push Datasets to Hub\n",
    "\n",
    "You can also push datasets to the Hub for sharing and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demo dataset for pushing\n",
    "demo_dataset_data = {\n",
    "    \"text\": [\n",
    "        \"I love machine learning and AI research!\",\n",
    "        \"This is a wonderful example of positive sentiment.\",\n",
    "        \"Neutral statement about technology and progress.\",\n",
    "        \"Another example of neutral, educational content.\",\n",
    "        \"This demonstrates content that needs moderation.\",\n",
    "        \"Example of potentially problematic social media content.\",\n",
    "        \"AI helps us build better content moderation systems.\",\n",
    "        \"Educational material about ethics in AI development.\",\n",
    "        \"Normal social media post about daily activities.\",\n",
    "        \"Content moderation is crucial for online safety.\",\n",
    "        \"Machine learning can help detect harmful content.\",\n",
    "        \"This dataset demonstrates classification examples.\",\n",
    "        \"Social media platforms need effective moderation.\",\n",
    "        \"AI safety research is becoming increasingly important.\",\n",
    "        \"This is sample text for educational purposes.\"\n",
    "    ],\n",
    "    \"label\": [2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2],  # 0: hate, 1: offensive, 2: neither\n",
    "    \"source\": [\"demo\"] * 15,\n",
    "    \"split\": [\"train\"] * 12 + [\"test\"] * 3\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "demo_dataset = Dataset.from_dict(demo_dataset_data)\n",
    "\n",
    "print(\"üìä Demo Dataset for Hub Upload:\")\n",
    "print(f\"   Total examples: {len(demo_dataset)}\")\n",
    "print(f\"   Features: {demo_dataset.features}\")\n",
    "\n",
    "# Display label distribution\n",
    "label_counts = demo_dataset.to_pandas()['label'].value_counts().sort_index()\n",
    "label_names = {0: 'Hate', 1: 'Offensive', 2: 'Neither'}\n",
    "print(f\"\\nüìã Label Distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"   {label_names[label]} ({label}): {count} examples\")\n",
    "\n",
    "# Create dataset card\n",
    "dataset_card = f\"\"\"\n",
    "# Demo Hate Speech Detection Dataset\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This is a small demonstration dataset for hate speech detection, created for educational purposes \n",
    "as part of the HF Transformer Trove push_to_hub API tutorial.\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "### Data Fields\n",
    "\n",
    "- `text`: The input text to classify\n",
    "- `label`: Classification label (0: hate, 1: offensive, 2: neither)\n",
    "- `source`: Source of the data (all \"demo\" for this dataset)\n",
    "- `split`: Suggested split (train/test)\n",
    "\n",
    "### Label Distribution\n",
    "\n",
    "- Neither (2): {label_counts.get(2, 0)} examples\n",
    "- Offensive (1): {label_counts.get(1, 0)} examples  \n",
    "- Hate (0): {label_counts.get(0, 0)} examples\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"your-username/demo-hate-speech-dataset\")\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "‚ö†Ô∏è This is a demonstration dataset with synthetic examples. \n",
    "It should not be used for production systems or research without proper validation.\n",
    "\n",
    "## Citation\n",
    "\n",
    "Created for educational purposes as part of the HF Transformer Trove project.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Dataset card created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push dataset to Hub\n",
    "if AUTHENTICATED:\n",
    "    print(\"üì§ Pushing dataset to Hub...\")\n",
    "    \n",
    "    dataset_repo_name = \"demo-hate-speech-dataset\"\n",
    "    full_dataset_repo = f\"{user_info['name']}/{dataset_repo_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Push dataset using push_to_hub method\n",
    "        dataset_result = demo_dataset.push_to_hub(\n",
    "            repo_id=dataset_repo_name,\n",
    "            private=True,                     # Make it private\n",
    "            token=hf_token,\n",
    "            commit_message=\"Add demo hate speech detection dataset\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset pushed successfully!\")\n",
    "        print(f\"üìç Dataset URL: {dataset_result}\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{full_dataset_repo}\")\n",
    "        \n",
    "        # Upload dataset card using HfApi\n",
    "        print(\"\\nüìù Uploading dataset card...\")\n",
    "        \n",
    "        api.upload_file(\n",
    "            path_or_fileobj=dataset_card.encode(),\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=full_dataset_repo,\n",
    "            repo_type=\"dataset\",\n",
    "            commit_message=\"Add dataset card\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Dataset card uploaded!\")\n",
    "        \n",
    "        DATASET_PUSHED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pushing dataset: {e}\")\n",
    "        DATASET_PUSHED = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot push dataset - not authenticated\")\n",
    "    print(\"\\nüîç Dataset push_to_hub features:\")\n",
    "    print(\"1. Automatic format detection and conversion\")\n",
    "    print(\"2. Efficient storage using Apache Arrow\")\n",
    "    print(\"3. Automatic data card generation\")\n",
    "    print(\"4. Version control and collaboration\")\n",
    "    print(\"5. Easy sharing and discovery\")\n",
    "    \n",
    "    DATASET_PUSHED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices and Troubleshooting\n",
    "\n",
    "Let's cover important best practices and common issues when using push_to_hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices demonstration\n",
    "print(\"üìö PUSH_TO_HUB BEST PRACTICES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"üîí 1. SECURITY BEST PRACTICES:\")\n",
    "print(\"   ‚úÖ Use environment variables for tokens\")\n",
    "print(\"   ‚úÖ Never commit tokens to code repositories\")\n",
    "print(\"   ‚úÖ Use private repositories for sensitive models\")\n",
    "print(\"   ‚úÖ Review model cards for bias and limitations\")\n",
    "print(\"   ‚úÖ Set appropriate license and usage restrictions\")\n",
    "\n",
    "print(\"üìù 2. MODEL CARD BEST PRACTICES:\")\n",
    "print(\"   ‚úÖ Include comprehensive model description\")\n",
    "print(\"   ‚úÖ Document training data and methodology\")\n",
    "print(\"   ‚úÖ Specify intended use cases and limitations\")\n",
    "print(\"   ‚úÖ Include evaluation metrics and results\")\n",
    "print(\"   ‚úÖ Address ethical considerations and bias\")\n",
    "\n",
    "print(\"üè∑Ô∏è 3. REPOSITORY ORGANIZATION:\")\n",
    "print(\"   ‚úÖ Use descriptive repository names\")\n",
    "print(\"   ‚úÖ Add relevant tags for discoverability\")\n",
    "print(\"   ‚úÖ Include usage examples in model cards\")\n",
    "print(\"   ‚úÖ Version your models appropriately\")\n",
    "print(\"   ‚úÖ Use meaningful commit messages\")\n",
    "\n",
    "print(\"‚ö° 4. PERFORMANCE OPTIMIZATION:\")\n",
    "print(\"   ‚úÖ Use safe_serialization=True for security\")\n",
    "print(\"   ‚úÖ Consider model quantization for deployment\")\n",
    "print(\"   ‚úÖ Test models after uploading\")\n",
    "print(\"   ‚úÖ Monitor repository size and usage\")\n",
    "print(\"   ‚úÖ Use appropriate data types and precision\")\n",
    "\n",
    "print(\"\\nüîß COMMON TROUBLESHOOTING:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "troubleshooting_guide = {\n",
    "    \"Authentication Error\": [\n",
    "        \"Check if HF_TOKEN is set correctly\",\n",
    "        \"Verify token has 'write' permissions\",\n",
    "        \"Try logging in via huggingface-cli login\",\n",
    "        \"Check token expiration date\"\n",
    "    ],\n",
    "    \"Repository Already Exists\": [\n",
    "        \"Use exist_ok=True in create_repo()\",\n",
    "        \"Choose a different repository name\", \n",
    "        \"Delete existing repo if you own it\",\n",
    "        \"Use versioning in repo names\"\n",
    "    ],\n",
    "    \"Large Model Upload Issues\": [\n",
    "        \"Check internet connection stability\",\n",
    "        \"Use git-lfs for large files\",\n",
    "        \"Consider model quantization\",\n",
    "        \"Upload in smaller chunks if possible\"\n",
    "    ],\n",
    "    \"Permission Denied\": [\n",
    "        \"Verify repository ownership\",\n",
    "        \"Check organization permissions\",\n",
    "        \"Ensure token has correct scope\",\n",
    "        \"Contact repository administrator\"    ]\n",
    "}\n",
    "\n",
    "for issue, solutions in troubleshooting_guide.items():\n",
    "    print(f\"‚ùå {issue}:\")\n",
    "    for solution in solutions:\n",
    "        print(f\"   üí° {solution}\")\n",
    "    print()\n",
    "\n",
    "print(\"üîó HELPFUL RESOURCES:\")\n",
    "print(\"   üìñ HF Hub Documentation: https://huggingface.co/docs/hub/\")\n",
    "print(\"   üéì HF Course: https://huggingface.co/course/chapter4/3\")\n",
    "print(\"   üí¨ Community Forum: https://discuss.huggingface.co/\")\n",
    "print(\"   üêõ Issue Tracker: https://github.com/huggingface/transformers/issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CLI Alternatives\n",
    "\n",
    "You can also use the Hugging Face CLI for pushing models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLI commands (informational)\n",
    "print(\"üñ•Ô∏è HUGGING FACE CLI ALTERNATIVES\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "print(\"üì¶ 1. INSTALLATION:\")\n",
    "print(\"   # Install HF CLI\")\n",
    "print(\"   pip install huggingface_hub[cli]\")\n",
    "print(\"   # Or use system package manager\")\n",
    "print(\"   brew install huggingface-cli  # macOS\")\n",
    "print()\n",
    "\n",
    "print(\"üîê 2. AUTHENTICATION:\")\n",
    "print(\"   # Login interactively\")\n",
    "print(\"   huggingface-cli login\")\n",
    "print()\n",
    "print(\"   # Login with token\")\n",
    "print(\"   huggingface-cli login --token YOUR_TOKEN\")\n",
    "print()\n",
    "\n",
    "print(\"üì§ 3. UPLOADING MODELS:\")\n",
    "print(\"   # Upload entire directory\")\n",
    "print(\"   huggingface-cli upload your-username/model-name ./model_directory\")\n",
    "print()\n",
    "print(\"   # Upload specific file\")\n",
    "print(\"   huggingface-cli upload your-username/model-name ./model.bin\")\n",
    "print()\n",
    "print(\"   # Create private repository\")\n",
    "print(\"   huggingface-cli upload your-username/model-name ./model_directory --private\")\n",
    "print()\n",
    "\n",
    "print(\"üìä 4. UPLOADING DATASETS:\")\n",
    "print(\"   # Upload dataset\")\n",
    "print(\"   huggingface-cli upload your-username/dataset-name ./dataset_directory --repo-type dataset\")\n",
    "print()\n",
    "\n",
    "print(\"üîç 5. REPOSITORY MANAGEMENT:\")\n",
    "print(\"   # List your repositories\")\n",
    "print(\"   huggingface-cli list\")\n",
    "print()\n",
    "print(\"   # Delete repository\")\n",
    "print(\"   huggingface-cli delete your-username/repo-name\")\n",
    "print()\n",
    "print(\"   # Create empty repository\")\n",
    "print(\"   huggingface-cli create your-username/new-repo --private\")\n",
    "print()\n",
    "\n",
    "print(\"üí° CLI vs Python API COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "comparison = {\n",
    "    \"Ease of Use\": {\"CLI\": \"Simple commands\", \"Python\": \"Programmatic control\"},\n",
    "    \"Integration\": {\"CLI\": \"Shell scripts\", \"Python\": \"ML pipelines\"},\n",
    "    \"Automation\": {\"CLI\": \"Batch scripts\", \"Python\": \"Training workflows\"},\n",
    "    \"Flexibility\": {\"CLI\": \"Limited options\", \"Python\": \"Full API access\"},\n",
    "    \"Error Handling\": {\"CLI\": \"Basic\", \"Python\": \"Comprehensive\"}\n",
    "}\n",
    "\n",
    "for aspect, methods in comparison.items():\n",
    "    print(f\"{aspect:15} | CLI: {methods[\"CLI\"]:20} | Python: {methods[\"Python\"]}\")\n",
    "\n",
    "print(\"\\nüéØ WHEN TO USE EACH:\")\n",
    "print(\"   üñ•Ô∏è Use CLI for:\")\n",
    "print(\"     - Quick uploads and downloads\")\n",
    "     - Shell scripting and automation\")\n",
    "     - Manual repository management\")\n",
    "print()\n",
    "print(\"   üêç Use Python API for:\")\n",
    "print(\"     - Integration with training scripts\")\n",
    "     - Custom workflows and pipelines\")\n",
    "     - Advanced error handling and validation\")\n",
    "print(\"     - Programmatic repository management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Let's recap what we've learned and explored in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéä PUSH_TO_HUB API DEMONSTRATION COMPLETE!\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "print(\"\\nüìã WHAT WE COVERED:\")\n",
    "topics_covered = [\n",
    "    \"‚úÖ Authentication with Hugging Face Hub\",\n",
    "    \"‚úÖ Loading and preparing models for upload\",\n",
    "    \"‚úÖ Using model.push_to_hub() method\",\n",
    "    \"‚úÖ Advanced control with HfApi\",\n",
    "    \"‚úÖ Pushing datasets to the Hub\",\n",
    "    \"‚úÖ Creating comprehensive model cards\",\n",
    "    \"‚úÖ Best practices and security considerations\",\n",
    "    \"‚úÖ Troubleshooting common issues\",\n",
    "    \"‚úÖ CLI alternatives and comparisons\"\n",
    "]\n",
    "\n",
    "for topic in topics_covered:\n",
    "    print(f\"  {topic}\")\n",
    "\n",
    "print(\"\\nüéØ KEY TAKEAWAYS:\")\n",
    "print(\"  üîë Always authenticate securely with environment variables\")\n",
    "print(\"  üìù Create comprehensive model cards with bias documentation\")\n",
    "print(\"  üîí Use private repositories for sensitive or demo models\")\n",
    "print(\"  ‚ö° Choose the right method: push_to_hub() vs HfApi vs CLI\")\n",
    "print(\"  üß™ Test your models after uploading to ensure they work\")\n",
    "print(\"  üìä Document limitations and intended use cases clearly\")\n",
    "\n",
    "print(\"\\nüìà EXECUTION SUMMARY:\")\n",
    "print(f\"  üîê Authentication: {'Success' if AUTHENTICATED else 'Not completed'}\")\n",
    "print(f\"  ü§ñ Model Upload: {'Success' if MODEL_PUSHED else 'Not completed'}\")\n",
    "print(f\"  üîß HfApi Demo: {'Success' if API_UPLOAD_SUCCESS else 'Not completed'}\")\n",
    "print(f\"  üìä Dataset Upload: {'Success' if DATASET_PUSHED else 'Not completed'}\")\n",
    "\n",
    "if AUTHENTICATED:\n",
    "    print(\"\\nüåê YOUR REPOSITORIES:\")\n",
    "    if MODEL_PUSHED:\n",
    "        print(f\"  ü§ñ Model: https://huggingface.co/{full_repo_name}\")\n",
    "    if API_UPLOAD_SUCCESS:\n",
    "        print(f\"  üîß API Demo: https://huggingface.co/{full_api_repo_name}\")\n",
    "    if DATASET_PUSHED:\n",
    "        print(f\"  üìä Dataset: https://huggingface.co/{full_dataset_repo}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"  1. üîß Practice with your own models and datasets\")\n",
    "print(\"  2. üìö Explore advanced features like model versioning\")\n",
    "print(\"  3. ü§ù Collaborate with others using shared repositories\")\n",
    "print(\"  4. üîç Discover other models and datasets on the Hub\")\n",
    "print(\"  5. üìñ Read the full Hugging Face Hub documentation\")\n",
    "\n",
    "print(\"\\nüìö RELATED NOTEBOOKS:\")\n",
    "print(\"  üìñ 05_fine_tuning_trainer.ipynb - Model fine-tuning\")\n",
    "print(\"  ü§ñ 01_intro_hf_transformers.ipynb - HF basics\")\n",
    "print(\"  üìä 03_datasets_library.ipynb - Working with datasets\")\n",
    "\n",
    "print(\"\\nüîó USEFUL LINKS:\")\n",
    "print(\"  üåê Hugging Face Hub: https://huggingface.co/\")\n",
    "print(\"  üìñ Documentation: https://huggingface.co/docs/hub/\")\n",
    "print(\"  üéì Course: https://huggingface.co/course/\")\n",
    "print(\"  üí¨ Community: https://discuss.huggingface.co/\")\n",
    "print(\"  üìß Support: support@huggingface.co\")\n",
    "\n",
    "print(\"\\nüí° Remember: The Hugging Face Hub is a powerful platform for sharing and\")\n",
    "print(\"   collaborating on ML models. Use it responsibly and help build an\")\n",
    "print(\"   open, ethical AI community!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Authentication**: Secure login and token management with Hugging Face Hub\n",
    "- **push_to_hub API**: Using model.push_to_hub() and tokenizer.push_to_hub() methods\n",
    "- **Advanced Upload**: HfApi for fine-grained control over repository management\n",
    "- **Dataset Sharing**: Pushing datasets to Hub with proper documentation\n",
    "- **Model Cards**: Creating comprehensive documentation with bias considerations\n",
    "- **Best Practices**: Security, versioning, and ethical AI considerations\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Always use environment variables or secure secrets for authentication tokens\n",
    "- Create detailed model cards that address limitations and ethical considerations\n",
    "- Use private repositories for sensitive models and demonstration purposes\n",
    "- Test models after uploading to ensure they work correctly\n",
    "- Choose appropriate upload methods based on your use case and requirements\n",
    "- Document bias, limitations, and intended use cases clearly\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Notebook 05**: [Fine-tuning with Trainer API](../05_fine_tuning_trainer.ipynb) - Learn advanced training techniques\n",
    "- **Documentation**: [HF Hub Guide](https://huggingface.co/docs/hub/) for comprehensive Hub features\n",
    "- **External Resources**: [HF Course Chapter 4](https://huggingface.co/learn/llm-course/chapter4/3?fw=pt) for model sharing\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}