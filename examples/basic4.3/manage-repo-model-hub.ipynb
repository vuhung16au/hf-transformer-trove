{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/manage-repo-model-hub.ipynb)\n",
    "[![Open with SageMaker](https://img.shields.io/badge/Open%20with-SageMaker-orange?logo=amazonaws)](https://studiolab.sagemaker.aws/import/github/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/manage-repo-model-hub.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/basic4.3/manage-repo-model-hub.ipynb)\n",
    "\n",
    "# Managing Repositories on Hugging Face Model Hub\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to authenticate with Hugging Face Hub\n",
    "- Creating and managing model repositories programmatically\n",
    "- Uploading models, tokenizers, and files to the Hub\n",
    "- Version control and repository management using Git and Python API\n",
    "- Creating comprehensive model cards for documentation\n",
    "- Best practices for model sharing and collaboration\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning models\n",
    "- Familiarity with Git version control\n",
    "- Hugging Face account (free at [huggingface.co](https://huggingface.co))\n",
    "- Knowledge of Transformers library basics\n",
    "\n",
    "## üìö What We'll Cover\n",
    "1. **Authentication & Setup**: Secure credential management\n",
    "2. **Repository Creation**: Creating model repositories on the Hub\n",
    "3. **File Management**: Uploading models, configs, and documentation\n",
    "4. **Python API Methods**: Using `huggingface_hub` library\n",
    "5. **Git Integration**: Traditional Git workflow for model repositories\n",
    "6. **Model Cards**: Creating comprehensive documentation\n",
    "7. **Best Practices**: Production-ready repository management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Authentication\n",
    "\n",
    "First, let's set up our environment and handle authentication securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers huggingface_hub datasets torch\n",
    "\n",
    "# Essential imports for Hugging Face Hub management\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Hugging Face ecosystem imports\n",
    "from huggingface_hub import (\n",
    "    HfApi, \n",
    "    Repository, \n",
    "    login,\n",
    "    whoami,\n",
    "    create_repo,\n",
    "    upload_file,\n",
    "    upload_folder,\n",
    "    delete_file,\n",
    "    list_models,\n",
    "    model_info\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"üì¶ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab TPU compatibility\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    COLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "\n",
    "def get_api_key(key_name: str, required: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load API key from environment or Google Colab secrets.\n",
    "    \n",
    "    Args:\n",
    "        key_name: Environment variable name\n",
    "        required: Whether to raise error if not found\n",
    "        \n",
    "    Returns:\n",
    "        API key string or None\n",
    "    \"\"\"\n",
    "    # Try Colab secrets first\n",
    "    if COLAB_AVAILABLE:\n",
    "        try:\n",
    "            return userdata.get(key_name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    api_key = os.getenv(key_name)\n",
    "    \n",
    "    if required and not api_key:\n",
    "        raise ValueError(\n",
    "            f\"{key_name} not found. Set it in:\\n\"\n",
    "            f\"- Local: .env.local file\\n\"\n",
    "            f\"- Colab: Secrets manager\"\n",
    "        )\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Automatically detect and return the best available device.\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") \n",
    "        print(\"üçé Using Apple MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Using CPU (consider GPU for better performance)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Set optimal device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Hugging Face Authentication\n",
    "\n",
    "To manage repositories on Hugging Face Hub, you need to authenticate. There are several ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load token from environment/secrets (recommended)\n",
    "try:\n",
    "    hf_token = get_api_key(\"HF_TOKEN\", required=False)\n",
    "    \n",
    "    if hf_token:\n",
    "        # Login programmatically\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Successfully authenticated with Hugging Face Hub\")\n",
    "        \n",
    "        # Verify authentication\n",
    "        user_info = whoami()\n",
    "        print(f\"üë§ Logged in as: {user_info['name']}\")\n",
    "        print(f\"üìß Email: {user_info.get('email', 'Not provided')}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No HF_TOKEN found. You can:\")\n",
    "        print(\"   1. Set HF_TOKEN environment variable\")\n",
    "        print(\"   2. Add HF_TOKEN to Colab secrets (if using Colab)\")\n",
    "        print(\"   3. Run: huggingface-cli login (in terminal)\")\n",
    "        print(\"   4. Use login() interactively (next cell)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"üí° Trying alternative authentication methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Interactive login (if token not available)\n",
    "# Uncomment and run this if you need to login interactively\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()  # This will prompt for your token\n",
    "\n",
    "# Method 3: Command line login (run in terminal)\n",
    "# !huggingface-cli login\n",
    "\n",
    "print(\"üí° Authentication Methods:\")\n",
    "print(\"1. Environment Variable: Set HF_TOKEN in your environment\")\n",
    "print(\"2. Colab Secrets: Add HF_TOKEN to Google Colab secrets manager\")\n",
    "print(\"3. Command Line: Run 'huggingface-cli login' in terminal\")\n",
    "print(\"4. Interactive: Use login() function (prompts for token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Repository Creation and Management\n",
    "\n",
    "Now let's learn how to create and manage repositories on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face API client\n",
    "api = HfApi()\n",
    "\n",
    "# Example repository configuration\n",
    "REPO_CONFIG = {\n",
    "    \"repo_id\": \"your-username/demo-hate-speech-detector\",  # Replace with your username\n",
    "    \"repo_type\": \"model\",\n",
    "    \"private\": False,  # Set to True for private repositories\n",
    "    \"description\": \"Educational demo: Fine-tuned RoBERTa for hate speech detection\"\n",
    "}\n",
    "\n",
    "def create_model_repository(repo_id: str, private: bool = False, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Create a new model repository on Hugging Face Hub.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier (username/repo-name)\n",
    "        private: Whether to create private repository\n",
    "        description: Repository description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create repository\n",
    "        repo_url = create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            private=private,\n",
    "            exist_ok=True  # Don't fail if repository already exists\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Repository created/verified: {repo_url}\")\n",
    "        print(f\"üîó Visit: https://huggingface.co/{repo_id}\")\n",
    "        return repo_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating repository: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Create repository (uncomment to try)\n",
    "# repo_url = create_model_repository(\n",
    "#     repo_id=REPO_CONFIG[\"repo_id\"],\n",
    "#     private=REPO_CONFIG[\"private\"],\n",
    "#     description=REPO_CONFIG[\"description\"]\n",
    "# )\n",
    "\n",
    "print(\"üìù Repository creation function ready!\")\n",
    "print(f\"üí° Example repo ID: {REPO_CONFIG['repo_id']}\")\n",
    "print(\"‚ö†Ô∏è  Remember to replace 'your-username' with your actual Hugging Face username\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working with the Reference Model (vuhung/hf-basic-4)\n",
    "\n",
    "Let's explore the existing model referenced in the issue and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference model from the issue\n",
    "REFERENCE_MODEL = \"vuhung/hf-basic-4\"\n",
    "\n",
    "def explore_model_repository(repo_id: str):\n",
    "    \"\"\"\n",
    "    Explore an existing model repository to understand its structure.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier to explore\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get model information\n",
    "        info = model_info(repo_id)\n",
    "        \n",
    "        print(f\"üîç Exploring Repository: {repo_id}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìã Task: {info.pipeline_tag or 'Not specified'}\")\n",
    "        print(f\"üìö Library: {info.library_name or 'Not specified'}\")\n",
    "        print(f\"üíæ Downloads: {info.downloads:,}\")\n",
    "        print(f\"üëç Likes: {info.likes}\")\n",
    "        print(f\"üè™ Created: {info.created_at}\")\n",
    "        print(f\"üìÖ Last Modified: {info.last_modified}\")\n",
    "        \n",
    "        # List files in the repository\n",
    "        if hasattr(info, 'siblings') and info.siblings:\n",
    "            print(f\"\\nüìÅ Repository Files:\")\n",
    "            for file_info in info.siblings:\n",
    "                size_mb = file_info.size / (1024 * 1024) if file_info.size else 0\n",
    "                print(f\"   üìÑ {file_info.rfilename} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Display tags if available\n",
    "        if info.tags:\n",
    "            print(f\"\\nüè∑Ô∏è  Tags: {', '.join(info.tags[:10])}\")\n",
    "        \n",
    "        return info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exploring repository: {e}\")\n",
    "        print(f\"üí° This might be a private repository or authentication issue\")\n",
    "        return None\n",
    "\n",
    "# Explore the reference model\n",
    "model_info_data = explore_model_repository(REFERENCE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test our preferred hate speech detection model\n",
    "# (Following repository focus on hate speech detection)\n",
    "PREFERRED_MODEL = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "\n",
    "def load_hate_speech_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Load and test a hate speech detection model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to load\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üì• Loading model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # Move to optimal device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully on {device}\")\n",
    "        print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "        \n",
    "        # Create pipeline for easy usage\n",
    "        classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if device.type == 'cuda' else -1\n",
    "        )\n",
    "        \n",
    "        # Test with examples\n",
    "        test_texts = [\n",
    "            \"This is a wonderful community project!\",\n",
    "            \"I strongly disagree with this approach.\",\n",
    "            \"Thanks for the helpful tutorial.\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüß™ Testing model with sample texts:\")\n",
    "        for text in test_texts:\n",
    "            result = classifier(text)\n",
    "            print(f\"   Text: '{text[:40]}...'\")\n",
    "            print(f\"   Result: {result[0]['label']} (confidence: {result[0]['score']:.3f})\")\n",
    "            print()\n",
    "        \n",
    "        return tokenizer, model, classifier\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load our preferred hate speech detection model\n",
    "tokenizer, model, classifier = load_hate_speech_model(PREFERRED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: File Upload Methods\n",
    "\n",
    "Learn different ways to upload files to your Hugging Face repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Upload individual files\n",
    "def upload_individual_file(repo_id: str, local_file_path: str, repo_file_path: str):\n",
    "    \"\"\"\n",
    "    Upload a single file to the repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier\n",
    "        local_file_path: Path to local file\n",
    "        repo_file_path: Path in the repository\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = upload_file(\n",
    "            path_or_fileobj=local_file_path,\n",
    "            path_in_repo=repo_file_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        print(f\"‚úÖ File uploaded: {repo_file_path}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: Upload entire folder\n",
    "def upload_model_folder(repo_id: str, local_folder_path: str):\n",
    "    \"\"\"\n",
    "    Upload an entire folder containing model files.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier\n",
    "        local_folder_path: Path to local folder\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = upload_folder(\n",
    "            folder_path=local_folder_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        print(f\"‚úÖ Folder uploaded: {local_folder_path} -> {repo_id}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading folder: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 3: Save and upload model directly\n",
    "def save_and_upload_model(model, tokenizer, repo_id: str, local_save_path: str = \"./temp_model\"):\n",
    "    \"\"\"\n",
    "    Save model locally and upload to Hub.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save\n",
    "        tokenizer: The tokenizer to save\n",
    "        repo_id: Repository identifier\n",
    "        local_save_path: Temporary local save path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create local directory\n",
    "        os.makedirs(local_save_path, exist_ok=True)\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        print(f\"üíæ Saving model to {local_save_path}...\")\n",
    "        model.save_pretrained(local_save_path)\n",
    "        tokenizer.save_pretrained(local_save_path)\n",
    "        \n",
    "        # Upload to Hub\n",
    "        print(f\"üì§ Uploading to {repo_id}...\")\n",
    "        result = upload_folder(\n",
    "            folder_path=local_save_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{repo_id}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving/uploading model: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"üì§ File upload methods ready!\")\n",
    "print(\"üí° These functions can be used to upload individual files, folders, or complete models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating Comprehensive Model Cards\n",
    "\n",
    "Model cards are crucial for documenting your models. Let's create a comprehensive model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_card(model_name: str, task: str, performance_metrics: Dict[str, float] = None) -> str:\n",
    "    \"\"\"\n",
    "    Create a comprehensive model card following Hugging Face standards.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        task: Primary task the model performs\n",
    "        performance_metrics: Dictionary of performance metrics\n",
    "    \n",
    "    Returns:\n",
    "        Formatted model card as markdown string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default metrics if none provided\n",
    "    if performance_metrics is None:\n",
    "        performance_metrics = {\n",
    "            \"accuracy\": 0.892,\n",
    "            \"f1_score\": 0.885,\n",
    "            \"precision\": 0.878,\n",
    "            \"recall\": 0.893\n",
    "        }\n",
    "    \n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "license: mit\n",
    "library_name: transformers\n",
    "pipeline_tag: text-classification\n",
    "tags:\n",
    "- hate-speech-detection\n",
    "- text-classification\n",
    "- roberta\n",
    "- social-media\n",
    "- content-moderation\n",
    "datasets:\n",
    "- tdavidson/hate_speech_offensive\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "- precision\n",
    "- recall\n",
    "model-index:\n",
    "- name: {model_name}\n",
    "  results:\n",
    "  - task:\n",
    "      name: Text Classification\n",
    "      type: text-classification\n",
    "    dataset:\n",
    "      name: Hate Speech Detection\n",
    "      type: hate_speech_offensive\n",
    "    metrics:\n",
    "    - name: Accuracy\n",
    "      type: accuracy\n",
    "      value: {performance_metrics.get('accuracy', 0.0):.3f}\n",
    "    - name: F1 Score\n",
    "      type: f1\n",
    "      value: {performance_metrics.get('f1_score', 0.0):.3f}\n",
    "---\n",
    "\n",
    "# {model_name.title()}\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is a fine-tuned version of RoBERTa specifically designed for {task}. It has been trained on curated datasets to identify and classify potentially harmful content, making it suitable for content moderation applications.\n",
    "\n",
    "### Model Details\n",
    "\n",
    "- **Developed by:** [Your Name/Organization]\n",
    "- **Model type:** RoBERTa for Sequence Classification\n",
    "- **Language(s):** English\n",
    "- **License:** MIT\n",
    "- **Finetuned from model:** cardiffnlp/twitter-roberta-base-hate-latest\n",
    "\n",
    "### Intended Uses\n",
    "\n",
    "#### Intended Use Cases\n",
    "- Content moderation for social media platforms\n",
    "- Automated hate speech detection in user-generated content\n",
    "- Research on hate speech detection and bias in NLP models\n",
    "- Educational purposes for learning about text classification\n",
    "\n",
    "#### Out-of-Scope Use Cases\n",
    "- Should not be used as the sole basis for content removal decisions\n",
    "- Not suitable for legal or judicial decision-making\n",
    "- Should not be used without human oversight in high-stakes scenarios\n",
    "\n",
    "## Training Data\n",
    "\n",
    "This model was trained on the Davidson et al. hate speech dataset, which contains:\n",
    "- **Training samples:** ~20,000 labeled tweets\n",
    "- **Labels:** Hate speech, Offensive language, Neither\n",
    "- **Language:** English\n",
    "- **Domain:** Social media (Twitter)\n",
    "\n",
    "## Performance\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Accuracy | {performance_metrics.get('accuracy', 0.0):.3f} |\n",
    "| F1 Score | {performance_metrics.get('f1_score', 0.0):.3f} |\n",
    "| Precision | {performance_metrics.get('precision', 0.0):.3f} |\n",
    "| Recall | {performance_metrics.get('recall', 0.0):.3f} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Direct Use\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "classifier = pipeline(\"text-classification\", model=\"{model_name}\")\n",
    "\n",
    "# Use the model\n",
    "text = \"This is a sample text for classification\"\n",
    "result = classifier(text)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"{model_name}\")\n",
    "\n",
    "# Tokenize and predict\n",
    "inputs = tokenizer(\"Your text here\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "```\n",
    "\n",
    "## Limitations and Bias\n",
    "\n",
    "### Known Limitations\n",
    "- Performance may vary on text from domains different from social media\n",
    "- May exhibit bias towards certain demographic groups\n",
    "- Context-dependent hate speech may be challenging to detect\n",
    "- Limited to English language content\n",
    "\n",
    "### Bias Analysis\n",
    "This model has been trained on social media data which may contain inherent biases. Users should:\n",
    "- Test the model on their specific use case\n",
    "- Monitor for bias in predictions\n",
    "- Use human oversight for final decisions\n",
    "\n",
    "## Training Procedure\n",
    "\n",
    "### Training Hyperparameters\n",
    "- Learning rate: 2e-5\n",
    "- Batch size: 16\n",
    "- Number of epochs: 3\n",
    "- Optimizer: AdamW\n",
    "- Weight decay: 0.01\n",
    "\n",
    "### Framework Versions\n",
    "- Transformers: 4.35.0\n",
    "- PyTorch: 2.1.0\n",
    "- Datasets: 2.14.0\n",
    "- Tokenizers: 0.15.0\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "\n",
    "```bibtex\n",
    "@misc{{{model_name.replace('/', '_')},\n",
    "  author = {{Your Name}},\n",
    "  title = {{{model_name.title()}: Fine-tuned RoBERTa for Hate Speech Detection}},\n",
    "  year = {{2024}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{model_name}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions or issues, please contact [your-email@example.com]\n",
    "\n",
    "---\n",
    "\n",
    "*This model is part of the educational series from [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove).*\n",
    "\"\"\"\n",
    "    \n",
    "    return model_card\n",
    "\n",
    "# Create example model card\n",
    "example_model_card = create_model_card(\n",
    "    model_name=\"your-username/hate-speech-detector-v1\",\n",
    "    task=\"hate speech detection\"\n",
    ")\n",
    "\n",
    "print(\"üìù Model card created successfully!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìÑ PREVIEW OF MODEL CARD:\")\n",
    "print(\"=\"*50)\n",
    "print(example_model_card[:1000] + \"...\\n[truncated for preview]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model card to file and upload\n",
    "def save_and_upload_model_card(repo_id: str, model_card_content: str, local_path: str = \"./README.md\"):\n",
    "    \"\"\"\n",
    "    Save model card locally and upload to repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier\n",
    "        model_card_content: Model card content as string\n",
    "        local_path: Local path to save the model card\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save locally\n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(model_card_content)\n",
    "        \n",
    "        print(f\"üíæ Model card saved locally: {local_path}\")\n",
    "        \n",
    "        # Upload to repository\n",
    "        result = upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add comprehensive model card with usage examples and bias documentation\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model card uploaded to {repo_id}\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{repo_id}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving/uploading model card: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to try)\n",
    "# save_and_upload_model_card(REPO_CONFIG[\"repo_id\"], example_model_card)\n",
    "\n",
    "print(\"üì§ Model card upload function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Git Integration for Advanced Repository Management\n",
    "\n",
    "For more complex workflows, you can use Git directly with Hugging Face repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git-based repository management\n",
    "def clone_and_manage_repo(repo_id: str, local_dir: str = \"./temp_repo\"):\n",
    "    \"\"\"\n",
    "    Clone repository and demonstrate Git-based management.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier\n",
    "        local_dir: Local directory for cloning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Cloning repository: {repo_id}\")\n",
    "        \n",
    "        # Clone repository using huggingface_hub\n",
    "        repo = Repository(\n",
    "            local_dir=local_dir,\n",
    "            clone_from=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            use_auth_token=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Repository cloned to: {local_dir}\")\n",
    "        \n",
    "        # List files in repository\n",
    "        import os\n",
    "        files = os.listdir(local_dir)\n",
    "        print(f\"üìÅ Files in repository: {files}\")\n",
    "        \n",
    "        return repo\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e}\")\n",
    "        return None\n",
    "\n",
    "def git_workflow_example(repo: Repository, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Demonstrate Git workflow for model updates.\n",
    "    \n",
    "    Args:\n",
    "        repo: Repository object\n",
    "        model: Model to save\n",
    "        tokenizer: Tokenizer to save\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Git workflow demonstration:\")\n",
    "        \n",
    "        # 1. Pull latest changes\n",
    "        print(\"1. Pulling latest changes...\")\n",
    "        repo.git_pull()\n",
    "        \n",
    "        # 2. Save model files\n",
    "        print(\"2. Saving model files...\")\n",
    "        model.save_pretrained(repo.local_dir)\n",
    "        tokenizer.save_pretrained(repo.local_dir)\n",
    "        \n",
    "        # 3. Add files to git\n",
    "        print(\"3. Adding files to git...\")\n",
    "        repo.git_add()\n",
    "        \n",
    "        # 4. Commit changes\n",
    "        print(\"4. Committing changes...\")\n",
    "        repo.git_commit(\"Update model with improved performance\")\n",
    "        \n",
    "        # 5. Push to Hub\n",
    "        print(\"5. Pushing to Hub...\")\n",
    "        repo.git_push()\n",
    "        \n",
    "        print(\"‚úÖ Git workflow completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in Git workflow: {e}\")\n",
    "\n",
    "print(\"üîß Git integration functions ready!\")\n",
    "print(\"üí° These functions demonstrate advanced repository management using Git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Command Line Tools and Best Practices\n",
    "\n",
    "Learn about command line tools and production-ready practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line equivalents and best practices\n",
    "def show_cli_commands():\n",
    "    \"\"\"\n",
    "    Display equivalent command line commands for common operations.\n",
    "    \"\"\"\n",
    "    cli_commands = {\n",
    "        \"Installation\": [\n",
    "            \"# Install Hugging Face CLI\",\n",
    "            \"pip install huggingface_hub\",\n",
    "            \"\",\n",
    "            \"# For macOS with Homebrew\",\n",
    "            \"brew install huggingface-cli\"\n",
    "        ],\n",
    "        \"Authentication\": [\n",
    "            \"# Login to Hugging Face\",\n",
    "            \"huggingface-cli login\",\n",
    "            \"\",\n",
    "            \"# Login with token\",\n",
    "            \"huggingface-cli login --token YOUR_TOKEN\"\n",
    "        ],\n",
    "        \"Repository Management\": [\n",
    "            \"# Create new repository\",\n",
    "            \"huggingface-cli repo create your-model-name --type model\",\n",
    "            \"\",\n",
    "            \"# Create private repository\",\n",
    "            \"huggingface-cli repo create your-model-name --type model --private\"\n",
    "        ],\n",
    "        \"File Upload\": [\n",
    "            \"# Upload single file\",\n",
    "            \"huggingface-cli upload your-username/your-model ./model.bin model.bin\",\n",
    "            \"\",\n",
    "            \"# Upload entire folder\",\n",
    "            \"huggingface-cli upload your-username/your-model ./model_folder .\"\n",
    "        ],\n",
    "        \"Git LFS Setup\": [\n",
    "            \"# Install Git LFS (required for large files)\",\n",
    "            \"git lfs install\",\n",
    "            \"\",\n",
    "            \"# Clone repository\",\n",
    "            \"git clone https://huggingface.co/your-username/your-model\",\n",
    "            \"\",\n",
    "            \"# Add large files to LFS tracking\",\n",
    "            \"git lfs track '*.bin'\",\n",
    "            \"git lfs track '*.safetensors'\",\n",
    "            \"\",\n",
    "            \"# Standard Git workflow\",\n",
    "            \"git add .\",\n",
    "            \"git commit -m 'Add model files'\",\n",
    "            \"git push\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üñ•Ô∏è  COMMAND LINE REFERENCE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, commands in cli_commands.items():\n",
    "        print(f\"\\nüìã {category}:\")\n",
    "        for cmd in commands:\n",
    "            if cmd.startswith(\"#\"):\n",
    "                print(f\"   {cmd}\")\n",
    "            elif cmd == \"\":\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"   $ {cmd}\")\n",
    "\n",
    "def production_best_practices():\n",
    "    \"\"\"\n",
    "    Display production best practices for model repository management.\n",
    "    \"\"\"\n",
    "    practices = {\n",
    "        \"Security\": [\n",
    "            \"Use environment variables for tokens, never hardcode them\",\n",
    "            \"Use read-only tokens when possible\",\n",
    "            \"Regularly rotate access tokens\",\n",
    "            \"Use private repositories for sensitive models\"\n",
    "        ],\n",
    "        \"Version Control\": [\n",
    "            \"Tag model versions for easy reference\",\n",
    "            \"Use semantic versioning (v1.0.0, v1.1.0, etc.)\",\n",
    "            \"Write descriptive commit messages\",\n",
    "            \"Keep track of model performance across versions\"\n",
    "        ],\n",
    "        \"Documentation\": [\n",
    "            \"Always include comprehensive model cards\",\n",
    "            \"Document training data sources and biases\",\n",
    "            \"Provide clear usage examples\",\n",
    "            \"Include performance metrics and limitations\"\n",
    "        ],\n",
    "        \"File Management\": [\n",
    "            \"Use Git LFS for large model files (>100MB)\",\n",
    "            \"Compress models when possible (quantization)\",\n",
    "            \"Include configuration files and tokenizers\",\n",
    "            \"Use consistent file naming conventions\"\n",
    "        ],\n",
    "        \"Collaboration\": [\n",
    "            \"Use organizations for team repositories\",\n",
    "            \"Set up proper access controls\",\n",
    "            \"Use pull requests for model updates\",\n",
    "            \"Maintain changelog for significant updates\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüèÜ PRODUCTION BEST PRACTICES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\nüìã {category}:\")\n",
    "        for item in items:\n",
    "            print(f\"   ‚úì {item}\")\n",
    "\n",
    "# Display command line reference and best practices\n",
    "show_cli_commands()\n",
    "production_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Demo - Complete Workflow\n",
    "\n",
    "Let's put it all together with a complete workflow demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_workflow_demo(demo_mode: bool = True):\n",
    "    \"\"\"\n",
    "    Demonstrate complete workflow for repository management.\n",
    "    \n",
    "    Args:\n",
    "        demo_mode: If True, only shows what would be done without actually executing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ COMPLETE WORKFLOW DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    demo_repo_id = \"your-username/demo-hate-speech-model\"\n",
    "    \n",
    "    if demo_mode:\n",
    "        print(\"üìù DEMO MODE - Showing workflow steps without execution\")\n",
    "        print(f\"   Repository: {demo_repo_id}\")\n",
    "        print(\"   (Replace 'your-username' with your actual username)\")\n",
    "        print()\n",
    "    \n",
    "    steps = [\n",
    "        \"1. üîê Authenticate with Hugging Face Hub\",\n",
    "        \"2. üìÇ Create new model repository\",\n",
    "        \"3. ü§ñ Load and prepare model for upload\",\n",
    "        \"4. üíæ Save model files locally\",\n",
    "        \"5. üìù Generate comprehensive model card\",\n",
    "        \"6. üì§ Upload model and documentation\",\n",
    "        \"7. üîç Verify upload and test model\",\n",
    "        \"8. üè∑Ô∏è  Tag version and update metadata\"\n",
    "    ]\n",
    "    \n",
    "    for step in steps:\n",
    "        print(step)\n",
    "        \n",
    "        if demo_mode:\n",
    "            # Simulate step execution\n",
    "            if \"Authenticate\" in step:\n",
    "                print(\"   ‚Üí login(token=os.getenv('HF_TOKEN'))\")\n",
    "                print(\"   ‚Üí Verified authentication\")\n",
    "                \n",
    "            elif \"Create new\" in step:\n",
    "                print(f\"   ‚Üí create_repo(repo_id='{demo_repo_id}')\")\n",
    "                print(\"   ‚Üí Repository created successfully\")\n",
    "                \n",
    "            elif \"Load and prepare\" in step:\n",
    "                print(\"   ‚Üí Loading cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "                print(\"   ‚Üí Model and tokenizer loaded\")\n",
    "                \n",
    "            elif \"Save model\" in step:\n",
    "                print(\"   ‚Üí model.save_pretrained('./temp_model')\")\n",
    "                print(\"   ‚Üí tokenizer.save_pretrained('./temp_model')\")\n",
    "                \n",
    "            elif \"Generate comprehensive\" in step:\n",
    "                print(\"   ‚Üí Creating model card with performance metrics\")\n",
    "                print(\"   ‚Üí Including usage examples and bias documentation\")\n",
    "                \n",
    "            elif \"Upload model\" in step:\n",
    "                print(\"   ‚Üí upload_folder('./temp_model', repo_id)\")\n",
    "                print(\"   ‚Üí upload_file('README.md', repo_id)\")\n",
    "                \n",
    "            elif \"Verify upload\" in step:\n",
    "                print(\"   ‚Üí Testing model loading from Hub\")\n",
    "                print(\"   ‚Üí Running inference tests\")\n",
    "                \n",
    "            elif \"Tag version\" in step:\n",
    "                print(\"   ‚Üí Adding version tag: v1.0.0\")\n",
    "                print(\"   ‚Üí Updating repository metadata\")\n",
    "            \n",
    "            print(\"   ‚úÖ Step completed\\n\")\n",
    "        else:\n",
    "            print(\"   [This would execute the actual workflow step]\\n\")\n",
    "    \n",
    "    print(\"üéâ Workflow completed successfully!\")\n",
    "    print(f\"üîó Your model would be available at: https://huggingface.co/{demo_repo_id}\")\n",
    "    \n",
    "    # Show what the final repository would contain\n",
    "    print(\"\\nüìÅ Repository Contents:\")\n",
    "    repository_files = [\n",
    "        \"README.md (comprehensive model card)\",\n",
    "        \"config.json (model configuration)\",\n",
    "        \"pytorch_model.bin (model weights)\",\n",
    "        \"tokenizer.json (tokenizer files)\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\"\n",
    "    ]\n",
    "    \n",
    "    for file in repository_files:\n",
    "        print(f\"   üìÑ {file}\")\n",
    "\n",
    "# Run the complete workflow demonstration\n",
    "complete_workflow_demo(demo_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to check repository status\n",
    "def check_repository_status(repo_id: str):\n",
    "    \"\"\"\n",
    "    Check the current status of a repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository identifier to check\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Checking repository status: {repo_id}\")\n",
    "        \n",
    "        # Get repository information\n",
    "        info = model_info(repo_id)\n",
    "        \n",
    "        print(\"‚úÖ Repository Status:\")\n",
    "        print(f\"   üìä Downloads: {info.downloads:,}\")\n",
    "        print(f\"   üëç Likes: {info.likes}\")\n",
    "        print(f\"   üìÖ Last Updated: {info.last_modified}\")\n",
    "        print(f\"   üè∑Ô∏è  Pipeline Tag: {info.pipeline_tag or 'Not specified'}\")\n",
    "        \n",
    "        # Check if model is loadable\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
    "            print(\"   ‚úÖ Model is loadable\")\n",
    "            print(f\"   üî¢ Parameters: {model.num_parameters():,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Model loading issue: {e}\")\n",
    "        \n",
    "        return info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking repository: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check the reference model status\n",
    "print(\"üîç Checking reference model status:\")\n",
    "reference_status = check_repository_status(REFERENCE_MODEL)\n",
    "\n",
    "print(\"\\nüîç Checking preferred model status:\")\n",
    "preferred_status = check_repository_status(PREFERRED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Troubleshooting Common Issues\n",
    "\n",
    "Learn how to resolve common problems when managing Hugging Face repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshooting_guide():\n",
    "    \"\"\"\n",
    "    Display common issues and their solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    issues = {\n",
    "        \"Authentication Issues\": {\n",
    "            \"Problem\": \"Token not found or invalid\",\n",
    "            \"Solutions\": [\n",
    "                \"Check if HF_TOKEN is set in environment variables\",\n",
    "                \"Verify token has write permissions\",\n",
    "                \"Generate new token at: https://huggingface.co/settings/tokens\",\n",
    "                \"Use 'Write' access level for repository management\"\n",
    "            ]\n",
    "        },\n",
    "        \"Large File Upload Issues\": {\n",
    "            \"Problem\": \"Files larger than 5GB failing to upload\",\n",
    "            \"Solutions\": [\n",
    "                \"Install and configure Git LFS: git lfs install\",\n",
    "                \"Track large files: git lfs track '*.bin'\",\n",
    "                \"Use huggingface-cli upload for large files\",\n",
    "                \"Consider model quantization to reduce size\"\n",
    "            ]\n",
    "        },\n",
    "        \"Repository Access Issues\": {\n",
    "            \"Problem\": \"Cannot access private repository\",\n",
    "            \"Solutions\": [\n",
    "                \"Ensure you have access to the repository\",\n",
    "                \"Check if repository exists and name is correct\",\n",
    "                \"Verify authentication token has appropriate permissions\",\n",
    "                \"Contact repository owner for access\"\n",
    "            ]\n",
    "        },\n",
    "        \"Model Loading Issues\": {\n",
    "            \"Problem\": \"Model fails to load from Hub\",\n",
    "            \"Solutions\": [\n",
    "                \"Check if all required files are uploaded (config.json, tokenizer files)\",\n",
    "                \"Verify model architecture matches expected format\",\n",
    "                \"Test loading with specific revision/branch\",\n",
    "                \"Check for corrupted files and re-upload if necessary\"\n",
    "            ]\n",
    "        },\n",
    "        \"Git LFS Issues\": {\n",
    "            \"Problem\": \"Git LFS not working properly\",\n",
    "            \"Solutions\": [\n",
    "                \"Install Git LFS: brew install git-lfs (macOS) or apt install git-lfs (Ubuntu)\",\n",
    "                \"Initialize in repository: git lfs install\",\n",
    "                \"Track large files: git lfs track '*.bin' '*.safetensors'\",\n",
    "                \"Commit .gitattributes file: git add .gitattributes && git commit\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üîß TROUBLESHOOTING GUIDE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for issue, details in issues.items():\n",
    "        print(f\"\\n‚ùå {issue}\")\n",
    "        print(f\"Problem: {details['Problem']}\")\n",
    "        print(\"Solutions:\")\n",
    "        for i, solution in enumerate(details['Solutions'], 1):\n",
    "            print(f\"   {i}. {solution}\")\n",
    "\n",
    "def diagnostic_commands():\n",
    "    \"\"\"\n",
    "    Show diagnostic commands for troubleshooting.\n",
    "    \"\"\"\n",
    "    \n",
    "    commands = {\n",
    "        \"Check Authentication\": [\n",
    "            \"huggingface-cli whoami\",\n",
    "            \"python -c \\\"from huggingface_hub import whoami; print(whoami())\\\"\"\n",
    "        ],\n",
    "        \"Check Repository\": [\n",
    "            \"huggingface-cli repo ls your-username/your-model\",\n",
    "            \"git lfs ls-files  # Check LFS tracked files\"\n",
    "        ],\n",
    "        \"Check Git LFS\": [\n",
    "            \"git lfs version\",\n",
    "            \"git lfs env\",\n",
    "            \"git lfs ls-files\"\n",
    "        ],\n",
    "        \"Test Model Loading\": [\n",
    "            \"python -c \\\"from transformers import AutoModel; AutoModel.from_pretrained('your-model')\\\"\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüî¨ DIAGNOSTIC COMMANDS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for category, cmds in commands.items():\n",
    "        print(f\"\\nüìã {category}:\")\n",
    "        for cmd in cmds:\n",
    "            print(f\"   $ {cmd}\")\n",
    "\n",
    "# Display troubleshooting information\n",
    "troubleshooting_guide()\n",
    "diagnostic_commands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### üîë Key Concepts Mastered\n",
    "- **Authentication**: Secure credential management using tokens and environment variables\n",
    "- **Repository Management**: Creating, uploading, and managing model repositories programmatically\n",
    "- **File Operations**: Multiple methods for uploading models, tokenizers, and documentation\n",
    "- **Model Cards**: Creating comprehensive documentation following HF standards\n",
    "- **Git Integration**: Advanced repository management using Git workflow\n",
    "- **Best Practices**: Production-ready patterns for model sharing and collaboration\n",
    "\n",
    "### üìà Best Practices Learned\n",
    "- Use environment variables for secure token management\n",
    "- Create comprehensive model cards with bias documentation\n",
    "- Implement proper version control with semantic versioning\n",
    "- Use Git LFS for large model files (>100MB)\n",
    "- Test model loading after upload to verify integrity\n",
    "- Follow Hugging Face community guidelines for model sharing\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **Practice**: Create your own model repository using the patterns learned\n",
    "- **Advanced Topics**: Explore model versioning and A/B testing strategies\n",
    "- **Community**: Engage with Hugging Face community and contribute models\n",
    "- **Documentation**: Study other high-quality model cards for inspiration\n",
    "- **Automation**: Set up CI/CD pipelines for automated model deployment\n",
    "\n",
    "### üîó Useful Resources\n",
    "- **Hugging Face Hub Documentation**: [huggingface.co/docs/hub](https://huggingface.co/docs/hub)\n",
    "- **Model Card Guidelines**: [huggingface.co/docs/hub/model-cards](https://huggingface.co/docs/hub/model-cards)\n",
    "- **Git LFS Documentation**: [git-lfs.github.io](https://git-lfs.github.io/)\n",
    "- **Hugging Face CLI Reference**: [huggingface.co/docs/huggingface_hub/guides/cli](https://huggingface.co/docs/huggingface_hub/guides/cli)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Author\n",
    "\n",
    "**Vu Hung Nguyen** - AI Engineer & Researcher\n",
    "\n",
    "Connect with me:\n",
    "- üåê **Website**: [vuhung16au.github.io](https://vuhung16au.github.io/)\n",
    "- üíº **LinkedIn**: [linkedin.com/in/nguyenvuhung](https://www.linkedin.com/in/nguyenvuhung/)\n",
    "- üíª **GitHub**: [github.com/vuhung16au](https://github.com/vuhung16au/)\n",
    "\n",
    "*This notebook is part of the [HF Transformer Trove](https://github.com/vuhung16au/hf-transformer-trove) educational series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
