# FAISS: Efficient Similarity Search for NLP and Hate Speech Detection

## Table of Contents
1. [What is FAISS?](#what-is-faiss)
2. [Core Concepts](#core-concepts)
3. [How to Use FAISS in HuggingFace](#how-to-use-faiss-in-huggingface)
4. [Why FAISS Matters for NLP](#why-faiss-matters-for-nlp)
5. [FAISS in Hate Speech Detection](#faiss-in-hate-speech-detection)
6. [Installation and Setup](#installation-and-setup)
7. [Basic Usage Examples](#basic-usage-examples)
8. [Advanced Features](#advanced-features)
9. [Performance Optimization](#performance-optimization)
10. [Best Practices](#best-practices)

---

## What is FAISS?

**[FAISS](https://faiss.ai/) (Facebook AI Similarity Search)** is a library developed by Meta AI Research (formerly Facebook AI Research) for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.

### Key Features

- **Scalability**: Handles billions of vectors efficiently
- **Speed**: Optimized for both CPU and GPU implementations
- **Flexibility**: Supports various index types for different use cases
- **Memory Efficiency**: Implements compression techniques to reduce memory footprint
- **Open Source**: Available under MIT license with active development

### Why FAISS Was Created

Traditional similarity search becomes prohibitively slow when dealing with:
- Large-scale vector databases (millions to billions of vectors)
- High-dimensional embeddings (768-1024 dimensions from transformer models)
- Real-time applications requiring sub-millisecond response times

FAISS solves these problems through:
- **Approximate Nearest Neighbor (ANN)** search algorithms
- **GPU acceleration** for massive parallelization
- **Quantization** techniques to compress vectors
- **Index structures** optimized for different data distributions

---

## Core Concepts

### 1. Vector Embeddings

In NLP, text is converted to dense vector representations (embeddings) that capture semantic meaning:

```python
from transformers import AutoTokenizer, AutoModel
import torch

# Generate embeddings from text
def get_embeddings(texts, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    """
    Convert texts to vector embeddings.
    
    Args:
        texts: List of text strings
        model_name: HuggingFace model for embeddings
        
    Returns:
        numpy array of embeddings (shape: [num_texts, embedding_dim])
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # Tokenize and get embeddings
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs)
        # Use mean pooling for sentence embeddings
        embeddings = outputs.last_hidden_state.mean(dim=1)
    
    return embeddings.numpy()

# Example usage
texts = [
    "This is a friendly message.",
    "Machine learning is fascinating.",
    "Natural language processing with transformers."
]

embeddings = get_embeddings(texts)
print(f"üìä Generated embeddings shape: {embeddings.shape}")
# Output: (3, 384) - 3 texts, 384-dimensional embeddings
```

### 2. Similarity Search

FAISS enables finding similar vectors efficiently:

$$\text{similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{\|v_1\| \|v_2\|}$$

Where:
- $v_1, v_2$ are vector embeddings
- $\cdot$ denotes dot product
- $\|\cdot\|$ denotes L2 norm (Euclidean norm)

### 3. Index Types

FAISS provides multiple index types optimized for different scenarios:

| Index Type | Description | Use Case | Speed | Memory |
|:-----------|:------------|:---------|:------|:-------|
| **IndexFlatL2** | Brute-force L2 distance | Small datasets, exact search | Slow | Low |
| **IndexFlatIP** | Brute-force inner product | Small datasets, cosine similarity | Slow | Low |
| **IndexIVFFlat** | Inverted file index | Medium datasets, approximate search | Fast | Medium |
| **IndexIVFPQ** | IVF with product quantization | Large datasets, compressed | Very Fast | Very Low |
| **IndexHNSW** | Hierarchical navigable small world | High-quality approximate search | Very Fast | High |

> üí° **Pro Tip**: Start with `IndexFlatL2` for development and switch to `IndexIVFFlat` or `IndexHNSW` for production with large datasets.

---

## How to Use FAISS in HuggingFace

### Integration with HuggingFace Datasets

HuggingFace Datasets library has built-in FAISS support for efficient similarity search:

```python
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel

# Set reproducible environment with repository standard seed=16
torch.manual_seed(16)
np.random.seed(16)

# Load preferred hate speech dataset
dataset = load_dataset("tdavidson/hate_speech_offensive", split="train")

# Sample for demonstration with seed=16
dataset_sample = dataset.shuffle(seed=16).select(range(1000))

print(f"üìä Dataset size: {len(dataset_sample)}")
```

### Adding FAISS Index to HuggingFace Dataset

```python
from transformers import AutoTokenizer, AutoModel
import torch

def add_embeddings_to_dataset(dataset, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    """
    Add embeddings column to dataset for FAISS indexing.
    
    Args:
        dataset: HuggingFace dataset
        model_name: Model for generating embeddings
        
    Returns:
        Dataset with embeddings column
    """
    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # Move to optimal device
    device = torch.device("cuda" if torch.cuda.is_available() 
                         else "mps" if torch.backends.mps.is_available() 
                         else "cpu")
    model = model.to(device)
    
    def embed_batch(examples):
        """Generate embeddings for a batch of texts."""
        texts = examples["text"]  # Adjust column name as needed
        
        # Tokenize
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=512,
            return_tensors="pt"
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate embeddings
        with torch.no_grad():
            outputs = model(**inputs)
            # Mean pooling
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return {"embeddings": embeddings.cpu().numpy()}
    
    # Add embeddings to dataset
    dataset_with_embeddings = dataset.map(
        embed_batch,
        batched=True,
        batch_size=32
    )
    
    return dataset_with_embeddings

# Add embeddings to dataset
print("üîÑ Generating embeddings...")
dataset_with_embeddings = add_embeddings_to_dataset(dataset_sample)

# Add FAISS index
print("üîÑ Building FAISS index...")
dataset_with_embeddings.add_faiss_index(column="embeddings")

print("‚úÖ FAISS index created successfully!")
```

### Performing Similarity Search

```python
def find_similar_texts(query_text, dataset, k=5):
    """
    Find k most similar texts to the query using FAISS.
    
    Args:
        query_text: Text to search for
        dataset: Dataset with FAISS index
        k: Number of similar items to return
        
    Returns:
        List of similar texts with scores
    """
    # Generate embedding for query
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    
    inputs = tokenizer(query_text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        query_embedding = outputs.last_hidden_state.mean(dim=1).numpy()
    
    # Search using FAISS
    scores, retrieved_examples = dataset.get_nearest_examples(
        "embeddings",
        query_embedding[0],
        k=k
    )
    
    return scores, retrieved_examples

# Example: Find similar texts
query = "This message contains offensive language"
scores, similar_texts = find_similar_texts(query, dataset_with_embeddings, k=5)

print(f"üîç Query: {query}\n")
print("üìã Top 5 Similar Texts:")
for i, (score, text) in enumerate(zip(scores, similar_texts["text"]), 1):
    print(f"{i}. Score: {score:.4f}")
    print(f"   Text: {text[:100]}...\n")
```

### Saving and Loading FAISS Indices

```python
# Save FAISS index to disk
dataset_with_embeddings.save_faiss_index("embeddings", "hate_speech_faiss.index")
print("üíæ FAISS index saved to disk")

# Load FAISS index from disk
dataset_with_embeddings.load_faiss_index("embeddings", "hate_speech_faiss.index")
print("üì• FAISS index loaded from disk")
```

---

## Why FAISS Matters for NLP

### 1. **Semantic Search at Scale**

Traditional keyword search fails to capture semantic meaning. FAISS enables:

```python
# Traditional keyword search (misses semantic matches)
query = "hostile comment"
# Would miss: "aggressive remark", "mean statement", "nasty message"

# FAISS semantic search (finds semantically similar content)
# Automatically finds all variations and related phrases
```

### 2. **Real-Time Applications**

FAISS enables sub-millisecond search across millions of documents:

- **Chatbots**: Finding relevant responses from knowledge bases
- **Content Moderation**: Detecting similar toxic/hate speech patterns
- **Recommendation Systems**: Suggesting similar articles/posts
- **Question Answering**: Retrieving relevant context passages

### 3. **Memory Efficiency**

For large-scale NLP applications with billions of embeddings:

```python
import faiss
import numpy as np

# Example: 1 billion 768-dimensional vectors
# Raw storage: 1B * 768 * 4 bytes = ~3TB
# With FAISS PQ compression: ~30GB (100x reduction!)

# Create compressed index
dimension = 768
m = 64  # Number of subquantizers
bits = 8  # Bits per subquantizer

quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFPQ(quantizer, dimension, 100, m, bits)

print("üóúÔ∏è FAISS enables 100x compression while maintaining search quality")
```

### 4. **Duplicate Detection**

Identify near-duplicate content efficiently:

```python
def find_duplicates(dataset, threshold=0.95):
    """
    Find near-duplicate texts using FAISS similarity search.
    
    Args:
        dataset: Dataset with FAISS index
        threshold: Similarity threshold for duplicates
        
    Returns:
        List of duplicate pairs
    """
    duplicates = []
    
    for idx, example in enumerate(dataset):
        # Find similar items
        scores, similar = dataset.get_nearest_examples(
            "embeddings",
            example["embeddings"],
            k=5
        )
        
        # Check for high similarity (excluding self)
        for score, similar_idx in zip(scores[1:], similar["id"][1:]):
            if score > threshold:
                duplicates.append((idx, similar_idx, score))
    
    return duplicates
```

### 5. **Transfer Learning Enhancement**

FAISS complements transformer models by:
- **Fast retrieval** of relevant training examples
- **Few-shot learning** by finding similar labeled examples
- **Active learning** by identifying uncertain or novel examples

---

## FAISS in Hate Speech Detection

FAISS is particularly valuable for hate speech detection systems:

### 1. **Pattern Recognition Across Variants**

Hate speech often uses creative spelling and variations to evade detection. FAISS helps find similar patterns:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import numpy as np

# Set reproducible environment with repository standard seed=16
torch.manual_seed(16)
np.random.seed(16)

# Load preferred hate speech detection model
model_name = "cardiffnlp/twitter-roberta-base-hate-latest"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create hate speech detection pipeline
device = torch.device("cuda" if torch.cuda.is_available() 
                     else "mps" if torch.backends.mps.is_available() 
                     else "cpu")
model = model.to(device)

hate_classifier = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# Example: Detect hate speech variants
test_texts = [
    "You are a terrible person",  # Original
    "U r a terrible person",      # Variant 1
    "Ur a terribel person",       # Variant 2 (misspelled)
    "You're a horrible individual", # Semantic variant
]

print("üõ°Ô∏è Hate Speech Detection with Variants:")
for text in test_texts:
    result = hate_classifier(text)
    print(f"Text: {text}")
    print(f"  Label: {result[0]['label']}, Score: {result[0]['score']:.4f}\n")
```

### 2. **Building Hate Speech Knowledge Bases**

Use FAISS to create searchable databases of known hate speech patterns:

```python
from datasets import load_dataset
import torch
import numpy as np

# Set seed for reproducibility
torch.manual_seed(16)
np.random.seed(16)

class HateSpeechKnowledgeBase:
    """
    FAISS-powered knowledge base for hate speech detection.
    """
    
    def __init__(self, model_name="cardiffnlp/twitter-roberta-base-hate-latest"):
        """Initialize with preferred hate speech model."""
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() 
                                   else "mps" if torch.backends.mps.is_available() 
                                   else "cpu")
        self.model = self.model.to(self.device)
        self.dataset = None
    
    def build_from_dataset(self, dataset_name="tdavidson/hate_speech_offensive"):
        """
        Build knowledge base from hate speech dataset.
        
        Args:
            dataset_name: HuggingFace dataset name
        """
        print(f"üì• Loading dataset: {dataset_name}")
        dataset = load_dataset(dataset_name, split="train")
        
        # Sample with repository standard seed
        self.dataset = dataset.shuffle(seed=16).select(range(5000))
        
        # Add embeddings
        print("üîÑ Generating embeddings...")
        self.dataset = self._add_embeddings(self.dataset)
        
        # Build FAISS index
        print("üîÑ Building FAISS index...")
        self.dataset.add_faiss_index(column="embeddings")
        
        print("‚úÖ Knowledge base ready!")
    
    def _add_embeddings(self, dataset):
        """Add embeddings to dataset."""
        def embed_batch(examples):
            inputs = self.tokenizer(
                examples["text"],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
            
            return {"embeddings": embeddings.cpu().numpy()}
        
        return dataset.map(embed_batch, batched=True, batch_size=32)
    
    def find_similar_hate_speech(self, query_text, k=5):
        """
        Find similar hate speech examples in knowledge base.
        
        Args:
            query_text: Text to analyze
            k: Number of similar examples to return
            
        Returns:
            Similar examples with scores
        """
        # Generate query embedding
        inputs = self.tokenizer(
            query_text,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            query_embedding = outputs.last_hidden_state.mean(dim=1)
        
        # Search FAISS index
        scores, examples = self.dataset.get_nearest_examples(
            "embeddings",
            query_embedding[0].cpu().numpy(),
            k=k
        )
        
        return scores, examples

# Example usage
kb = HateSpeechKnowledgeBase()
kb.build_from_dataset()

# Test with a query
query = "This is offensive language"
scores, similar = kb.find_similar_hate_speech(query, k=3)

print(f"\nüîç Query: {query}")
print("\nüìã Similar Hate Speech Examples:")
for i, (score, text, label) in enumerate(zip(scores, similar["text"], similar["class"]), 1):
    print(f"{i}. Similarity: {score:.4f}, Label: {label}")
    print(f"   Text: {text[:100]}")
```

### 3. **Real-Time Content Moderation**

FAISS enables fast lookup of similar flagged content:

```python
class RealTimeContentModerator:
    """
    Production-ready content moderation with FAISS and hate speech detection.
    """
    
    def __init__(self, knowledge_base, model_name="cardiffnlp/twitter-roberta-base-hate-latest"):
        """
        Initialize moderator with knowledge base and classifier.
        
        Args:
            knowledge_base: HateSpeechKnowledgeBase instance
            model_name: Hate speech detection model
        """
        self.kb = knowledge_base
        
        # Load hate speech classifier
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        self.classifier = pipeline(
            "text-classification",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
    
    def moderate_content(self, text, similarity_threshold=0.85):
        """
        Moderate content using both classification and similarity search.
        
        Args:
            text: Text to moderate
            similarity_threshold: Threshold for similarity matching
            
        Returns:
            Moderation decision with reasoning
        """
        # Step 1: Direct classification
        classification = self.classifier(text)[0]
        
        # Step 2: FAISS similarity search
        scores, similar_examples = self.kb.find_similar_hate_speech(text, k=3)
        
        # Step 3: Make decision
        decision = {
            "text": text,
            "classification": {
                "label": classification["label"],
                "score": classification["score"]
            },
            "similar_flagged_content": []
        }
        
        # Check for similar flagged content
        for score, similar_text, similar_label in zip(
            scores, 
            similar_examples["text"], 
            similar_examples["class"]
        ):
            if score > similarity_threshold:
                decision["similar_flagged_content"].append({
                    "text": similar_text[:100],
                    "similarity": float(score),
                    "label": int(similar_label)
                })
        
        # Make final decision
        if classification["score"] > 0.8 and classification["label"] in ["hate", "offensive"]:
            decision["action"] = "BLOCK"
            decision["reason"] = "High-confidence hate speech detection"
        elif len(decision["similar_flagged_content"]) >= 2:
            decision["action"] = "REVIEW"
            decision["reason"] = "Similar to multiple flagged content"
        else:
            decision["action"] = "ALLOW"
            decision["reason"] = "No concerning patterns detected"
        
        return decision

# Example: Real-time moderation
moderator = RealTimeContentModerator(kb)

test_messages = [
    "What a beautiful day!",
    "This is slightly concerning language",
    "Extremely offensive hate speech example",
]

print("üõ°Ô∏è Real-Time Content Moderation Results:\n")
for msg in test_messages:
    result = moderator.moderate_content(msg)
    print(f"Text: {msg}")
    print(f"  Classification: {result['classification']['label']} ({result['classification']['score']:.3f})")
    print(f"  Action: {result['action']}")
    print(f"  Reason: {result['reason']}")
    print(f"  Similar flagged: {len(result['similar_flagged_content'])} items\n")
```

### 4. **Explainability and Transparency**

FAISS helps explain why content was flagged:

```python
def explain_hate_speech_detection(text, knowledge_base, classifier, k=5):
    """
    Explain hate speech detection decision using FAISS similarity.
    
    Args:
        text: Text being analyzed
        knowledge_base: HateSpeechKnowledgeBase
        classifier: Hate speech classifier pipeline
        k: Number of similar examples to show
        
    Returns:
        Explanation with examples
    """
    # Get classification
    classification = classifier(text)[0]
    
    # Get similar examples
    scores, similar = knowledge_base.find_similar_hate_speech(text, k=k)
    
    print(f"üìù Analyzing: {text}\n")
    print(f"ü§ñ Model Decision: {classification['label']} (confidence: {classification['score']:.3f})\n")
    print(f"üìä Similar Examples from Training Data:\n")
    
    for i, (score, example_text, label) in enumerate(zip(scores, similar["text"], similar["class"]), 1):
        print(f"{i}. Similarity: {score:.3f}")
        print(f"   Label: {['hate speech', 'offensive', 'neither'][label]}")
        print(f"   Text: {example_text[:100]}")
        print()
    
    print("üí° Explanation: The model's decision is based on:")
    print("   1. Learned patterns from training data")
    print("   2. Similarity to known examples (shown above)")
    print("   3. Contextual understanding from transformer model")

# Example explanation
explain_hate_speech_detection(
    "This is problematic language",
    kb,
    hate_classifier,
    k=3
)
```

### 5. **Continuous Learning**

Update hate speech detection as new patterns emerge:

```python
def update_knowledge_base(knowledge_base, new_examples):
    """
    Update FAISS index with new hate speech examples.
    
    Args:
        knowledge_base: Existing HateSpeechKnowledgeBase
        new_examples: List of new examples with labels
    """
    from datasets import Dataset
    
    # Create new dataset from examples
    new_dataset = Dataset.from_dict(new_examples)
    
    # Add embeddings
    new_dataset = knowledge_base._add_embeddings(new_dataset)
    
    # Combine with existing dataset
    combined = concatenate_datasets([knowledge_base.dataset, new_dataset])
    
    # Rebuild FAISS index
    combined.add_faiss_index(column="embeddings")
    
    knowledge_base.dataset = combined
    print(f"‚úÖ Knowledge base updated with {len(new_examples)} new examples")

# Example: Add newly discovered hate speech patterns
new_patterns = {
    "text": [
        "New variant of hate speech detected",
        "Another concerning pattern found",
    ],
    "class": [0, 1],  # Labels: 0=hate, 1=offensive, 2=neither
}

update_knowledge_base(kb, new_patterns)
```

---

## Installation and Setup

### Installing FAISS

```bash
# CPU version (recommended for development)
uv add faiss-cpu

# Or with pip as fallback
pip install faiss-cpu

# GPU version (for production with CUDA)
uv add faiss-gpu

# Or with pip
pip install faiss-gpu
```

### Verifying Installation

```python
import faiss
import numpy as np

# Check FAISS version
print(f"FAISS version: {faiss.__version__}")

# Check GPU availability
print(f"GPU available: {faiss.get_num_gpus() > 0}")

# Test basic functionality
dimension = 128
vectors = np.random.random((100, dimension)).astype('float32')

# Create simple index
index = faiss.IndexFlatL2(dimension)
index.add(vectors)

print(f"‚úÖ FAISS installed successfully! Index contains {index.ntotal} vectors")
```

### HuggingFace Integration Setup

```python
# Install required packages
# uv add transformers datasets faiss-cpu

from datasets import load_dataset
import faiss

# Verify HuggingFace integration
print(f"‚úÖ Datasets library supports FAISS: {hasattr(load_dataset, '__call__')}")
```

---

## Basic Usage Examples

### Example 1: Simple Similarity Search

```python
import faiss
import numpy as np

# Set reproducible environment with repository standard seed=16
np.random.seed(16)

# Create sample data (e.g., sentence embeddings)
dimension = 384  # Common dimension for sentence transformers
num_vectors = 10000

# Generate random embeddings (in practice, these come from a model)
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# Create FAISS index
index = faiss.IndexFlatL2(dimension)  # L2 distance metric
index.add(embeddings)  # Add vectors to index

print(f"üìä Index contains {index.ntotal} vectors")

# Search for similar vectors
query_vector = np.random.random((1, dimension)).astype('float32')
k = 5  # Find 5 nearest neighbors

distances, indices = index.search(query_vector, k)

print(f"\nüîç Top {k} Similar Vectors:")
for i, (dist, idx) in enumerate(zip(distances[0], indices[0]), 1):
    print(f"{i}. Index: {idx}, Distance: {dist:.4f}")
```

### Example 2: Semantic Text Search

```python
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import numpy as np

# Set seed for reproducibility
torch.manual_seed(16)
np.random.seed(16)

class SemanticSearch:
    """
    Semantic search engine using FAISS and HuggingFace transformers.
    """
    
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        """Initialize with sentence transformer model."""
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
        self.index = None
        self.documents = []
    
    def encode_text(self, text):
        """Convert text to embedding vector."""
        inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Mean pooling
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return embeddings.cpu().numpy()
    
    def build_index(self, documents):
        """
        Build FAISS index from documents.
        
        Args:
            documents: List of text documents
        """
        self.documents = documents
        
        print(f"üîÑ Encoding {len(documents)} documents...")
        embeddings = []
        
        for i, doc in enumerate(documents):
            if i % 100 == 0:
                print(f"  Progress: {i}/{len(documents)}")
            embedding = self.encode_text(doc)
            embeddings.append(embedding)
        
        embeddings = np.vstack(embeddings).astype('float32')
        
        # Create FAISS index
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)
        
        print(f"‚úÖ Index built with {self.index.ntotal} documents")
    
    def search(self, query, k=5):
        """
        Search for similar documents.
        
        Args:
            query: Search query text
            k: Number of results to return
            
        Returns:
            List of (document, score) tuples
        """
        # Encode query
        query_embedding = self.encode_text(query).astype('float32')
        
        # Search
        distances, indices = self.index.search(query_embedding, k)
        
        # Return results
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            results.append((self.documents[idx], float(dist)))
        
        return results

# Example usage
documents = [
    "Machine learning is a subset of artificial intelligence.",
    "Deep learning uses neural networks with multiple layers.",
    "Natural language processing helps computers understand human language.",
    "Computer vision enables machines to interpret visual information.",
    "Reinforcement learning trains agents through rewards and penalties.",
    "Transfer learning reuses pre-trained models for new tasks.",
    "Hate speech detection identifies toxic content in social media.",
    "Sentiment analysis determines emotional tone of text.",
]

# Build search engine
search_engine = SemanticSearch()
search_engine.build_index(documents)

# Perform search
query = "How do machines understand text?"
results = search_engine.search(query, k=3)

print(f"\nüîç Query: {query}\n")
print("üìã Top 3 Results:")
for i, (doc, score) in enumerate(results, 1):
    print(f"{i}. Score: {score:.4f}")
    print(f"   Document: {doc}\n")
```

### Example 3: Clustering with FAISS

```python
import faiss
import numpy as np

# Set seed for reproducibility
np.random.seed(16)

# Generate sample embeddings
dimension = 384
num_vectors = 1000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# Number of clusters
num_clusters = 10

# Configure k-means clustering
kmeans = faiss.Kmeans(
    d=dimension,
    k=num_clusters,
    niter=20,  # Number of iterations
    verbose=True,
    seed=16  # Repository standard seed
)

# Train clustering
print("üîÑ Training k-means clustering...")
kmeans.train(embeddings)

# Assign vectors to clusters
distances, cluster_assignments = kmeans.index.search(embeddings, 1)

print(f"\n‚úÖ Clustering completed!")
print(f"üìä Cluster sizes:")
unique, counts = np.unique(cluster_assignments, return_counts=True)
for cluster_id, count in zip(unique, counts):
    print(f"  Cluster {cluster_id}: {count} vectors")
```

---

## Advanced Features

### 1. GPU Acceleration

```python
import faiss
import numpy as np

# Set seed for reproducibility
np.random.seed(16)

# Create large dataset
dimension = 768
num_vectors = 1000000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# Create CPU index
cpu_index = faiss.IndexFlatL2(dimension)

# Move to GPU if available
if faiss.get_num_gpus() > 0:
    print(f"üöÄ Using GPU acceleration ({faiss.get_num_gpus()} GPUs available)")
    
    # Create GPU resources
    res = faiss.StandardGpuResources()
    
    # Move index to GPU
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
    
    # Add vectors (much faster on GPU)
    gpu_index.add(embeddings)
    
    print(f"‚úÖ Added {gpu_index.ntotal} vectors to GPU index")
    
    # Search on GPU
    query = np.random.random((1, dimension)).astype('float32')
    distances, indices = gpu_index.search(query, 10)
    
else:
    print("üíª GPU not available, using CPU")
    cpu_index.add(embeddings)
```

### 2. Product Quantization (Memory Efficient)

```python
import faiss
import numpy as np

# Set seed for reproducibility
np.random.seed(16)

dimension = 768
num_vectors = 100000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# Create product quantization index
m = 64  # Number of subquantizers (must divide dimension)
bits = 8  # Bits per subquantizer

# Flat index for exact search (baseline)
index_flat = faiss.IndexFlatL2(dimension)
index_flat.add(embeddings)

# PQ index for compressed search
index_pq = faiss.IndexPQ(dimension, m, bits)
index_pq.train(embeddings)  # PQ requires training
index_pq.add(embeddings)

# Compare memory usage
import sys
print(f"üìä Memory Comparison:")
print(f"  Flat index: ~{num_vectors * dimension * 4 / 1024 / 1024:.1f} MB")
print(f"  PQ index: ~{num_vectors * m * bits / 8 / 1024 / 1024:.1f} MB")
print(f"  Compression ratio: {(num_vectors * dimension * 4) / (num_vectors * m * bits / 8):.1f}x")

# Search comparison
query = np.random.random((1, dimension)).astype('float32')

# Exact search
distances_flat, indices_flat = index_flat.search(query, 10)

# Approximate search (compressed)
distances_pq, indices_pq = index_pq.search(query, 10)

print(f"\nüîç Search Results Comparison:")
print(f"  Flat index top result distance: {distances_flat[0][0]:.4f}")
print(f"  PQ index top result distance: {distances_pq[0][0]:.4f}")
```

### 3. Inverted File Index (IVF)

```python
import faiss
import numpy as np
import time

# Set seed for reproducibility
np.random.seed(16)

dimension = 384
num_vectors = 100000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# IVF parameters
num_clusters = 100  # Number of Voronoi cells
num_probes = 10  # Number of cells to visit during search

# Create IVF index
quantizer = faiss.IndexFlatL2(dimension)
index_ivf = faiss.IndexIVFFlat(quantizer, dimension, num_clusters)

# Train the index (IVF requires training)
print("üîÑ Training IVF index...")
index_ivf.train(embeddings)

# Add vectors
print("üîÑ Adding vectors to index...")
index_ivf.add(embeddings)

# Configure search
index_ivf.nprobe = num_probes  # Trade-off between speed and accuracy

print(f"‚úÖ IVF index created with {index_ivf.ntotal} vectors")

# Benchmark search speed
query = np.random.random((100, dimension)).astype('float32')

start_time = time.time()
distances, indices = index_ivf.search(query, 10)
search_time = time.time() - start_time

print(f"\n‚è±Ô∏è Search Performance:")
print(f"  100 queries in {search_time:.4f} seconds")
print(f"  {search_time / 100 * 1000:.2f} ms per query")
```

### 4. HNSW Index (Hierarchical Navigable Small World)

```python
import faiss
import numpy as np

# Set seed for reproducibility
np.random.seed(16)

dimension = 384
num_vectors = 50000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

# HNSW parameters
M = 32  # Number of connections per layer
efConstruction = 40  # Size of dynamic candidate list during construction
efSearch = 16  # Size of dynamic candidate list during search

# Create HNSW index
index_hnsw = faiss.IndexHNSWFlat(dimension, M)
index_hnsw.hnsw.efConstruction = efConstruction

# Add vectors (no training required for HNSW)
print("üîÑ Building HNSW index...")
index_hnsw.add(embeddings)

# Configure search
index_hnsw.hnsw.efSearch = efSearch

print(f"‚úÖ HNSW index created with {index_hnsw.ntotal} vectors")

# Search
query = np.random.random((1, dimension)).astype('float32')
distances, indices = index_hnsw.search(query, 10)

print(f"\nüîç Top 10 Results:")
for i, (dist, idx) in enumerate(zip(distances[0], indices[0]), 1):
    print(f"{i}. Index: {idx}, Distance: {dist:.4f}")

print("\nüí° HNSW Benefits:")
print("  - Fast search even for high-dimensional data")
print("  - Good recall with proper parameter tuning")
print("  - No training required")
print("  - Trade-off: Higher memory usage than IVF")
```

---

## Performance Optimization

### 1. Choosing the Right Index

```python
def recommend_index(num_vectors, dimension, memory_constraint, speed_requirement):
    """
    Recommend FAISS index type based on requirements.
    
    Args:
        num_vectors: Number of vectors in dataset
        dimension: Vector dimensionality
        memory_constraint: 'low', 'medium', or 'high'
        speed_requirement: 'slow', 'medium', or 'fast'
        
    Returns:
        Recommended index configuration
    """
    if num_vectors < 10000:
        return {
            "index_type": "IndexFlatL2",
            "reason": "Small dataset - use exact search",
            "config": {"dimension": dimension}
        }
    
    elif memory_constraint == 'low':
        if speed_requirement == 'fast':
            return {
                "index_type": "IndexIVFPQ",
                "reason": "Low memory + fast search - use IVF with PQ compression",
                "config": {
                    "dimension": dimension,
                    "nlist": int(np.sqrt(num_vectors)),
                    "m": min(64, dimension // 2),
                    "bits": 8
                }
            }
        else:
            return {
                "index_type": "IndexPQ",
                "reason": "Low memory - use product quantization",
                "config": {
                    "dimension": dimension,
                    "m": min(64, dimension // 2),
                    "bits": 8
                }
            }
    
    elif speed_requirement == 'fast':
        return {
            "index_type": "IndexHNSWFlat",
            "reason": "Fast search required - use HNSW",
            "config": {
                "dimension": dimension,
                "M": 32,
                "efConstruction": 40
            }
        }
    
    else:
        return {
            "index_type": "IndexIVFFlat",
            "reason": "Balanced approach - use IVF",
            "config": {
                "dimension": dimension,
                "nlist": int(np.sqrt(num_vectors))
            }
        }

# Example usage
recommendation = recommend_index(
    num_vectors=1000000,
    dimension=768,
    memory_constraint='medium',
    speed_requirement='fast'
)

print(f"üìã Recommended Index Configuration:")
print(f"  Type: {recommendation['index_type']}")
print(f"  Reason: {recommendation['reason']}")
print(f"  Config: {recommendation['config']}")
```

### 2. Batch Processing

```python
def batch_search_efficient(index, queries, k=10, batch_size=1000):
    """
    Efficient batch search with progress tracking.
    
    Args:
        index: FAISS index
        queries: Array of query vectors
        k: Number of neighbors to retrieve
        batch_size: Process queries in batches
        
    Returns:
        Distances and indices for all queries
    """
    num_queries = len(queries)
    all_distances = []
    all_indices = []
    
    print(f"üîÑ Processing {num_queries} queries in batches of {batch_size}...")
    
    for i in range(0, num_queries, batch_size):
        batch_end = min(i + batch_size, num_queries)
        batch_queries = queries[i:batch_end]
        
        # Search batch
        distances, indices = index.search(batch_queries, k)
        
        all_distances.append(distances)
        all_indices.append(indices)
        
        if (i // batch_size + 1) % 10 == 0:
            print(f"  Progress: {batch_end}/{num_queries} queries")
    
    # Combine results
    all_distances = np.vstack(all_distances)
    all_indices = np.vstack(all_indices)
    
    print(f"‚úÖ Completed {num_queries} queries")
    
    return all_distances, all_indices
```

### 3. Index Serialization

```python
import faiss
import numpy as np

# Set seed for reproducibility
np.random.seed(16)

# Create and populate index
dimension = 384
num_vectors = 10000
embeddings = np.random.random((num_vectors, dimension)).astype('float32')

index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# Save index to disk
index_file = "my_faiss_index.bin"
faiss.write_index(index, index_file)
print(f"üíæ Index saved to {index_file}")

# Load index from disk
loaded_index = faiss.read_index(index_file)
print(f"üì• Index loaded: {loaded_index.ntotal} vectors")

# Verify loaded index works
query = np.random.random((1, dimension)).astype('float32')
distances, indices = loaded_index.search(query, 5)
print(f"‚úÖ Loaded index is functional")
```

---

## Best Practices

### 1. **Data Preprocessing**

```python
def normalize_embeddings(embeddings):
    """
    Normalize embeddings for cosine similarity search.
    
    Args:
        embeddings: numpy array of embeddings
        
    Returns:
        Normalized embeddings
    """
    # L2 normalization
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    normalized = embeddings / (norms + 1e-8)  # Add epsilon to avoid division by zero
    
    return normalized.astype('float32')

# For cosine similarity, use IndexFlatIP with normalized vectors
embeddings = np.random.random((1000, 384)).astype('float32')
normalized_embeddings = normalize_embeddings(embeddings)

index = faiss.IndexFlatIP(384)  # Inner product = cosine similarity for normalized vectors
index.add(normalized_embeddings)

print("‚úÖ Using normalized embeddings for cosine similarity")
```

### 2. **Parameter Tuning**

```python
def tune_ivf_parameters(index_ivf, validation_queries, ground_truth, nprobe_values=[1, 5, 10, 20, 50]):
    """
    Tune IVF nprobe parameter for optimal speed/accuracy trade-off.
    
    Args:
        index_ivf: IVF index
        validation_queries: Query vectors for validation
        ground_truth: True nearest neighbors
        nprobe_values: Values of nprobe to test
        
    Returns:
        Optimal nprobe value
    """
    import time
    
    print("üîß Tuning nprobe parameter...\n")
    
    best_nprobe = nprobe_values[0]
    best_score = 0
    
    for nprobe in nprobe_values:
        index_ivf.nprobe = nprobe
        
        # Measure search time
        start = time.time()
        distances, indices = index_ivf.search(validation_queries, 10)
        search_time = time.time() - start
        
        # Calculate recall (how many true neighbors were found)
        recall = np.mean([
            len(set(indices[i]) & set(ground_truth[i])) / len(ground_truth[i])
            for i in range(len(validation_queries))
        ])
        
        print(f"nprobe={nprobe:3d}: Recall={recall:.3f}, Time={search_time:.4f}s")
        
        # Simple scoring: balance recall and speed
        score = recall - (search_time / 100)  # Penalize slow searches
        
        if score > best_score:
            best_score = score
            best_nprobe = nprobe
    
    print(f"\n‚úÖ Optimal nprobe: {best_nprobe}")
    return best_nprobe
```

### 3. **Error Handling**

```python
def safe_faiss_search(index, query, k=10):
    """
    Perform FAISS search with comprehensive error handling.
    
    Args:
        index: FAISS index
        query: Query vector
        k: Number of neighbors
        
    Returns:
        Search results or None if error
    """
    try:
        # Validate inputs
        if index is None:
            raise ValueError("Index is None")
        
        if index.ntotal == 0:
            raise ValueError("Index is empty")
        
        if len(query.shape) != 2:
            raise ValueError(f"Query must be 2D array, got shape {query.shape}")
        
        if query.shape[1] != index.d:
            raise ValueError(
                f"Query dimension {query.shape[1]} doesn't match index dimension {index.d}"
            )
        
        if k > index.ntotal:
            print(f"‚ö†Ô∏è Warning: k={k} > index size={index.ntotal}, reducing k")
            k = index.ntotal
        
        # Perform search
        distances, indices = index.search(query.astype('float32'), k)
        
        return distances, indices
        
    except Exception as e:
        print(f"‚ùå FAISS search error: {e}")
        return None, None
```

### 4. **Monitoring and Logging**

```python
class FAISSSearchLogger:
    """
    Logger for FAISS search operations with performance tracking.
    """
    
    def __init__(self):
        """Initialize logger."""
        self.search_count = 0
        self.total_time = 0
        self.slow_searches = []
    
    def log_search(self, query_id, k, search_time, num_results):
        """
        Log a search operation.
        
        Args:
            query_id: Identifier for the query
            k: Number of neighbors requested
            search_time: Time taken for search
            num_results: Number of results returned
        """
        self.search_count += 1
        self.total_time += search_time
        
        # Track slow searches (> 100ms)
        if search_time > 0.1:
            self.slow_searches.append({
                "query_id": query_id,
                "k": k,
                "time": search_time
            })
    
    def get_statistics(self):
        """Get search statistics."""
        if self.search_count == 0:
            return "No searches logged"
        
        avg_time = self.total_time / self.search_count
        
        stats = {
            "total_searches": self.search_count,
            "average_time": avg_time,
            "total_time": self.total_time,
            "slow_searches": len(self.slow_searches)
        }
        
        return stats
    
    def print_report(self):
        """Print performance report."""
        stats = self.get_statistics()
        
        print("üìä FAISS Search Performance Report")
        print("=" * 50)
        print(f"Total searches: {stats['total_searches']}")
        print(f"Average time: {stats['average_time']*1000:.2f}ms")
        print(f"Total time: {stats['total_time']:.2f}s")
        print(f"Slow searches (>100ms): {stats['slow_searches']}")
        
        if self.slow_searches:
            print(f"\n‚ö†Ô∏è Slowest searches:")
            sorted_slow = sorted(self.slow_searches, key=lambda x: x['time'], reverse=True)[:5]
            for i, search in enumerate(sorted_slow, 1):
                print(f"  {i}. Query {search['query_id']}: {search['time']*1000:.2f}ms (k={search['k']})")

# Example usage
logger = FAISSSearchLogger()

# Simulate searches
import time
for i in range(100):
    start = time.time()
    # ... perform FAISS search ...
    search_time = time.time() - start
    logger.log_search(query_id=i, k=10, search_time=search_time, num_results=10)

logger.print_report()
```

### 5. **Testing and Validation**

```python
def validate_faiss_index(index, embeddings, sample_size=100):
    """
    Validate FAISS index correctness.
    
    Args:
        index: FAISS index
        embeddings: Original embeddings used to build index
        sample_size: Number of samples to test
        
    Returns:
        Validation results
    """
    import numpy as np
    
    # Set seed for reproducible validation
    np.random.seed(16)
    
    print("üîç Validating FAISS index...")
    
    # Check index size
    if index.ntotal != len(embeddings):
        print(f"‚ùå Size mismatch: index has {index.ntotal}, expected {len(embeddings)}")
        return False
    
    # Sample random queries
    sample_indices = np.random.choice(len(embeddings), size=min(sample_size, len(embeddings)), replace=False)
    
    validation_passed = True
    
    for idx in sample_indices:
        query = embeddings[idx:idx+1]
        distances, indices = index.search(query, 1)
        
        # The query itself should be the nearest neighbor
        if indices[0][0] != idx:
            print(f"‚ùå Validation failed for index {idx}")
            validation_passed = False
            break
        
        # Distance should be ~0 (or very small due to floating point)
        if distances[0][0] > 1e-4:
            print(f"‚ö†Ô∏è Warning: Self-distance is {distances[0][0]} for index {idx}")
    
    if validation_passed:
        print(f"‚úÖ Validation passed: {sample_size} samples tested")
    
    return validation_passed
```

---

## Summary

### üîë Key Concepts Mastered

- **FAISS Fundamentals**: Understanding similarity search, vector embeddings, and approximate nearest neighbors
- **HuggingFace Integration**: Using FAISS with HuggingFace datasets and transformers
- **Index Types**: Choosing appropriate index structures for different use cases
- **Hate Speech Detection**: Applying FAISS to build efficient content moderation systems

### üìà Best Practices Learned

- **Index Selection**: Match index type to dataset size, memory constraints, and speed requirements
- **Normalization**: Use normalized embeddings with `IndexFlatIP` for cosine similarity
- **Parameter Tuning**: Balance speed and accuracy through proper parameter configuration
- **GPU Acceleration**: Leverage GPU resources for large-scale applications
- **Error Handling**: Implement robust error handling and validation

### üöÄ Next Steps

- **Advanced Indexing**: Explore composite indices and multi-stage retrieval
- **Distributed Search**: Scale FAISS to multiple machines with [Faiss-Server](https://github.com/facebookresearch/faiss/wiki)
- **Real-time Updates**: Implement online learning with incremental index updates
- **Hybrid Search**: Combine FAISS with traditional search (BM25, TF-IDF) for better results
- **Production Deployment**: Build production-ready search systems with monitoring and logging

### üìö Further Resources

- **Official Documentation**: [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)
- **Research Paper**: [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)
- **HuggingFace Datasets**: [FAISS Integration Guide](https://huggingface.co/docs/datasets/faiss_es)
- **Tutorials**: [FAISS Tutorial by Pinecone](https://www.pinecone.io/learn/faiss-tutorial/)
- **Community**: [FAISS GitHub Issues](https://github.com/facebookresearch/faiss/issues)

---

> **Key Takeaway**: FAISS is essential for scaling NLP applications beyond small datasets. By enabling efficient similarity search across millions or billions of embeddings, FAISS makes real-time semantic search, content moderation, and recommendation systems practical. Its integration with HuggingFace libraries makes it the go-to solution for production NLP systems that require both speed and accuracy.

> üí° **Pro Tip**: Start simple with `IndexFlatL2` during development, profile your application to understand bottlenecks, then optimize with advanced indices like `IndexIVFFlat` or `IndexHNSW` for production deployment.

> üõ°Ô∏è **For Hate Speech Detection**: FAISS enables real-time detection of hate speech variants, building searchable knowledge bases of toxic patterns, and explaining moderation decisions through similarity to known examples. This makes content moderation systems more transparent, accurate, and scalable.
