# ğŸ¤— HF Transformer Trove

A comprehensive collection of valuable models, code, and notebooks for learning and mastering the Hugging Face ecosystem. This repository serves as a practical guide through the essential concepts and tools in the world of transformers and natural language processing.

## ğŸ“š Repository Structure

### ğŸ“– Documentation (`docs/`)
Comprehensive explanations of key Hugging Face concepts with practical code examples:
- Key terminology and concepts
- API references and best practices
- Advanced techniques and patterns

### ğŸ’» Examples (`examples/`)
Jupyter notebooks demonstrating each topic with hands-on implementations:

1. **01 - Introduction to Hugging Face and Transformers** (`01_intro_hf_transformers.ipynb`)
2. **02 - Tokenizers** (`02_tokenizers.ipynb`) 
3. **03 - The Datasets Library** (`03_datasets_library.ipynb`)
4. **04 - Mini-Project: Integration** (`04_mini_project.ipynb`)
5. **05 - Fine-Tuning with Trainer API** (`05_fine_tuning_trainer.ipynb`)
6. **06 - Fine-Tuning from Scratch** (`06_fine_tuning_scratch.ipynb`)
7. **07 - Summarization Pipeline** (`07_summarization.ipynb`)
8. **08 - Question Answering Model** (`08_question_answering.ipynb`)
9. **09 - PEFT LoRA QLoRA** (`09_peft_lora_qlora.ipynb`)
10. **10 - LLMs RLHF** (`10_llms_rlhf.ipynb`)

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install -r requirements.txt
```

### Quick Start
1. Clone this repository
2. Install dependencies
3. Open Jupyter Lab/Notebook
4. Start with `examples/01_intro_hf_transformers.ipynb`

## ğŸ¯ Learning Path

**Beginner Level:**
- Start with notebooks 01-04 to build foundational understanding
- Focus on tokenization concepts and dataset handling

**Intermediate Level:**
- Progress to notebooks 05-08 for fine-tuning and specific tasks
- Explore different pipeline implementations

**Advanced Level:**
- Dive into notebooks 09-10 for cutting-edge techniques
- Experiment with PEFT and RLHF approaches

## ğŸ“‹ Topics Covered

- **Transformers Library**: Core concepts, model loading, inference
- **Tokenization**: Subword tokenization, special tokens, padding/truncation
- **Datasets**: Loading, processing, streaming large datasets
- **Fine-tuning**: Transfer learning, hyperparameter tuning, evaluation
- **Task-specific Models**: Summarization, Q&A, classification
- **Advanced Techniques**: LoRA, QLoRA, RLHF for large language models

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Hugging Face team for their amazing ecosystem
- The open-source community for continuous innovation
- Contributors and maintainers of transformer models

---

*Happy learning with Hugging Face! ğŸ¤—*
