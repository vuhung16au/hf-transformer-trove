# GitHub Copilot Instructions for HF Transformer Trove

## Project Overview
This repository is an educational resource for learning the Hugging Face ecosystem and serves as a study note for those who want to learn NLP. All code should be:
- Educational and well-documented with clear explanations and comments
- Follow best practices for machine learning workflows
- Use modern Hugging Face APIs and patterns
- Make it easy to understand and educational
- Assume readers are familiar with machine learning, PyTorch, and NLP basics

### Learning Resources References
For basic terms and definitions, link to:
1. [PyTorch Mastery Documentation](https://github.com/vuhung16au/pytorch-mastery/docs/)
2. [NLP Learning Journey Documentation](https://github.com/vuhung16au/nlp-learning-journey/docs/)

## Code Style Guidelines

### Python Code
- Use Python 3.8+ features
- Follow PEP 8 style guidelines
- **Prefer OOP implementation** - use classes and objects when appropriate
- Include type hints where beneficial for learning
- Use meaningful variable names that explain ML concepts
- Add docstrings for functions explaining ML terminology
- **Keep source code as simple as possible** while being educational
- **Comprehensive error handling** - handle errors gracefully with informative messages

### Jupyter Notebooks
- **Add badges at the top**: Include "Open in Colab" and "View on GitHub" badges
- Start each notebook with clear title and learning objectives
- Use markdown cells to explain concepts before code cells
- Include visualization wherever possible to aid understanding
- End sections with summary points or key takeaways
- Use consistent cell structure: Import → Load Data → Process → Model → Evaluate → Visualize
- **Make sure all notebooks run locally, on Colab, and Kaggle**
- **Use LaTeX equations in Markdown cells when needed** for mathematical explanations
- **Use Mermaid diagrams in Markdown cells when needed** for visual explanations

### Documentation
- Explain technical terms in context
- Provide links to official Hugging Face documentation
- Include code examples that are self-contained
- Use clear section headers and bullet points

## Technical Preferences

### Framework and Technology Preferences
- **Prefer PyTorch over TensorFlow** - use PyTorch for all deep learning implementations
- **Device Awareness**: Automatic optimization for CUDA, MPS (Apple Silicon), and CPU
- **Use TensorBoard integration** for comprehensive training visualization when possible

### Credential Management
- **When run locally**: Credentials (Hugging Face API key, Google Gemini API key, etc.) are loaded from `.env.local`
- **When run on Colab**: Credentials are loaded from Google Colab secrets manager
- **Never hard-code credentials** in notebooks or source code

### Language Support
- **English is the primary language** for NLP tasks and documentation
- **When second language is needed**: Use Vietnamese
- **Third language option**: Japanese

### Hugging Face Ecosystem
- Prefer `transformers` library over direct PyTorch/TensorFlow when possible
- Use `datasets` library for data handling
- Utilize `tokenizers` for efficient tokenization
- Implement `Trainer` API for training workflows
- Show both pipeline and manual approaches where applicable

### Dependencies Management
- Keep dependencies minimal and focused
- Pin versions for reproducibility
- Use requirements.txt for dependencies
- Prefer popular, stable packages

### Error Handling
- Include proper error handling in educational examples
- Show common pitfalls and how to avoid them  
- Provide troubleshooting tips in comments
- **Comprehensive error handling gracefully** - always handle exceptions with clear messages

### Device Awareness Implementation
```python
import torch

# Automatic device detection with priority: CUDA > MPS > CPU
def get_device():
    """Get the best available device for training/inference."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps") 
    else:
        return torch.device("cpu")

# Use in models
device = get_device()
model = model.to(device)
```

### Credential Loading Pattern
```python
import os
from google.colab import userdata  # Only available in Colab

def get_api_key(key_name: str) -> str:
    """Get API key from environment or Colab secrets."""
    try:
        # Try Colab secrets first (when in Colab environment)
        return userdata.get(key_name)
    except:
        # Fall back to local environment variable
        api_key = os.getenv(key_name)
        if not api_key:
            raise ValueError(f"Please set {key_name} in .env.local or Colab secrets")
        return api_key
```

## Learning-Focused Features
- Include progressive complexity (basic → advanced)
- Show before/after comparisons
- Provide multiple approaches to the same problem
- Include performance and memory considerations
- Add timing information for expensive operations

## Comments and Documentation Style

### Code Comments
```python
# Load a pre-trained BERT model for sequence classification
# This will download the model if not cached locally
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2,  # Binary classification
    output_attentions=False,  # Don't return attention weights
    output_hidden_states=False,  # Don't return hidden states
)

# Automatically detect and use the best available device
device = get_device()
model = model.to(device)
print(f"Using device: {device}")
```

### Notebook Badge Template
Add these badges at the top of every notebook:
```markdown
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/hf-transformer-trove/blob/main/examples/NOTEBOOK_NAME.ipynb)
[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/hf-transformer-trove/blob/main/examples/NOTEBOOK_NAME.ipynb)
```

### Mathematical Notation
Use LaTeX for mathematical explanations:
```markdown
The attention mechanism is computed as:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q$ is the query matrix
- $K$ is the key matrix  
- $V$ is the value matrix
- $d_k$ is the dimension of the key vectors
```

### Visual Diagrams
Use Mermaid for process diagrams:
```markdown
```mermaid
graph LR
    A[Input Text] --> B[Tokenizer]
    B --> C[Token IDs] 
    C --> D[Model]
    D --> E[Predictions]
```

## Avoid
- Overly complex examples that obscure learning goals
- Deprecated APIs or patterns
- Hard-coded paths or credentials
- Examples that require excessive computational resources
- Code without educational context or explanation