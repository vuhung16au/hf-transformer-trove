# GitHub Copilot Instructions for HF Transformer Trove

## Project Overview
This repository is an educational resource for learning the Hugging Face ecosystem. All code should be:
- Educational and well-documented
- Include clear explanations and comments
- Follow best practices for machine learning workflows
- Use modern Hugging Face APIs and patterns

## Code Style Guidelines

### Python Code
- Use Python 3.8+ features
- Follow PEP 8 style guidelines
- Include type hints where beneficial for learning
- Use meaningful variable names that explain ML concepts
- Add docstrings for functions explaining ML terminology

### Jupyter Notebooks
- Start each notebook with a clear title and learning objectives
- Use markdown cells to explain concepts before code cells
- Include visualization wherever possible to aid understanding
- End sections with summary points or key takeaways
- Use consistent cell structure: Import → Load Data → Process → Model → Evaluate → Visualize

### Documentation
- Explain technical terms in context
- Provide links to official Hugging Face documentation
- Include code examples that are self-contained
- Use clear section headers and bullet points

## Technical Preferences

### Hugging Face Ecosystem
- Prefer `transformers` library over direct PyTorch/TensorFlow when possible
- Use `datasets` library for data handling
- Utilize `tokenizers` for efficient tokenization
- Implement `Trainer` API for training workflows
- Show both pipeline and manual approaches where applicable

### Dependencies Management
- Keep dependencies minimal and focused
- Pin versions for reproducibility
- Use requirements.txt for dependencies
- Prefer popular, stable packages

### Error Handling
- Include proper error handling in educational examples
- Show common pitfalls and how to avoid them
- Provide troubleshooting tips in comments

## Learning-Focused Features
- Include progressive complexity (basic → advanced)
- Show before/after comparisons
- Provide multiple approaches to the same problem
- Include performance and memory considerations
- Add timing information for expensive operations

## Comments and Documentation Style
```python
# Load a pre-trained BERT model for sequence classification
# This will download the model if not cached locally
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2,  # Binary classification
    output_attentions=False,  # Don't return attention weights
    output_hidden_states=False,  # Don't return hidden states
)
```

## Avoid
- Overly complex examples that obscure learning goals
- Deprecated APIs or patterns
- Hard-coded paths or credentials
- Examples that require excessive computational resources
- Code without educational context or explanation